1 Einleitung
1.1 Hintergrund
Die deutliche Ausweitung des mautpflichtigen Streckennetzes auf Bundesstraßen ist mit der
vorhandenen Fahrzeuggeräteflotte (insbesondere der Altgeräte) und der bislang dezentralen
Erkennungs- und Tarifierungslogik aufgrund von Speicherrestriktionen nicht umsetzbar. Die
Verlagerung der Erkennungs- und Tarifierungslogik und damit auch der umfangreicheren
Betriebsdaten in zentrale Systeme wird damit notwendig.
Aufgrund des gesteigerten Umfangs des Erkennungsmodells ist es zudem nicht mehr möglich, die
bisherige Praxis der reinen manuellen Modellierung aufrecht zu erhalten. Fehler im teilautomatisch
generierten Modell müssen erkannt werden und es muss durch die nachfolgende Anwendung
statistischer Methoden kontinuierlich optimiert werden.
Im Zuge der Zentralisierung der Erkennung soll die Qualitätssicherung des Automatischen Verfahrens
ebenfalls zentralisiert werden.
Im Projekt QS-AV wurden die Komponenten zur automatischen Qualitätssicherung realisiert.
Gegenstand dieses Dokuments ist die Detailspezifikation dieser Komponenten, die im Folgenden
nach Schichten kategorisiert aufgeführt werden:
Applikation
Die QS-AV Applikation mit den Teilkomponenten:
 QS-FzG (Qualitätssicherung der Fahrzeuggeräte)
 QS-Erh (Qualitätssicherung der Erhebung)
Persistenz
Verteilte Datenhaltung in einem DataStax Enterprise Cluster basierend auf der
Cassandra Datenbank.
Präsentation
Komponenten zur Darstellung und Interaktion:
 Gesundheitsakte (Subkomponente von QS-FzG) zum Abruf von Fahrzeuggeräteinformationen
 Zeppelin zum Zugriff auf die QS-AV Persistenz via Web-Notebooks zu Analysezwecken (Apache
Zeppelin)
Betriebliche Verwaltung und Überwachung
OpsCenter zum betrieblichen Monitoring und der Verwaltung des DataStax Enterprise Clusters
Einleitung
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 18 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
1.2 Wichtige Hinweise
Die fachlichen Anforderungen, sowie die fachlichen Klassendiagramme und fundierten Hintergründe
wurden und werden konsistent über den gesamten Entwicklungszyklus im [3] Lastenheft QS-FzG und
Lastenheft QS-Erhebung dokumentiert und aktualisiert.
Die Klärungshistorie zur konkreten Umsetzung der Anforderungen inklusive der beteiligten
Ansprechpartner aus Fachbereich und Entwicklung können den entsprechenden User Stories in JIRA1
entnommen werden.
Hinweis: Im Dokumentenverlauf werden wichtige Hinweise gerahmt und mit einem
Ausrufezeichen markiert hervorgehoben.
Weiterführende Informationen und wichtige Dokumentenverweise werden gerahmt und mit einer
Büroklammer markiert hervorgehoben.
Wichtige Informationen und Definitionen werden gerahmt und mit einem Informationssymbol
markiert hervorgehoben.
Trigger: Informationen zu zeitgesteuerten Jobs und Trigger (Bookkeeper) werden gerahmt und
mit einer Uhr markiert hervorgehoben.

1
http://blntcjira01.tollcollect.cs.tc.corp:8080 (Projekt: QS-AV)
Einleitung
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 19 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
1.3 Gegenstand dieses Dokuments
Die Detailspezifikation zur Qualitätssicherung Automatisches Verfahren (QS-AV) vermittelt dem Leser
die technischen Details zur Überwachung und Qualitätssicherung der Mauterhebung.
Abbildung 1: Systemüberblick (schematisch)
1.4 Zielsetzung
Ziele und Anforderungen entnehmen Sie bitte dem Referenzdokument [3] Lastenheft QS-FzG
und Lastenheft QS-Erhebung.
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 20 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2 Rahmenbedingungen
2.1 Technischen Rahmenbedingungen
2.1.1 Hardware
Deployment und Betrieb erfolgen in einer AppAgile Umgebung, basierend auf der Cloud Computing
Plattform OpenShift (Red Hat Linux). Alle Software-Komponenten werden in Docker-Images integriert
und zusammen mit der jeweils zugehörigen Deployment-Konfiguration ausgeliefert. Die
entsprechenden Docker-Container werden als Pods auf AppAgile-Knoten deployed und gestartet.
Alle Details zur Hardware-Umgebung inklusive der technischen Voraussetzungen finden Sie in
den Referenzdokumenten [17] [A_QSAV_HBB_00] QS-AV Betriebshandbuch und
[16] [A_QSAV_HBI_00] QS-AV Installationshandbuch.
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 21 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.2 Software QS-AV Persistenz: DataStax Enterprise Enterprise
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen für die QSAV Persistenzschicht aufgelistet.
2.1.2.1 DataStax Enterprise
Bezeichnung DataStax Analytics Node / DataStax Agent
Version 5.0.4 / 6.0.8
Einsatz Zentrale Datenhaltung für QS-AV
DataStax Analytics Node unter Einsatz von Cassandra 3.0.9
Agent
Datastax Agent in der Version 6.0.8 zur Kommunikation mit dem OpsCenter
Detailinformationen zu DataStax Enterprise 5.0.4 entnehmen Sie der Produktdokumentation2
.
Tabelle 3: Einsatz von DataStax Enterprise
2.1.3 Software QS-AV Applikation inklusive Bookkeeper
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen für die QSAV Applikation gemeinsam für die Teilkomponenten QS-FzG und QS-Erh aufgelistet.
Abbildung 2: Software-Einsatz für der QS-AV Applikationsschicht

2
Siehe http://www.datastax.com/products/datastax-enterprise
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 22 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.3.1 DataStax Enterprise
Bezeichnung DataStax Enterprise (DSE)
Teilkomponente/n QS-FzG / QS-Erh
Version 5.0.4
Einsatz Wichtig: Ruhende Installation zum Zugriff auf den DSE-Ring (QS-AV Datenhaltung). Der
Bookkeeper-Pod ist nicht als Teil des DSE-Rings konfiguriert.
Zugriff auf DSE Cassandra DB (lesend / schreibend)
 Einsatz der Analytics-Komponente:
Spark 1.6.2 (spark-core, spark-sql, spark-hive, spark-streaming, spark-test-tags,
spark-network-common) unter Nutzung von DataFrames und RDDs
 Spark Cassandra Connector 1.6.2 (für Scala und Java), Cassandra Treiber
Version 3.0.3
Detailinformationen zu DataStax Enterprise 5.0.4 entnehmen Sie der
Produktdokumentation3
.
Tabelle 4: Einsatz von DataStax Enterprise
2.1.3.2 snappy-java
Bezeichnung snappy-java
Teilkomponente/n QS-FzG / QS-Erh
Version 1.1.2.6
Einsatz qs-common (Bibliothek)
Die Bibliothek wird zur optionalen Kompression von Requests und Responses auf
Transport-Ebene für den Cassandra Treiber eingesetzt.
Tabelle 5: Einsatz von snappy-java
2.1.3.3 Scala
Bezeichnung Scala (scala-library, scala-compiler)
Teilkomponente/n QS-FzG / QS-Erh
Version 2.10.5
Einsatz Programmierung der Anwendungslogik
Einsatz u.a. aufgrund der Kompatibilität und Performance-Vorteile mit dem Spark
Framework Layer
Tabelle 6: Einsatz von Scala

3
Siehe http://www.datastax.com/products/datastax-enterprise
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 23 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.3.4 nscala-time
Bezeichnung nscala-time
Teilkomponente/n QS-FzG / QS-Erh
Version 2.12.0
Einsatz qs-common (Bibliothek)
Scala Wrapper für Joda Time, Nutzung als Bibliothek für Datums- und Zeitberechnungen
Tabelle 7: Einsatz von nscala-time
2.1.3.5 play-json
Bezeichnung play-json
Teilkomponente/n QS-FzG / QS-Erh
Version 2.4.8
Einsatz qs-common
Scala JSON Bibliothek
Tabelle 8: Einsatz von play-json
2.1.3.6 Jetty
Bezeichnung jetty-all-server
Teilkomponente/n QS-Commons
Version 8.1.14.v20131031
Einsatz Webserver
Zur Exposition von REST-Services
Tabelle 9: Einsatz von Jetty
2.1.3.7 Java
Bezeichnung Java (JDK und JRE)
Teilkomponente/n QS-FzG / QS-Erh
Version 1.8.0_u102
Einsatz Laufzeitumgebung (JRE)
Benötigt für die Laufzeitumgebung (JVM)
Entwicklung (JDK)
Zum Einsatz in der Entwicklung (im Zusammenspiel mit Scala)
Unit-Testing (JUnit)
Für in Java geschriebene Unit-Tests
Tabelle 10: Einsatz von Java
2.1.3.8 Google Guava
Bezeichnung guava
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 24 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Teilkomponente/n QS-FzG / QS-Erh
Version 19.0
Einsatz Allgemein
Erweiternde Java-Bibliothek
Tabelle 11: Einsatz von Guava
2.1.3.9 slf4j-api
Bezeichnung slf4j-api
Teilkomponente/n QS-FzG / QS-Erh
Version 1.7.12
Einsatz Logging
Logging API
Tabelle 12: Einsatz von slf4j-api
2.1.3.10 JUnit
Bezeichnung JUnit
Teilkomponente/n QS-FzG / QS-Erh
Version 4.12
Einsatz Unit-Testing
Für in Java geschriebene Unit-Tests.
Tabelle 13: Einsatz von JUnit
2.1.3.11 ScalaTest
Bezeichnung ScalaTest
Teilkomponente/n QS-FzG / QS-Erh
Version 2.2.5
Einsatz Testing
Für in Scala geschriebene Tests.
Tabelle 14: Einsatz von ScalaTest
2.1.3.12 spark-testing-base
Bezeichnung spark-testing-base
Teilkomponente/n QS-FzG / QS-Erh
Version 1.6.2_0.4.5
Einsatz Testing
Basisklassen für Spark-Tests
Tabelle 15: Einsatz von spark-testing-base
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 25 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.3.13 H2 Database
Bezeichnung h2
Teilkomponente/n QS-FzG / QS-Erh
Version 1.4.192
Einsatz Testing
Java SQL-Datenbank zur Testunterstützung
Tabelle 16: Einsatz der H2 Database
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 26 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.4 Software Timebucket-Switcher Applikation
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen für den
Timebucket-Switcher aufgelistet.
2.1.4.1 snappy-java
Bezeichnung snappy-java
Version 1.1.2.6
Einsatz qs-common (Bibliothek)
Die Bibliothek wird zur optionalen Kompression von Requests und Responses auf TransportEbene für den Cassandra Treiber eingesetzt.
Tabelle 17: Einsatz von snappy-java
2.1.4.2 Scala
Bezeichnung Scala (scala-library, scala-compiler)
Version 2.10.5
Einsatz Programmierung der Anwendungslogik
Tabelle 18: Einsatz von Scala
2.1.4.3 nscala-time
Bezeichnung nscala-time
Version 2.12.0
Einsatz qs-common (Bibliothek)
Scala Wrapper für Joda Time, Nutzung als Bibliothek für Datums- und Zeitberechnungen
Tabelle 19: Einsatz von nscala-time
2.1.4.4 Java (JRE)
Bezeichnung Java (JRE)
Version 1.8.0_u102
Einsatz Für den TB-Switcher benötigte Java Version (JRE)
Tabelle 20: JRE-Version
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 27 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.5 Software QS-AV Webservice-Applikation
2.1.5.1 Scala
Bezeichnung Scala (scala-library, scala-compiler)
Version 2.10.6
Einsatz Programmierung der Anwendungslogik
Tabelle 21: Einsatz von Scala
2.1.5.2 cassandra-caseclass-adapter
Bezeichnung cassandra-caseclass-adapter
Version 0.1
Einsatz Bibliothek zum automatischen Mapping zwischen case classes und Cassandra-Tabellen
Tabelle 22: Einsatz von cassandra-caseclass-adapter
2.1.5.3 DataStax Cassandra Treiber
Bezeichnung DataStax Cassandra Treiber
Version 3.0.7
Einsatz Zum Zugriff auf den DSE-Ring zur Realisierung der Cognos-Schnittstelle
Tabelle 23: Einsatz des DatStax Cassandra Treibers
2.1.5.4 Play Framework
Bezeichnung playframework
Version 2.4.x
Einsatz Bibliothek zur Erstellung von Webapplikationen basierend auf Akka Streams
Tabelle 24: Einsatz von playframework
2.1.5.5 Bootstrap
Bezeichnung bootstrap
Version 3.3.7-1
Einsatz HTML-, CSS- und JS-Framework zur Benutzung innerhalb Webapplikationen
Tabelle 25: Einsatz von bootstrap
2.1.5.6 Shiro
Bezeichnung shiro-core
Version 1.3.2
Einsatz Sicherheits-Framework mit Funktionalitäten zur Authentifizierung, Autorisierung,
Kryptographie und Session-Management
Tabelle 26: Einsatz von shiro-core
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 28 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.5.7 jBCrypt
Bezeichnung jbcrypt
Version 0.4
Einsatz Bibliothek zum Passwort-Hashing (Blowfish)
Tabelle 27: Einsatz von jbcrypt
2.1.5.8 Macro Paradise
Bezeichnung paradise
Version 2.1.0
Einsatz Kompatibilitäts-Plugin für zahlreiche Scala-Compiler
Tabelle 28: Einsatz von paradise
2.1.5.9 ScalaTest
Bezeichnung ScalaTest
Version 2.2.6
Einsatz Testing
Für in Scala geschriebene Tests.
Tabelle 29: Einsatz von ScalaTest
2.1.5.10 scalatestplus-play
Bezeichnung scalatestplus-play
Version 1.4.0
Einsatz Scala Bibliothek zum Testen von Play-Applikationen
Tabelle 30: Einsatz von play-json
2.1.5.11 ScalaMock
Bezeichnung ScalaMock
Version 3.2.2
Einsatz Testing
Library für in Scala geschriebene Tests.
Tabelle 31: Einsatz von ScalaTest
2.1.5.12 slf4j-api
Bezeichnung slf4j-api
Version 1.7.25
Einsatz Logging
Logging API
Tabelle 32: Einsatz von slf4j-api
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 29 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.5.13 scala-logging
Bezeichnung scala-logging
Version 2.1.2
Einsatz Logging
Logging API
Tabelle 33: Einsatz von scala-logging
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 30 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.6 Software Gesundheitsakte
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen für die
Gesundheitsakte aufgelistet.
2.1.6.1 HTTP Server
Bezeichnung Apache HTTP Server
Version 2.4.25
Einsatz HTTP Server für die Web-Anwendung
Tabelle 34: HTTP Server
2.1.6.2 Python
Bezeichnung Python
Version 2.7.6
Einsatz Programmierung der Anwendungslogik, inklusive:
 Python CGI (aktuellste Version)
 Python_Sqlite 3.2.6
 PySpark 1.6.8
 Python_Cassandra 3.7.0
 Python_CQL 1.4.0
 Python_Base64 (aktuellste Version)
 Python_Numpy 1.8.2
 Python_Matplotlib 1.3.1
Tabelle 35: Python
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 31 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.7 Software Zeppelin
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen für die
Zeppelin-Installation aufgelistet.
2.1.7.1 Zeppelin
Bezeichnung Zeppelin
Version 0.7.1
Einsatz Apache Zeppelin Installation
Benutzter Built-In Interpreter: Spark-Interpreter und Cassandra-Interpreter
Tabelle 36: Einsatz von Zeppelin
2.1.7.2 DataStax Enterprise
Bezeichnung DataStax Enterprise (DSE)
Version 5.0.4
Einsatz Wichtig: Ruhende Installation zum Zugriff auf den DSE-Ring (QS-AV Datenhaltung). Der
Zeppelin-Pod ist nicht als Teil des DSE-Rings konfiguriert.
Detailinformationen zu DataStax Enterprise 5.0.4 entnehmen Sie der Produktdokumentation4
.
Tabelle 37: Einsatz von DataStax Enterprise
2.1.7.3 Java (JRE)
Bezeichnung Java (JRE)
Version 1.8.0_u102
Einsatz Für Apache Zeppelin benötigte Java Version (JRE)
Tabelle 38: JRE-Version
2.1.7.4 Google Guava
Bezeichnung guava
Version 19.0
Einsatz Allgemein erweiternde Java-Bibliothek
Tabelle 39: Einsatz von Guava
2.1.7.5 slf4j-api
Bezeichnung slf4j-api
Version 1.7.12
Einsatz Logging API
Tabelle 40: Einsatz von slf4j-api

4
Siehe http://www.datastax.com/products/datastax-enterprise
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 32 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.1.8 Software OpsCenter
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen für die
OpsCenter-Installation aufgelistet.
2.1.8.1 OpsCenter Server
Bezeichnung OpsCenter Server
Version 6.0.8
Einsatz OpsCenter Server zur Kommunikation mit dem DSE-Ring (Monitoring und Verwaltung)
Tabelle 41: Einsatz von OpsCenter Server
2.1.8.2 Java (JRE)
Bezeichnung Java (JRE)
Version 1.8.0_u102
Einsatz Für OpsCenter benötigte Java Version (JRE)
Tabelle 42: JRE-Version
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 33 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.2 Organisatorische Rahmenbedingungen
2.2.1 Richtlinien
2.2.1.1 Vorgaben zum Logging
Siehe [10] White Paper – AppAgile Logging-Format.
2.2.1.2 Entwicklerrichtlinien
 Entwicklungsrichtlinien AppAgile: [11] Entwicklungsrichtlinien AppAgile – Technische
Dokumentation
 Scala Style Guide: https://github.com/databricks/scala-style-guide
 Spark Guide: http://www.cloudera.com/documentation/enterprise/5-6-x/PDF/clouderaspark.pdf
2.2.1.3 Benennung der Cassandra-Tabellen
Zur Vereinheitlichung der Tabellennamen wird folgendes Schema für die Tabellennamen angewandt:
Schema
Systempräfix – Tabellennamen optional inkl. fachlichem Präfix – optional: Suffix für Art der Daten
2.2.1.3.1 Aufbau des Tabellennamens
[ fzg | erh | bkpr | tech | lds ] _tabellenname_ ( [ mvw | tmp | prv ] )
Man beachte die korrespondierenden Farben zwischen dem Pattern und zum Aufbau des
Tabellennamens selbst. In blau ist hier der Systempräfix, in violett der Tabellenname selbst und in
grün der optionale Suffix abgebildet.
Der Tabellenname wird je Wortbedeutung in Lowercase-Camelcase geschrieben
(wortFürWortBeginneKlein) und diverse Wortbedeutungen werden jeweils durch einen
Unterstrich getrennt (z.B: fzg_fachlicheBedeutung_by_ehkid). Wie im Code sollte auch hier die
Grundsprache Englisch sein mit jeweils eingedeutschten Fachbezeichnungen.
In LDS-Tabellen, beginnend mit dem "lds_"-Präfix sind die Primärdaten aus den
Eingangsschnittstellen zu finden. Alle anderen Sekundärdaten, welche erzeugt werden, sind in
anderen Tabellen zu finden. Diese unterteilen sich in fachliche sowie technische Daten.
Systempräfix
Fachliche Daten sind zu finden in – nach Systempräfix plus deren vorrangiger, hierarchischer
Trennung:
 fzg – für alle Daten, welche zu QS-FzG gehören
 erh – für alle Daten, welche zu QS-Erh gehören
 lds – für alle Primärdaten aus den LDS-Systemen (Schnittstellentabellen)
Technische Daten sind zu finden in – nach Systempräfix plus deren vorrangiger, hierarchischer
Trennung:
 bkpr – für alle Daten, welche dem Bookkeeper gehören
 tech – für alle anderen technischen Daten, wie temporäre und Staging-Tabellen
Tabellenname in der Mitte
Der Tabellenname in der Mitte kann ein fachliches oder technisches inneres Präfix enthalten – je nach
Tabellenart. Die Beispiele unten zeigen die Verwendung interner Präfixe.
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 34 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
optionaler Suffix
Das Suffix ist jeweils optional. Die beiden vorgesehenen Suffixe sind:
 mvw – ein Suffix für eine abgeleitete Materialized View aus einer anderen Tabelle
 tmp – ein Suffix für eine Tabelle, welche lediglich zur Laufzeit konsistent und befüllt sein muss
 prv – Tabellen mit Zwischenergebnissen, welche aus technischen Gründen vorgehalten
werden müssen
2.2.1.3.2 Beispiele
Beispiele QS-FzG
 fzg_someItems
 fzg_verdichtung_by_ehkid
 fzg_someOtherItems
Beispiele QS-Erh
 erh_gdbs_grunddaten
 erh_mo_listOfMos
 erh_mo_listOfMos_by_state
Beispiele Mischung
 fzg_verdichtung_by_ehkid
 erh_gdbs_grunddaten
 erh_mo_listOfMos
 erh_mo_listOfMos_by_state
 bkpr_persistedStates
 bkpr_otherData
 tech_stagingTable_ruleAbc
 tech_otherData
 lds_dm_useCaseA
 lds_es_useCaseA
 lds_es_useCaseC
 lds_edm_useCaseB
Rahmenbedingungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 35 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2.2.1.4 Pattern für Schwellwert- & Konfigurationstabelle
Die Tabellen für fachliche Schwellwerte und technische Konfigurationen sind:
 tech_rule_schwellwert
 tech_config
Die in beiden Tabellen befindlichen Keys sind mit einem Präfix zu versehen, wie die Tabellennamen
der Cassandra-Tabellen selbst auch. Auf diese Art sollen ähnliche und zusammengehörige Werte
schneller gefunden werden können. Das Präfix soll vom fachlichen Modul oder aus Fachbezeichnern
abgeleitet werden. Die Keys der Tabellen sollen in beiden Tabellen:
 komplett in Kleinbuchstaben geschrieben sein,
 mit einem Punkt getrennt werden,
 ein führendes Systempräfix enthalten und
 danach die inneren Präfixe vom Allgemeinen ins Spezielle aufweisen.
2.2.1.4.1 Schwellwertschlüssel
Zugelassene Systempräfixe für Schlüssel in Schwellwerttabellen:
 qserh
 qsfzg
Beispiele für Keys der fachlichen Schwellwerte in tech_rule_schwellwert:
 qserh.moba.betrachtungszeitraum.enddate
 qserh.onq.nummernkreis.oben
 qserh.moba.anothervalue
 qserh.eo.somevalue
 qsfzg.fsm.somevalue
2.2.1.4.2 Konfigurationsschlüssel
Zugelassene Systempräfixe für Schlüssel in Konfigurationstabellen:
 qserh
 qsfzg
 bkpr
 tech
Beispiele für technische Konfigurationen in tech_config:
 qsfzg.sw.aktuelleversion
 bkpr.timebucket.fsm
 bkpr.timebucket.mdn
 bkpr.timebucket.befahrungsparameter
 tech.configurationvalue
Architekturentscheidungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 36 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3 Architekturentscheidungen
In diesem Kapitel sind die Architekturentscheidungen unter Angabe der Ziele und externen
Festlegungen für QS-AV beschrieben. Im Anschluss wird die daraus resultierende Grobarchitektur
dargestellt und begründet.
Hintergründe zu den Architekturentscheidungen aus der Projektplanungsphase finden Sie im
Referenzdokument [15] QS-AV – Entscheidungsempfehlung zur technischen Architektur
(Capgemini). Die folgenden Inhalte innerhalb dieses Kapitels basieren auf der
Entscheidungsempfehlung und führen diese weiter.
3.1 Ziele
In diesem Abschnitt werden die wichtigsten Ziele aufgelistet, die bei der Architekturbewertung
berücksichtigt wurden.
1. Kompatibilität der Betriebsinfrastruktur
Die Zielarchitektur soll zur bestehenden Betriebsinfrastruktur (HSR2 oder AppAgile)
kompatibel sein.
2. Einfache Betreibbarkeit
Die Zielarchitektur soll möglichst einfach betreibbar sein. Das bedeutet, dass
Aufrechterhaltung und Änderung der Lösung im Betrieb mit möglichst geringem Aufwand
durchgeführt werden können.
3. Herstellersupport 3 Jahre+
Für eingesetzte Standardkomponenten soll ein Enterprise- und Long-Term-Support von
mindestens drei Jahren angeboten werden.
4. Geeignet für Mengengerüst
Bei der Auswahl der Zielarchitektur muss das – durch die Aggregation von Daten aus allen
Systemen des Leistungsdatenstroms vergleichsweise große – Mengengerüst berücksichtigt
werden.
5. Ausreichende Performance
Bei der Auswahl der Zielarchitektur muss die Komplexität der Korrelationen innerhalb der
Qualitätssicherungsalgorithmen hinsichtlich der Performance berücksichtigt werden.
6. Nutzung als Data-Science-Platform
Die Lösung muss eine Data-Science-Platform für explorative Datenanalysen (Fachbereich)
bieten.
7. Geringer Entwicklungsaufwand
Der Entwicklungsaufwand muss möglichst gering gehalten werden.
8. Einhaltung der Datenschutzvorgaben
Das hohe Datenschutzniveau muss bei der Auswahl der Architekturkomponenten
berücksichtigt werden.
3.2 Festlegung
Folgende Festlegung wurde im Projektverlauf von außen an das Projekt getragen:
 Betriebsinfrastruktur: AppAgile
Die QS-AV-Systeme sollen in der AppAgile-Umgebung betrieben werden.
Architekturentscheidungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 37 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3.3 Entscheidungsprozess
3.3.1.1 Schritt 1: Identifizierung von Lösungskandidaten
Folgende Lösungskandidaten für die Datenhaltung wurden anhand der obigen Ziele und Festlegungen
identifiziert und evaluiert:
Kurzname Lösungskomponente Evaluierte Version
Splunk Splunk inklusive Prediction App 6.3.3 / 1.0
Oracle Oracle 12c
DSE DataStax Enterprise 4.8
Hadoop Cloudera Hadoop Distribution (CDH) 5.5.2
Hinweis: Splunk wurde u.a. aus folgenden Gründen bereits vor der Detailbewertung aus der Auswahl
für QS-AV entfernt: Für die Prediction App wird kein Enterprise-Support angeboten, die Verdichtungen
und Korrelationen sind nicht trivial durchzuführen und zu persistieren, der Prediction App fehlten
zudem notwendige Algorithmen.
3.3.1.2 Schritt 2: Bewertung nach Betriebskriterien
 Kriterium / Lösung  Oracle DSE Hadoop
Kompatibilität zur bestehenden Infrastruktur ++ + +
Einfacher Betrieb + ++ -
Long-Term-Support für mind. 3 Jahre ++ 0 --
Third-Level-Support vorhanden ++ ++ ++
Belastbarkeit des Third-Level-Supports ++ + +
Verfügbarkeit von Sicherheitsupdates / Patches ++ ++ ++
Zukunftssicherheit nach Marktanalysten ++ + -
Punkte 13 9 2
Skala: ++=2 Punkte, +=1 Punkt, 0=0 Punkte, -=-1Punkt, --=-2 Punkte
Architekturentscheidungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 38 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3.3.1.3 Schritt 3: Bewertung nach Data-Science-Kriterien
 Kriterium / Lösung  Oracle DSE Hadoop
Eignung für QS-AV Aufgaben im Allgemeinen - ++ ++
Wertschöpfungshöhe für die Verarbeitung von Geodaten ++ ++ -
Eignung für Zeitreihen - ++ --
Mächtigkeit des Analysestacks und Verfügbarkeit von SQL und
Flexibilität
0 ++ ++
Punkte 0 8 1
Skala: ++=2 Punkte, +=1 Punkt, 0=0 Punkte, -=-1Punkt, --=-2 Punkte
3.3.1.4 Schritt 4: Bewertung nach Entwicklungskriterien
 Kriterium / Lösung  Oracle DSE Hadoop
Entwicklungsaufwand ++ ++ +
Hohes Datenschutzniveau ++ ++ 0
Last & Performance -- ++ +
Wissensbasis, Erfahrung und Referenzprojekte ++ 0 0
Flexibilität bzgl. zukünftiger Anforderungen + ++ ++
Punkte 5 8 4
Skala: ++=2 Punkte, +=1 Punkt, 0=0 Punkte, -=-1Punkt, --=-2 Punkte
3.3.1.5 Schritt 5: Bewertung nach weiteren technischen Kriterien
 Kriterium / Lösung  Oracle DSE Hadoop
Durchgehende Verfügbarkeit (HA) 0 ++ ++
Transaktionssicherheit ++ + -
Verfügbarkeit OR-Mapper ++ - -
Verfügbarkeit Queues ++ 0 ++
Vollindizierung auf Dokumentenbasis -- ++ ++
Punkte 4 4 4
Skala: ++=2 Punkte, +=1 Punkt, 0=0 Punkte, -=-1Punkt, --=-2 Punkte
Architekturentscheidungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 39 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3.3.1.6 Schritt 4: Summieren der Punkte
 Bereich / Lösung  Oracle DSE Hadoop
Betriebskriterien 13 9 2
Data-Science-Kriterien 0 8 1
Entwicklungskriterien 5 8 4
Weitere technische Kriterien 4 4 4
Gesamtpunkte 22 29 11
3.3.1.7 Schritt 5: Berücksichtigung des Gartner-Quadranten (ODBMS 2015)
Allgemein: Oracle und DataStax werden als Leader kategorisiert, Oracle im Marktführungsbereich.
Architekturentscheidungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 40 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Analyse des 3-Jahrestrends: Durch immer stärkere Marktdurchdringung gibt es ausschließlich bei
DataStax eine steigende Tendenz.
Zusammenfassung der Bewertung
 Oracle scheidet durch Datenmenge und die Performance bei deren Verarbeitung aus.
o Oracle kann schnell fließende Daten (Velocity) voraussichtlich nicht bewältigen.
o Oracle kann die Datenmengen (Volume) voraussichtlich nicht bewältigen.
 QS-AV kann hinsichtlich Datenmenge und Performance sowohl mit DSE als auch Hadoop
realisiert werden.
o DSE ist im Vergleich die dominante Lösung in allen untersuchten Bereichen.
o DSE ist im Vergleich zu Hadoop einfacher zu betreiben und verfügt über einen
schlankeren Technologiestack.
 Fazit: Realisierung mit DataStax Enterprise auf der AppAgile-Plattform. Als
Programmiersprache wurde Scala aufgrund der Performance-Benchmarks gewählt. Das
Spark-Framework bietet performanten Zugriff auf die Datenhaltung und ermöglicht die
Nutzung sowohl von DataFrames (API) als auch SQL-Statements.
 Zusätzlich wurde Apache Zeppelin als Frontend zum Zugriff auf die Datenhaltung gewählt.
Gründe dafür sind die Verfügbarkeit zahlreicher Interpreter (z.B. Spark, Cassandra, R) zum
Zugriff auf die Datenhaltung sowie die Möglichkeit unterschiedlicher Visualisierungen und
einer LDAP-Anbindung.
Im folgenden Kapitel wird die daraus resultierende Grobarchitektur dargestellt und die Aufteilung der
Teilkomponenten begründet.
Architekturentscheidungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 41 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3.4 Grobarchitektur
Dieser Abschnitt gibt einen Überblick über die Komponenten von QS-AV. Weiterführende
Informationen zu den einzelnen Teilkomponenten finden Sie im Kapitel 4.2 Technischer Kontext.
Abbildung 3: QS-AV Architekturkomponenten
Allgemein: Alle oben dargestellten QS-AV-Teilkomponenten werden in AppAgile als Pods (Docker
Container) betrieben.
Anmerkungen zur Plattformentscheidung AppAgile und Auswirkungen
Der Fokus des Platform-as-a-Service-Angebots von AppAgile liegt auf dem Betrieb von stateless
Cloud-Diensten mit persistenten Features. Um die vom Hersteller genannten Vorteile der Plattform,
wie Skalierbarkeit, Schnelligkeit und Flexibilität optimal nutzen zu können, empfiehlt sich der Einsatz
von weitestgehend statuslosen Microservice-Architekturen. Die persistenten Features bestehen kurz
zusammengefasst aus transparent nutzbarerem NFS-Storage für die Persistenz, sowie dem Support
ausgewählter Datenbankprodukte (u.a. Hadoop, mongoDB, PostgreSQL und standalone Cassandra).
Hinweis: Das für die Realisierung von QS-AV ausgewählte Produkt DataStax Enterprise basiert
vereinfacht auf dem Prinzip, dass Datenbankabfragen durch einen optimierten DataStaxeigenen Cassandra-Treiber im Zusammenspiel mit dem Spark-Framework verteilt im DatenbankCluster ausgeführt werden. Die Abfragen werden dabei automatisch so zerlegt und auf die einzelnen
Knoten im Cluster verteilt, dass Teilergebnisdaten parallel und gezielt auf denjenigen Knoten, auf
denen die betreffenden Partitionen tatsächlich gespeichert sind, ermittelt werden. Die Teilergebnisse
werden weitestgehend vorgefiltert und transformiert, bevor sie im letzten Schritt zusammengeführt
werden und das Gesamtergebnis der Abfrage ermittelt wird.
Im Zusammenspiel von DataStax Enterprise mit AppAgile als Plattform ergeben sich folgende
Spannungsfelder:
Architekturentscheidungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 42 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
 Die beiden Kernkomponenten der DSE Analytics Nodes - Cassandra DB und der SparkContext - können nicht auf verschiedene Pods verteilt werden (Mischung von stateless und
stateful Services = Microservices Anti-Pattern).
 DataStax als Hersteller empfiehlt allgemein, einen Baremetal-Betrieb virtualisierten Lösungen
vorzuziehen.
 Um die Performance-Vorteile der verteilten Verarbeitung effizient schöpfen zu können,
müssen die den einzelnen Knoten im Cluster zugeordneten Datenpartitionen für diese lokal
zugreifbar sein. Anstatt der standardmäßig in der AppAgile-Plattform eingesetzten NFSStorages wird die Anbindung lokaler SSDs von DataStax dringend empfohlen.
Bei einem Review der QS-AV Architektur im Januar 2017 wurde unter Einbeziehung des Teilprojekts
Transition und BzS aus Gründen der Vereinheitlichung im Projekt MaB40K AppAgile als Plattform für
den Betrieb von QS-AV festgelegt. Die für QS-AV gewählte Architektur wurde auf Basis dieser
Entscheidung adaptiert und umgesetzt.
3.4.1 Datenhaltungsschicht (DSE-Ring)
Die QS-AV Datenhaltung und die Verarbeitung der Datenbankoperationen erfolgt verteilt in einem
DataStax Analytics Cluster (DSE-Ring), basierend auf DataStax Enterprise. Der DSE-Ring besteht
softwareseitig aus n DataStax Analytics Nodes, die via Konfiguration verbunden werden. Bestandteile
dieser sind u.a. Cassandra als verteiltes Datenbanksystem, sowie das Apache Spark Framework für
performante Datenbankoperationen im Cluster.
Die DataStax Agent Software wird eingesetzt, um die Kommunikation mit dem DataStax OpsCenter
(s.u.) zur betrieblichen Verwaltung und Überwachung zu ermöglichen.
Die DataStax Analytics Nodes werden zusammen mit dem DataStax Agent in einem Docker Container
ausgeliefert und als Pods betrieben („DSE-Pods“).
3.4.1.1 Vorteile der Architektur
 Infrastrukturelle Trennung der Datenhaltungsschicht von Applikations- und
Präsentationsschicht
 Einfach erweiterbares Datenbankcluster mit verteiltem Analyse-Framework mit folgenden
Vorteilen: Einfach skalierbar, hoch verfügbar, performant auch bei großem Datenvolumen
 Datenbank-Upgrades und –Patches sind im laufenden Betrieb möglich (Rolling Upgrade)
3.4.2 Applikationsschicht (QS-AV Applikation und Timebucket-Switcher)
Die proprietäre QS-AV Applikation enthält die spezifische Geschäftslogik zur Qualitätssicherung des
Automatischen Verfahrens. Sie wird auf einem eigenen Pod ausgeliefert und gestartet („BookkeeperPod“) und an den Spark Context des DSE-Rings gebunden. Die spezifischen Datenbankanfragen
können so durch das Spark Framework performanceoptimiert im Cluster verteilt abgearbeitet werden.
Die QS-AV Applikation ist in zwei Teilkomponenten aufgeteilt: QS-FzG (Qualitätssicherung der
Fahrzeuggeräte) und QS-Erh (Qualitätssicherung der Erhebung).
Beim Timebucket-Switcher handelt es sich um eine proprietäre Applikation, die folgende Aufgaben
erfüllt: Der Timebucket-Switcher generiert 1. in konfigurierbaren Abständen neue zeitbasierte Partition
Keys (Timebuckets) für Cassandra Tabellen und fungiert 2. als Taktgeber für die Abarbeitung der
Jobs innerhalb der QS-AV Applikation, die durch das Ändern der Timebuckets getriggert werden.
Die Schnittstelle zwischen Timebucket-Switcher und QS-AV Applikation ist die gemeinsame
Datenhaltung (DSE-Ring). Der Timebucket-Switcher schreibt direkt per CQL in die CassandraDatenbank, die QS-AV Applikation greift via Spark zu (Schnittstellentabellen). 
Architekturentscheidungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 43 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3.4.2.1 Vorteile der Architektur
 Infrastrukturelle Trennung der Applikationsschicht von Datenhaltungs- und
Präsentationsschicht
 Die Datenbankanfragen der QS-AV Applikation werden im Cluster verteilt durch das Spark
Framework optimiert ausgeführt. Dies ermöglicht hohe Performanzwerte bei gleichzeitig
niedrigem Netzwerk-Traffic.
 Applikations-Updates erfordern keine Downtime der Persistenz (im Regelfall).
 Die Trennung der schlanken Timebucket-Switcher Applikation von der QS-AV Applikation soll
gewährleisten, dass Partitionen innerhalb der Cassandra DB auch bei einem Ausfall oder
einer Störung der QS-AV Applikation nicht überlaufen können und die Taktung
aufrechterhalten bleibt.
 Ein Upgrade der QS-AV Applikation beschränkt sich auf das Deployment des BookkeeperPods (insbesondere für Regeländerungen und ggf. verkürzte Release-Prozesse relevant).
3.4.3 Applikationsschicht: QS-AV Webservice-Applikation
Mit der QS-AV Webservice-Applikation wird eine Web-Applikation zur Exposition von Webservices
u.a. zur Realisierung der Schnittstelle 884 zu Cognos zur Verfügung gestellt. Die Authentifizierung und
Autorisierung erfolgen gegen das TC-Active Directory anhand der darin verwalteten Benutzer und
Gruppen. Die QS-AV Webservice-Applikation wird in einem Docker Container ausgeliefert und im
„Webservice-Pod“ betrieben.
Die QS-AV Webservice-Applikation greift per CQL (Cassandra Treiber) auf die Persistenz (DSE-Ring)
zu.
3.4.3.1 Vorteile der Architektur
 Infrastrukturelle Trennung der Applikationsschicht von Datenhaltungs- und
Präsentationsschicht
 Infrastrukturelle Trennung der Schnittstellen-Komponente (Webservice) von der fachlichen
Applikationslogik
 Zentrale Berechtigungsverwaltung
 Applikations-Updates erfordern keine Downtime der Persistenz (im Regelfall).
 Ein Upgrade der QS-AV Webservice-Applikation beschränkt sich auf das Deployment des
Webservice-Pods.
3.4.4 Präsentationsschicht: Gesundheitsakte
Mit der Gesundheitsakte wird eine Web-Applikation zum einfachen und schnellen Abruf von
Informationen zu einem Fahrzeuggerät zur Verfügung gestellt. Sie ist als Subkomponente (UI) von
QS-FzG eingeordnet. Die Authentifizierung und Autorisierung erfolgen gegen das TC-Active Directory
anhand der darin verwalteten Benutzer und Gruppen. Die Gesundheitsakte wird in einem Docker
Container ausgeliefert und im „Gesundheitsakte-Pod“ betrieben.
3.4.4.1 Vorteile der Architektur
 Infrastrukturelle Trennung der Präsentationsschicht von Applikations- und
Datenhaltungsschicht
 Zentrale Berechtigungsverwaltung
 Ein Upgrade der Gesundheitsakte-Applikation beschränkt sich auf das Deployment des
Gesundheitsakte-Pods.
Architekturentscheidungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 44 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3.4.5 Präsentationsschicht / Data-Science (Zeppelin)
Mit Apache-Zeppelin werden Web-Notebooks mit Zugriff auf die Daten im DSE-Ring zu Analyse- und
Reporting-Zwecken für die Fachbereiche zur Verfügung gestellt. Mitarbeiter des Regelbetriebs
erhalten zusätzlich die Berechtigung, technische Konfigurationen und Schwellwerte anzupassen. Die
Authentifizierung und Autorisierung erfolgen unter Verwendung des Shiro-Frameworks gegen das
TC-Active Directory anhand der darin verwalteten Benutzer und Gruppen. Zeppelin wird in einem
Docker Container inklusive vorkonfigurierter Spark- und Cassandra-Interpreter ausgeliefert und im
„Zeppelin-Pod“ betrieben.
Hinweis: Für die korrekte Darstellung der Web-Notebooks ist der Einsatz von Firefox auf ClientSeite erforderlich.
3.4.5.1 Vorteile der Architektur
 Infrastrukturelle Trennung der Präsentationsschicht von Applikations- und
Datenhaltungsschicht
 Zentrale Berechtigungsverwaltung
 Die Datenbankanfragen, die via Spark-Interpreter abgesetzt werden, werden im Cluster
verteilt und optimiert ausgeführt. Dies ermöglicht hohe Performanz bei gleichzeitig niedrigem
Netzwerk-Traffic.
 Ein Upgrade der Zeppelin-Version beschränkt sich auf das Deployment des Zeppelin-Pods.
3.4.6 Betriebliche Verwaltung und Überwachung des DSE-Rings
(OpsCenter)
Mit dem DataStax OpsCenter wird eine Web-UI mit Funktionalitäten zur Verwaltung und
Überwachung des DSE-Rings im Betrieb zur Verfügung gestellt. Das DataStax OpsCenter
kommuniziert mit Hilfe der DataStax Agents mit den DSE Analytics Nodes. Dabei werden u.a.
Informationen zur Ermittlung des Cluster-Gesamtstatus übertragen. Bei Bedarf können
Konfigurationsparameter im laufenden Betrieb angepasst und im Cluster propagiert werden. Des
Weiteren stehen u.a. Cluster-Repair-Funktionalitäten sowie Übersichten zu Auslastungen und HotSpots zur Verfügung. Der DataStax OpsCenter Server wird in einem eigenen Docker Container
ausgeliefert und im „OpsCenter-Pod“ betrieben.
3.4.6.1 Vorteile der Architektur
 Betriebliches Tool vom Hersteller zur Überwachung des Cluster-Status inklusive RepairFunktionalitäten
 Cluster-Optimierung im laufenden Betrieb möglich
 Schlankes Kommunikationsprotokoll
3.4.7 Anbindung an Active Directory
Zur Authentifizierung und Autorisierung von Benutzern und technischen Benutzern werden die QS-AV
Komponenten an das TC-Active Directory angebunden.
3.4.7.1 Vorteile
 Nutzung der zentralen IAM IT-Infrastruktur
 Nutzung der zentralen Benutzerverwaltungs- und Berechtigungsprozesse
 Berechtigungen können zentral auditiert werden
Architekturentscheidungen
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 45 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3.4.8 Überblick und weiterführende Informationen
Die einzelnen Teilkomponenten/Pods werden in der folgenden Tabelle aufgelistet. Weiterführende
Informationen finden Sie in den referenzierten Kapiteln.
Komponente Kurzbeschreibung Anzahl Kapitel
DSE-Pod n DSE-Pods bilden den DSE-Ring n 4.2.2
OpsCenter-Pod Web-UI zur Überwachung und Verwaltung des DSE-Rings 1 4.2.8
Bookkeeper-Pod Enthält die Software-Komponenten mit der QS-AVspezifischen Applikationslogik (QS-AV
Applikationsschicht)
1 4.2.3
TimebucketSwitcher-Pod
Enthält mit dem Timebucket-Switcher den Generator für
Partition Keys (Timebuckets) und Taktgeber für die
Abarbeitung der QS-AV Applikationslogik
1 4.2.4
Webservice-Pod Enthält eine Web-Applikation zur Exposition von
Webservices u.a. zur Realisierung der Schnittstelle 884
QS-AV  Cognos
1 4.2.5
GesundheitsaktePod
Stellt eine Web-UI zum berechtigungsgesteuerten Abruf
von FzG-Informationen zur Verfügung
1 4.2.6
Zeppelin-Pod Stellt berechtigungsgesteuerten Spark-Zugriff auf den
DSE-Ring für Web-Notebooks zur Verfügung.
1 4.2.7
Tabelle 43: Teilkomponenten (Pods)
DataStax Enterprise bietet mit Cassandra allgemeine DB-Clusterfunktionalitäten, wie z.B.
automatische Replikation (gemäß Replikationsfaktor), Lastverteilung, Datenverteilung, Einhaltung der
Konsistenzlevel. Die Konfiguration dieser Features erfolgt im Rahmen der Konfiguration der einzelnen
Knoten. In einigen Fällen (z.B. Consistency Level) können die konfigurierten Werte für einzelne
Requests von der Applikation überschrieben werden.
Spark-Requests, die über die QS-AV Applikation an das DB-Cluster gestellt werden, werden ebenfalls
über das Cluster verteilt.
Weiterführende Informationen zu DataStax Enterprise sowie AppAgile entnehmen Sie bitte der
jeweiligen Produktdokumentation.
Details zu Einrichtung und Konfiguration werden in den Installations- und Betriebshandbüchern
dokumentiert.
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 46 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4 Systemarchitektur
In diesem Kapitel wird die QS AV-Systemarchitektur untergliedert nach fachlichem und technischem
Kontext beschrieben.
4.1 Fachlicher Kontext
Der an dieser Stelle gebotene Überblick stellt die Top-Level-Beschreibung der realisierten Systeme im
fachlichen Kontext dar. Er dient als Grundlage für das weitere Verständnis der Detailspezifikation. Die
detaillierte fachliche Spezifikation entnehmen Sie dem [3] Lastenheft QS-FzG und Lastenheft QSErhebung.
4.1.1 Allgemein
Die Qualitätssicherung des Automatischen Verfahrens basiert kurz zusammengefasst auf der
Verarbeitung und regelbasierten Verarbeitung und Bewertung von eingehenden Betriebs- und
Leistungsdaten parallel zum Leistungsdatenstrom.
Abbildung 4: Systemüberblick im fachlichen Kontext
Eingabe
• Betriebs- und Leistungsdaten via
Schnittstellen
Verarbeitung
• Anwendung der
Qualitätssicherungsalgorithmen
Ausgabe
• Bereitstellung der Ergebnisse zur
Modellverbesserung, Monitoring
und Reporting
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 47 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.1.2 Eingabe
Die Eingabedaten (Betriebs- und Leistungsdaten) werden via Schnittstellen von folgenden Systemen
zur Verfügung gestellt und in der QS-AV Datenhaltung gespeichert:
 GDBS Betriebsdaten (Grunddaten)
 DM
 ES Leistungsdaten (Daten aus dem Leistungsdatenstrom)
 EDM
Abbildung 5: Eingehende Schnittstellen
Weiterführende Informationen zu den Schnittstellen finden Sie im Kapitel 6 Schnittstellensicht
und den entsprechenden Schnittstellenspezifikationen (siehe Referenzdokumente).
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 48 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.1.3 Verarbeitung
Auf Basis der Eingabedaten werden die Qualitätssicherungsalgorithmen in Form von Regeln
angewendet, getrennt nach:
 QS-FzG: Fahrzeuggerätspezifische Qualitätssicherung
 QS-Erh: Erhebungsspezifische Qualitätssicherung
Die Verarbeitung innerhalb der QS-FzG und der QS-Erh verläuft dabei nach folgendem Schema:
Abbildung 6: Verarbeitungsprinzip
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 49 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.1.4 Ausgabe
Die Ergebnisse der Qualitätssicherung, Monitoring- und Reporting-Informationen werden für folgende
Systeme zur Verfügung gestellt:
 GDBS
 MonP
 Cognos
Die System-Logs werden zentral archiviert:
 LogArchiv Server
Abbildung 7: Ausgehende Schnittstellen
Weiterführende Informationen zu den Schnittstellen finden Sie im Kapitel 6 Schnittstellensicht
und den entsprechenden Schnittstellenspezifikationen (siehe Referenzdokumente).
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 50 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.1.5 Kontext AV
Die folgende Lösungsskizze des Architekturmanagements zeigt die Einordnung der QS AVKomponenten im übergeordneten Kontext des AV-Projekts.
Abbildung 8: Lösungsskizze Projekt AV
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 51 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
In der folgenden Abbildung ist der QS-AV-relevante Ausschnitt im Lösungskontext vergrößert
dargestellt.
Abbildung 9: Lösungsskizze Detailansicht QS-AV
Weiterführende Informationen zu den Schnittstellen finden Sie im Kapitel 6 Schnittstellensicht
und den entsprechenden Schnittstellenspezifikationen (siehe Referenzdokumente).
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 52 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2 Technischer Kontext
Die folgende Abbildung stellt die vereinfachte AV Architektur- und Datenflussübersicht dar.
Abbildung 10: Vereinfachte AV Architektur- und Datenflussübersicht
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 53 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
In der Abbildung 10: Vereinfachte AV Architektur- und Datenflussübersicht sind im oberen Bereich die
QS-AV-spezifischen Komponenten vereinfacht dargestellt.
Darunter sind von links nach rechts in Datenflussrichtung die Komponenten des Leistungsdatenstroms
dargestellt, bestehend aus DM, ES und EDM. Die Darstellung der Schnittstellen erfolgt durch Pfeile,
wobei der Leistungsdatenstrom durch rote Pfeile und Hilfsdatenströme durch blaue Pfeile visualisiert
werden (nach Datenflussrichtung).
4.2.1 Übersicht
Abbildung 11: QS-AV Architekturkomponenten
In den folgenden Unterkapiteln wird die Architektur der einzelnen Komponenten im Detail dargestellt.
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 54 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2.2 DSE-Ring (Datenhaltung)
Der DSE-Ring besteht aus 1-n durch Konfiguration verbundene DSE-Pods.
Abbildung 12: Aufbau DSE-Pod
4.2.2.1 AppAgile Node
Alle DSE-Pods laufen auf AppAgile Worker-Nodes als Docker Container. Der AppAgile Worker-Node
stellt u.a. die Docker Engine und ein gemountetes Dateisystem (NFS oder SSD) für die Persistenz zur
Verfügung.
4.2.2.2 Docker Container
Die DSE-Pods enthalten die DataStax Komponenten DSE Analytics Node (inkl. Cassandra DB) und
DataStax Agent. Im Rahmen der Installation werden die DSE-Pods so konfiguriert, dass sie einen
DSE-Ring bilden und via DataStax Agent mit dem OpsCenter-Server kommunizieren können.
Details zur Konfiguration entnehmen Sie bitte dem Referenzdokument [16] [A_QSAV_HBI_00]
QS-AV Installationshandbuch.
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 55 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2.2.2.1 DSE Analytics Node
Die DSE Analytics Nodes bestehen aus folgenden Komponenten:
 Spark Worker
 Spark Master
(hier startet der Spark-Driver-Prozess)
 Cassandra File System (CFS)
 DataStax Enterprise File System (DSEFS)
 Cassandra DB
Die DSE Analytics Nodes können in unterschiedlicher Basiskonfiguration laufen:
Aspekt Modus Initiale Verteilung Beschreibung
ClusterTopologie
Seed oder Non-Seed Seed: zusätzlich in
einem5 DSE-Pod
Non-Seed: in allen
DSE- Pods
Seed: DSE Analytics Node, der für
das Bootstrapping neuer Nodes im
Cluster verantwortlich ist.
Wichtig: Der Ausfall des Seeds hat
keinen Einfluss auf die
Funktionsfähigkeit des Clusters an
sich, sondern lediglich auf die
Erweiterbarkeit des Clusters im
Ausfallzeitraum (Bootstrapping).
Spark Master oder Worker Master: zusätzlich
in einem DSE-Pod
Worker: in allen
DSE-Pods
Ein Spark Master
(Failover-sicher durch Session Replication)
Master = Der DSE Analytics Node,
auf dem der Spark Master läuft. Der
Spark Master verteilt die SparkApplikationsanfragen auf die Spark
Worker im Cluster. Der Spark-Master
kann im Cluster wandern.
Tabelle 44: DSE Analytics Node Aspekte
Hintergrund: DSE Analytics Nodes können als reine Spark Worker und einer im Cluster
zusätzlich als Spark Master laufen. Der Spark Master verteilt die Spark-Applikationsanfragen auf
die Spark Worker im Cluster. Die Spark Worker sind für die eigentliche Verarbeitung bzw. die
Transaktionen mit der Cassandra-Datenbank verantwortlich. Die folgende Abbildung stellt dies
schematisch dar.

5
Standardmäßig wird ein Seed konfiguriert. Der Einsatz von zwei Seeds ist technisch möglich.
Persistenz & Logs auf gemountetem FS
Datenzugriffschicht (Apache Spark)
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 56 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Abbildung 13: Verteilung im Cluster, schematische Darstellung (DataStax Enterprise - Spark Integration)6
4.2.2.2.2 DataStax Agent
Der DataStax Agent kommuniziert mit dem DSE Analytics Node innerhalb des gleichen Pods via CQL
(native Zugriffe auf die Cassandra DB) sowie via JMX (JVM Monitoring). Nach außen kommuniziert
der DataStax Agent via STOMP, CQL und JMX mit dem OpsCenter-Server. Der Agent übermittelt
nicht nur selbstständig Statusinformationen, sondern reagiert auch auf konkrete Anfragen und
Befehle, die vom OpsCenter-Server gesendet werden.

6 Quelle: https://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/spark/sparkIntro.html
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 57 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2.3 QS-AV Applikation inklusive Bookkeeper (Applikation)
Abbildung 14: Aufbau Bookkeeper-Pod
4.2.3.1 AppAgile Node
Der Bookkeeper-Pod läuft auf einem AppAgile Worker-Node als Docker Container.
4.2.3.2 Docker Container
Die im Bookkeeper-Pod enthaltenen Komponenten werden im Folgenden beschrieben.
4.2.3.2.1 QS-AV Schema
Enthält die DDLs für die aktuell eingesetzte Version QS-AV Applikation (zum Aufsetzen/Patchen der
Datenbank).
4.2.3.2.2 QS-AV Applikation
Die eigentliche Geschäftslogik von QS-FzG und QS-Erh ist in einer proprietären Spark-Applikation,
der „QS-AV Applikation“, implementiert. Sie wird mit allen Dependencies zur Verfügung gestellt.
Gestartet wird die Applikation durch Übermitteln der Applikation an den Spark-Kontext des DSEClusters mittels dse spark-submit7
.
Mit dem Übermitteln wird die QS-AV Applikation in die Laufzeitumgebungen von Spark-Master und
Spark-Workern zur Ausführung im Cluster übergeben.
Weiterführende Informationen zur QS-AV Applikation und dem Bookkeeper finden Sie in den
Kapiteln 5.2 QS-AV Applikation und 5.1 Schwerpunktkapitel: Bookkeeper (qs-common).

7 Weitere Informationen: https://spark.apache.org/docs/1.6.1/submitting-applications.html
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 58 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2.3.2.3 DSE Analytics Node (Library)
Der Bookkeeper-Pod-spezifische Docker Container enthält neben der proprietären
Applikationskomponenten auch eine Distribution des DataStax Enterprise Analytics Node zur Nutzung
als Bibliothek. Wichtig: Der DataStax Analytics Node wird auf dem Bookkeeper-Pod nicht als Teil des
DSE-Rings konfiguriert und auch nicht gestartet.
4.2.3.2.4 Persistent Volume Claim (PVC)
Gemountetes Dateisystem zur persistenten Speicherung von Konfigurationseinstellungen.
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 59 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2.4 Timebucket-Switcher (Applikation)
Abbildung 15: Aufbau TB-Switcher-Pod
4.2.4.1 AppAgile Node
Der TB-Switcher-Pod läuft auf einem AppAgile Worker-Node als Docker Container.
4.2.4.2 Docker Container: Timebucket-Switcher Applikation
Die standalone Scala-Applikation Timebucket-Switcher enthält die Logik zur automatischen
Generierung zeitbasierter Partition Keys (Timebuckets) für die Cassandra Tabellen innerhalb des
DSE-Rings und fungiert als Taktgeber für die Abarbeitung der Jobs innerhalb der QS-AV Applikation,
die durch das Ändern der Timebuckets getriggert werden.
Die Schnittstelle zwischen Timebucket-Switcher und QS-AV Applikation ist die gemeinsame
Datenhaltung (DSE-Ring). Der Timebucket-Switcher schreibt direkt per CQL in die CassandraDatenbank des DSE-Rings.
Weiterführende Informationen zum Timebucket-Switcher finden Sie im Kapitel 5.3 TimebucketSwitcher Applikation.
4.2.4.2.1 Persistent Volume Claim (PVC)
Gemountetes Dateisystem zur persistenten Speicherung von Konfigurationseinstellungen.
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 60 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2.5 QS-AV Webservice-Applikation (Applikation)
Abbildung 16: Aufbau Webservice-Pod
4.2.5.1 AppAgile Node
Der Webservice-Pod läuft auf einem AppAgile Worker-Node als Docker Container.
4.2.5.2 Docker Container: QS-AV Webservice-Applikation
Die QS-AV Webservice-Applikation stellt eine Web-Applikation zur Exposition von Webservices u.a.
zur Realisierung der Schnittstelle 884 zu Cognos zur Verfügung.
Die QS-AV Webservice-Applikation greift per CQL (Cassandra Treiber) auf die Persistenz (DSE-Ring)
zu.
Weiterführende Informationen zur QS-AV Webservice-Applikation finden Sie im Kapitel
5.4 QS-AV Webservice-Applikation.
4.2.5.2.1 Persistent Volume Claim (PVC)
Gemountetes Dateisystem zur persistenten Speicherung von Konfigurationseinstellungen.
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 61 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2.6 Gesundheitsakte (Präsentation)
Abbildung 17: Aufbau Gesundheitsakte-Pod
4.2.6.1 AppAgile Node
Der Gesundheitsakte-Pod läuft auf einem AppAgile Worker-Node als Docker Container.
4.2.6.2 Docker Container: Web-Applikation und HTTP-Server
Die auf Python/CGI basierende Web-Applikation „Gesundheitsakte“ wird inklusive benötigter
Drittanbieter-Bibliotheken auf dem Gesundheitsakte-Pod ausgeliefert. Die Web-Applikation wird mit
Hilfe eines vorkonfigurierten Apache HTTP-Servers zur Verfügung gestellt. Authentifizierung und
Autorisierung erfolgen konfigurierbar per LDAP gegen Active Directory.
Weiterführende Informationen zur Gesundheitsakte finden Sie im Kapitel5.6 Gesundheitsakte.
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 62 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2.7 Zeppelin (Präsentation)
Apache Zeppelin bietet autorisierten Benutzern aus den Fachbereichen Web-Notebooks mit Zugriff
auf den DSE-Ring zu Analyse- und Auswertungszwecken.
Abbildung 18: Einordnung und Aufbau Zeppelin-Pod
4.2.7.1 AppAgile Node
Der Zeppelin-Pod läuft auf einem AppAgile Node als Docker. Der AppAgile Node stellt u.a. die Docker
Engine und ein gemountetes Dateisystem für allgemeine Konfigurationsdaten und die WebNotebooks, die außerhalb des Containers gesichert werden sollen, zur Verfügung.
Authentifizierung und Autorisierung erfolgen zweistufig:
1. Zwischen Benutzer und Zeppelin: Konfigurierbar per LDAP gegen Active Directory gemäß
Rollen- und Rechtekonzept
2. Zwischen Zeppelin und dem DSE-Ring (Cassandra): Konfigurierbar durch
Zeppelin-Administratoren innerhalb der Interpreter anhand technischer Cassandra-Benutzer
gemäß Rollen- und Rechtekonzept
4.2.7.2 Docker Container: Apache Spark, Zeppelin
Der Zeppelin-spezifische Docker Container enthält folgende Komponenten:
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 63 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
 Distribution des DataStax Enterprise Analytic Nodes zur Nutzung als Bibliothek.
Wichtig: Der DataStax Analytics Node wird auf dem Bookkeeper-Pod nicht als Teil des DSERings konfiguriert und auch nicht gestartet.
 Apache Zeppelin-Installation, u.a. inklusive:
o Jetty (Webserver)
o Spark- und Cassandra-Interpreter
 Gemountetes Dateisystem zur persistenten Speicherung von Konfigurationseinstellungen und
Web-Notebooks
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 64 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2.8 OpsCenter (Betriebliche Verwaltung und Überwachung)
Das DataStax OpsCenter bietet autorisierten Benutzern ein Web-UI zur Wartung und zum Monitoring
des DSE-Rings im Betrieb.
Hinweis: Benutzer- und Zugriffsrechte werden direkt im OpsCenter konfiguriert.
Tabelle 45: Einordnung und Aufbau OpsCenter-Pod
4.2.8.1 AppAgile Node
Der OpsCenter-Pod läuft auf einem AppAgile Node als Docker Container.
4.2.8.2 Docker Container: DataStax OpsCenter
Der OpsCenter-spezifische Docker Container enthält folgende Komponenten:
 DataStax OpsCenter Server (konfiguriert zum Zugriff auf den DSE-Ring)
o OpsCenter Daemon (Interaktion mit dem Cassandra Cluster über die auf den DSE
Analytics Nodes installierten DataStax Agents, exponiert Management- und
Monitoring- Funktionalitäten per API für das Web-UI)
o HTTP-Server (Bereitstellung des Web-UI)
Folgende Protokolle werden zur Kommunikation mit dem DSE-Ring eingesetzt: STOMP, JMX, CQL
Die folgende Grafik visualisiert die Schnittstellen und Kommunikationsprotokolle des OpsCenter
Servers:
Abbildung 19: OpsCenter Server - Schnittstellen und Kommunikationsprotokolle8

8 Quelle: http://www.datastax.com/dev/blog/opscenter-an-architecture-overview
Systemarchitektur
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 65 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4.2.9 Heißer DSE-Ring – Kalter DSE-Ring
Hinweis: Konzepte, die neben dem ersten DSE-Ring für OLTP-Workloads („heißer Ring“), wie oben
beschrieben, den Aufbau und Betrieb eines zweiten DSE-Rings für OLAP-artige Queries („kalter
Ring“) beinhalten, wurden bislang nicht verabschiedet / umgesetzt.
Hintergrund: Ansatz eines Kalten-Ring-Konzepts ist es, den heißen Ring für alle schreibenden
Operationen im Regelbetrieb (OLTP) sowie als Basis für die QS-AV Applikation und ihre
umfangreichen Qualitätssicherungsalgorithmen vorzubehalten.
Für alle OLAP-artigen Workloads auf den Daten in der Persistenzschicht (wie z.B. FB-Analysen via
Zeppelin, Einzelfallrecherchen, Problem- und Incident-Management, Erzeugung von Reports) könnte
ein zweiter „kalter“ Ring mit den replizierten Daten des heißen Rings aufgebaut werden. Die
Replikation der Daten darf dabei nur unidirektional vom heißen Ring in Richtung des kalten Rings
erfolgen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 66 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5 Komponentensicht
In diesem Kapitel werden die Softwarekomponenten und Funktionen der proprietären Komponenten
 QS-AV Applikation inklusive Bookkeeper
 Gesundheitsakte
und
 Timebucket-Switcher
beschrieben.
Hinweis: Einen Überblick über die Komponenten finden Sie in den Kapiteln 3.4 Grobarchitektur
und 4 Systemarchitektur (Unterkapitel 4.2 Technischer Kontext).
5.1 Schwerpunktkapitel: Bookkeeper (qs-common)
Der Bookkeeper ist eine Subkomponente der QS-AV Applikation (Maven-Projekt: qs-common).
Aufgrund seiner zentralen Aufgabe als Steuerkomponente für den Programmablauf und zum
besseren Verständnis der Bezüge in den Folgekapiteln wird dieses Schwerpunktkapitel vorangestellt.
Die Qualitätssicherungsalgorithmen der QS-AV Applikation basieren auf der mehrstufigen
Transformation von Eingangsdaten aus dem Leistungsdatenstrom: Die Daten werden in der QS-AV
Applikation normiert, korreliert, verdichtet und bewertet. Während der Korrelierungs-, Verdichtungsund Bewertungsoperationen müssen Daten aus unterschiedlichen Datenquellen miteinander in
Beziehung gebracht und ausgewertet werden. Die Ergebnisse der Verarbeitungsoperationen werden
in Zieltabellen und über ausgehende Schnittstellen (Ausgangsdatenströmen) zur Verfügung gestellt.
Die Bookkeeper-Komponente ist innerhalb der QS-AV Applikation für die Ablaufsteuerung zuständig.
Die Verarbeitung erfolgt innerhalb sogenannter RuleJobs (im weiteren Verlauf auch kurz als Job
bezeichnet), für die sowohl inhaltliche als auch zeitliche Abhängigkeiten konfiguriert werden können.
5.1.1 Hauptmerkmale des Bookkeepers
 Implementierung in Scala (100%)
 Verwendung von UTC
 Single-Threaded (sequenzielle Ausführung der Jobs)
 Trennung der Steuerungslogik von der Verarbeitungslogik
 Konfigurative Ablaufsteuerung
 Intervallverarbeitung von nach Zeitscheiben partitionierten Daten aus CassandraTabellen (=Timebuckets): Konfiguration der Abhängigkeiten von Quelltabelle/n 
Verarbeitung (RuleJobs)  Zieltabelle/n
 Signalgesteuerte Verarbeitung: Konfiguration von Signal-Events als Vor- oder
Nachbedingungen
 Zeitlich gesteuerte Verarbeitung: Konfiguration von (frühesten) Startzeiten
 Informationen zur Konfiguration entnehmen Sie dem Kapitel 5.1.5 Konfiguration
 RuleJobRepository mit Wiederaufsatzfunktion nach Applikationsneustart
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 67 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.1.2 Bookkeeper Glossar
In diesem Abschnitt werden die wichtigsten Begriffe und Konzepte des Bookkeepers erläutert.
Lesehinweis: Das alphabetisch sortierte Glossar kann im Lesefluss zunächst überblättert werden und
zum besseren Verständnis der folgenden Kapitel immer wieder herangezogen werden.
Anchor-Timebucket
Das Anchor-Timebucket ist das verankernde  Timebucket, für die Beschreibung jedes konkreten
RuleJobs, der vom Bookkeeper generiert wird.
Für jeden RuleJob gibt es genau ein Anchor-Timebucket, das einen eindeutigen Ausgangspunkt
bestimmt, wie aus dem zugrundeliegenden  Job-Schema alle konkreten Timebuckets der Vor- und
Nachbedingungen für den zu generierenden RuleJob errechnet werden können.
Im einfachsten Fall entspricht das Anchor-Timebucket dem Timebucket der Vorbedingung. Dies ist
genau dann der Fall, wenn es genau ein  Timebucket-Schema als Vorbedingung gibt, der Offset
dieses  Timebucket-Schemas 0 ist und die TruncationDimensions von  Job-Schema und
 Vorbedingung gleich sind.
Gelten mehrere  Vorbedingungen, so wird das jeweils früheste relevante  Timebucket als
Anchor-Timebucket gesetzt. Aus diesem errechnet sich auch die  Referenzuhrzeit.
Job (RuleJobExecutor)
Ein RuleJobExecutor oder auch kurz Job im Sinne des Bookkeepers ist eine durch den
Bookkeeper ausführbare Scala-Klasse, in der Fachlogik implementiert ist, und die das Trait
RuleJobExecutorHeavy implementiert. Die konkreten Klassen sind je nach Zugehörigkeit in den
entsprechenden Teilkomponenten QS-Fzg oder QS-Erh implementiert.
Job-Daten (RuleJob)
Ein RuleJob beschreibt die Datengrundlage, die als Parameter dem dazugehörigen
RuleJobExecutor ( Job ) übergeben wird. Die Erzeugung obliegt dem Bookkeeper unter
Verwendung eines  Anchor-Timebuckets und eines RuleSchemas ( Job-Schema) aus der
RuleConfig ( Job-Konfiguration).
Job-Konfiguration (RuleConfig)
Die Job-Konfiguration enthält alle  Job-Schemata,  Timebucket-Schemata sowie die 
Timebucket-Konfiguration.
Job-Schema (RuleSchema)
Jeder  Job muss, damit er ausgeführt werden kann, innerhalb der  Job-Konfiguration
(RuleConfig) anhand eines RuleSchemas konfiguriert werden.
Im RuleSchema können neben dem Ausführungsintervall („TruncationDimansion“) bzw. -zeitpunkt
auch  Vor- und  Nachbedingungen für die Ausführung eines konkreten  Jobs aus QS-FzG oder
QS-Erh festgelegt werden.
 Vor- und  Nachbedingungen werden durch  Timebucket-Schemata ( Timebucket-Schema)
definiert.
Weitere Informationen finden Sie in den Kapiteln 5.1.5.1 RuleConfig – Job-Konfiguration und
5.1.5.2 RuleConfigTimebuckets – Timebucket-Konfiguration. 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 68 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Jobkette
Eine Jobkette wird durch die Verknüpfung mehrerer Jobs anhand ihrer im jeweiligen  Job-Schema
definierten Vor- und Nachbedingungen gebildet.
Beispiel: Eine Jobkette, in der die Ausführung eines Jobs J2 von einem Job J1 abhängt, wird
sinngemäß folgendermaßen konfiguriert: Nachbedingung für Job1 = Vorbedingung für
Job2
Nachbedingung (postCondition)
Im Bookkeeper-Kontext kann ein  Timebucket-Schema als Nachbedingung innerhalb eines
 Job-Schemas festgelegt werden. Nachbedingungen dienen in erster Linie dazu  Jobketten
aufzubauen.
Referenzuhrzeit
Die Referenzuhrzeit ist ein Zeitpunkt, der ausgehend vom  Anchor-Timebucket berechnet wird.
Dabei wird zum Anchor-Timebucket-Timestamp das Ausführungsintervall laut  Job-Konfiguration
(TruncationDimension des RuleSchemas) addiert.
Beispiel:
Zeitstempel Anchor-Timebucket = 010617_1500
Ausführungsintervall des  Jobs = Hours(1)
Die Referenzuhrzeit ist 16:00 Uhr am 1. Juni 2017.
Die Referenzuhrzeit wird über das RuleJob-Objekt (konkreter  Job) zur Verfügung gestellt.
Hintergrund:  Jobs wie z.B. ProcessDSRC15 benötigen die Referenzuhrzeit zur Ermittlung von
Daten abhängig vom aktuellen Timebucket (Endzeitpunkt des Ausführungsintervalls).
Timebucket (Timebucket)
Die Verarbeitung innerhalb der QS-AV Applikation basiert auf der Partitionierung von Daten nach
Zeitscheiben, die in Blöcken („Mini-Batches“) abgearbeitet werden. Ein Timebucket besteht aus
einem Zeitstempel im Format „DDMMYY_hhmm“ und gehört immer zu einem spezifischen Datenstrom.
Der Datenstrom wird durch einen String bestimmt, der im Falle einer Cassandra-Tabelle deren
Namen enthält.
Der  Timebucket-Katalog aller Timebuckets mit ihren jeweils aktuellen Zeitstempeln wird in der
Cassandra DB gehalten.
Siehe auch  Timebucket-Schema.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 69 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Timebucket-Katalog (Tabelle en_q_timebuckets)
Der Timebucket-Katalog enthält für alle konfigurierten  Timebuckets die jeweils aktuellen
Timebucket-Zeitstempel. Er ist in der Tabelle en_q_timebuckets in der Cassandra-DB
gespeichert.
Werden Datensätze durch den LDS-Client (außerhalb von QS-AV) in nach Timebuckets
partitionierten Tabellen geschrieben, so muss zunächst der jeweils aktuelle Wert für den TimebucketZeitstempel dem Timebucket-Katalog entnommen werden. Der Timebucket-Zeitstempel wird dann als
(Teil des) Partition Key(s) gespeichert.
Der Timebucket-Katalog wird durch den  Timebucket-Switcher aktualisiert.
Hinweis: Werden Datensätze durch einen  Job (innerhalb von QS-AV) geschrieben, so ist die JobImplementierung selbst dafür verantwortlich, die gewünschten Ziel-Timebuckets für die zu
beschreibenden Cassandra-Tabellen zu bestimmen. Dafür stehen die Attribute aus den  Job-Daten
zur Verfügung. Dabei sind insbesondere das  Anchor-Timebucket, aber auch  Timebuckets der
 Vor- und Nachbedingungen relevant.
Timebucket-Konfiguration (RuleConfigTimebuckets)
Die Timebucket-Konfiguration enthält alle  Timebucket-Schemata. Sie wird für die
 Job-Konfiguration (Vor- und Nachbedingungen im  Job-Schema) sowie zur Konfiguration des
Timebucket-Switchers benötigt.
Timebucket-Schema (TimebucketSchema)
Ein Timebucket-Schema ist eine abstrakte Beschreibung eines  Timebuckets, innerhalb der
 Timebucket-Konfiguration. Für ein vollständige Beschreibung sind drei Angaben nötig:
1. Die TruncationDimension, die bestimmt, in welchem zeitlichen Abstand  Timebuckets aus
diesem Timebucket-Schema erzeugt werden können (z.B. alle 5 Minuten).
2. Ein Offset, das den Abstand zum  Anchor-Timebucket innerhalb eines  Job-Schemas
bestimmt.
3. Ein String, der den Datenstrom des erzeugten Timebuckets referenziert. Bei Timebuckets für
Cassandra-Tabellen ist hier der Cassandra-Tabellen-Name verzeichnet.
Formal können folgende drei Timebucket-Typen konfiguriert werden:
 Tabellen-Timebuckets: Timebucket, das auf eine konkrete Tabelle zeigt, z.B. LDS-Tabellen.
Die Daten der Tabellen werden via  Timebucket-Switcher Zeitscheiben zugeordnet und als
Timebuckets im  Timebucket-Katalog persistiert.
 Timer-Timebuckets: Besitzen keinen Bezug auf eine Tabelle. Timer-Timebuckets werden
zur reinen Zeitsteuerung von  Jobs, die keinerlei inhaltliche Abhängigkeiten zu anderen
Jobs oder Datenströmen als Vorbedingung haben, verwendet. Stattdessen wird ein 
Timebucket-Schema übergeben, welches ein Timer-Timebucket beschreibt. Der 
Timebucket-Switcher switcht die Timebuckets, die im  Timebucket-Katalog persistiert
werden.
 Signal-Timebuckets: Signal-Timebuckets sind Timebuckets, die als Vorbedingung für einen
zeitgesteuerten  Job fungieren, bzw. als Ergebnis (=Nachbedingung) eines  Jobs
erzeugt werden. Sie werden nicht vom Timebucket-Switcher betrachtet, da sie in der Kette
der Jobausführungen direkt ins Zustands-Log geschrieben und abgearbeitet werden.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 70 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Timebucket-Switcher
Die Timebucket-Switcher Applikation ist für die Aktualisierung („switch“) der Timebucket-Zeitstempel
im  Timebucket-Katalog zuständig. Die Zeitscheibengröße kann dabei je  Timebucket anhand
des  Timebucket-Schemas konfiguriert werden. Nach jeder Aktualisierung schreibt der TimebucketSwitcher einen entsprechenden Eintrag ins  Zustands-Log. Da sowohl die Datenpartitionierung als
auch die Ablaufsteuerung von den  Timebuckets abhängt, werden hohe
Verfügbarkeitsanforderungen an die Timebucket-Switcher Applikation gestellt. Weitere Details zum
Timebucket-Switcher finden Sie im Kapitel 5.3 Timebucket-Switcher Applikation.
Vorbedingung (preCondition)
Im Bookkeeper-Kontext kann ein  Timebucket-Schema als Vorbedingung innerhalb eines
 Job-Schemas festgelegt werden. Vorbedingungen definieren die Abhängigkeiten eines Jobs von
einem oder mehreren  Timebuckets.
Zustands-Log
Der Bookkeeper verwaltet ein Zustands-Log bestehend aus den drei Cassandra-Tabellen
bkpr_eventlog, bkpr_waiting_timebuckets und bkpr_rule_jobs.
Im Zustands-Log erfolgt die Speicherung und Protokollierung (Historie) von Events
(Statusübergängen), Job-Statusinformationen und –Daten, sowie Timebucket- Zuordnungen zu Jobs
(Vorbedingungen) und deren Status.
Das Zustands-Log fungiert als zentrale Informationsstelle und Schnittstelle zwischen Bookkeeper und
 Timebucket-Switcher Applikation.
Weitere Informationen finden Sie im Kapitel 5.1.6 Zustands-Log.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 71 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.1.3 Überblick und allgemeine Funktionsweise
Abbildung 20: Überblick Bookkeeper (Kontext)
In der obigen Abbildung sind mittig der Bookkeeper (Teil der QS-AV Applikation) und die umgebenden
Teil-/komponenten mit Fokus auf die Ablaufsteuerung dargestellt.
Der Bookkeeper enthält die Ablaufsteuerung („Wiederaufsatz und Job-Ausführung“) für die in den
Teilkomponenten QS-FzG und QS-Erh implementierten Jobs. Zur Steuerung durch den Bookkeeper
können sowohl inhaltliche Abhängigkeiten als auch zeitliche Abhängigkeiten konfiguriert werden.
Nähere Informationen zu den unterschiedlichen Abhängigkeiten finden Sie im Kontext der Kapitel
5.1.4 Wiederaufsatz und Job-Ausführung und 5.1.5 Konfiguration.
Die Erfüllung der zeitlichen Abhängigkeiten (Ausführungsintervalle) wird anhand der Systemzeit im
Zusammenspiel mit der aktuellen Zeitscheibe (Anchor-Timebucket) geprüft. Die inhaltlichen
Abhängigkeiten werden anhand des Zustands-Logs geprüft. Die Job-Status(-übergänge) werden vom
Bookkeeper ebenfalls ins Zustands-Log geschrieben. Weitere Informationen zum Zustands-Log finden
Sie im Kapitel 5.1.6 Zustands-Log.
Die Verarbeitung innerhalb der QS-AV Applikation basiert auf der Partitionierung von Daten
nach Zeitscheiben („Timebuckets“), die in Blöcken abgearbeitet werden. Ein Timebucket besteht
aus einem Zeitstempel im Format „DDMMYY_hhmm“ und gehört immer zu einem spezifischen
Datenstrom (i.d.R. einer Cassandra-Tabelle). Der Timebucket-Katalog aller Timebuckets mit ihren
jeweils aktuellen Zeitstempeln wird in der Cassandra DB gehalten.
Werden Datensätze durch einen an das QS-AV liefernden LDS-Client in nach Timebuckets
partitionierten Tabellen geschrieben, so muss zunächst der jeweils aktuelle Wert für den TimebucketZeitstempel dem Timebucket-Katalog entnommen werden. Dieser wird dann als (Teil des) Partition
Key(s) gespeichert.
Die Verwendung der Timebuckets ermöglicht es, asynchron eintreffende oder verarbeitete Daten
unterschiedlicher Datenquellen performant aus der Cassandra DB selektieren zu können. Diese durch
die Timebuckets synchronisierten Daten können dann innerhalb von Batch-Operationen verarbeitet
werden.
Weitere Informationen zu Konfiguration und Arten von Timebuckets entnehmen Sie den Kapiteln 5.1.5
Konfiguration und 5.1.2 Bookkeeper Glossar (Timebucket-Schema).
Die Timebucket-Switcher Applikation ist für die Aktualisierung („switch“) der Timebucket-Timestamps
im Timebucket-Katalog zuständig. Die Zeitscheibengröße kann dabei je Timebucket-Schema
konfiguriert werden. Nach jeder Aktualisierung schreibt der Timebucket-Switcher einen
entsprechenden Eintrag ins Zustands-Log. Da sowohl die Datenpartitionierung als auch die
Ablaufsteuerung von den Timebuckets abhängt, werden hohe Verfügbarkeitsanforderungen an die 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 72 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Timebucket-Switcher Applikation gestellt. Weitere Details zum Timebucket-Switcher finden Sie im
Kapitel 5.3 Timebucket-Switcher Applikation.
Über eine REST-Schnittstelle („Admin-Interface“) können Statusinformationen zum Bookkeeper aus
dem Zustands-Log abgerufen und abgebrochene Jobs neu gestartet werden. Weitere Informationen
zum Admin-Interface finden Sie im Kapitel 5.1.7 Admin-Interface (REST-API).
5.1.3.1 Detailabbildung
Die folgende Abbildung 21: Bookkeeper Überblick Detail (Kontext) gibt einen detaillierteren Überblick
über die Zusammenhänge. In den darauffolgenden Kapiteln werden die einzelnen Bestandteile und
Konzepte beschrieben.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 73 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Abbildung 21: Bookkeeper Überblick Detail (Kontext)
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 74 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.1.4 Wiederaufsatz und Job-Ausführung
In der folgenden Abbildung sind die Beziehungen der zentralen Klassen und Objekte der BookkeeperAblaufsteuerung dargestellt.
Abbildung 22: Bookkeeper – Klassen und Objekte (Ablauf)
 Die Klasse LongRunner wird bei Programmstart instanziiert und gestartet (siehe auch Kapitel
5.2.2 qs-boot: Hauptapplikation (run)). Sie kontrolliert die Jobausführung über die
Komponente JobExecutor und behandelt alle auftretenden nicht-fatalen Fehler. 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 75 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
 Der JobExecutor führt folgende Schritte aus:
Abbildung 23: Detail - JobExecutor Ablauf (stark vereinfacht)
1. Bei der Initialisierung des JobExecutors wird geprüft, ob Recovery-Maßnahmen
notwendig sind. Bei Bedarf wird ein Wiederaufsatz mit Hilfe der Informationen im
Zustands-Log durchgeführt.
Nach der Initialisierung des JobExectors startet der LongRunner die Verarbeitung der
regulär konfigurierten und der per Admin-Interface neugestarteten Jobs. Dazu ruft der
LongRunner in einer Schleife die step()-Methode des JobExecutors auf. Je
Durchlauf werden dabei folgende Schritte ausgeführt:
2. Im Zustands-Log wird nach Jobs gesucht, die per Admin-Interface zum Neustart
vorgemerkt wurden. Diese Jobs werden nach dem FIFO-Prinzip direkt ausgeführt.
3. Die regulär konfigurierten Jobs (gemäß RuleConfig) werden ausgeführt:
 Prüfung der zeitlichen Abhängigkeiten: Anhand der aktuellen Zeitscheibe wird
ermittelt, welche Jobs gemäß ihres Ausführungsintervalls zum Start bereit
sind. Die betreffenden Jobs werden zur Ausführung vorgemerkt, indem
entsprechende Einträge ins Zustands-Log aufgenommen werden (siehe auch
Kapitel 5.1.6 Zustands-Log).
 Prüfung der inhaltlichen Abhängigkeiten: Mit Hilfe des
RulePreconditionCheckers wird ermittelt, ob die in der RuleConfig
(Job-Konfiguration) spezifizierten Vorbedingungen (preConditions) für den
Job-Start erfüllt sind. Dazu zählt u.a. ob alle Vorgänger-Jobs ausgeführt
wurden, vorausgesetzte Tabellen-Timebuckets gefüllt sind und/oder
vorausgesetzte Signal-Events registriert wurden (Signal-Timebuckets).
 Sind alle Abhängigkeiten bzw. Vorbedingungen für einen Job erfüllt, wird er
ausgeführt: Anhand des zugehörigen RuleSchemas (Job-Schema) wird über
das CassandraRuleJobRepository eine konkrete Instanz des RuleJobs
erzeugt und die Ausführung durch den konfigurierten
RuleJobExecutorHeavy angetriggert. Die Implementierungen der
RuleJobs an sich befinden sich in den Teilkomponenten qs-fzg und 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 76 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
qs-erh.
Innerhalb der gleichen Jobkette ggf. abhängige Jobs werden direkt im
Anschluss ausgeführt.
 Informationen zum Status bzw. zu Statusübergängen werden im ZustandsLog festgehalten.
Nach jedem Schleifendurchlauf wird 2 Sekunden pausiert.
5.1.5 Konfiguration
Die Job- und Timebucket-Konfiguration erfolgt über die Klassen RuleConfig und
RuleConfigTimebuckets, wie in folgender Abbildung dargestellt.
Abbildung 24: Bookkeeper – Klassen und Objekte (Konfiguration)
In der Klasse RuleConfig erfolgt die Job-Konfiguration für die einzelnen Jobs jeweils anhand eines
RuleSchemas (Job-Schema) mit ihren Vor- und Nachbedingungen.
In der Klasse RuleConfigTimebuckets (Timebucket-Konfiguration) werden die innerhalb der JobKonfiguration als Vor- und Nachbedingungen genutzten Timebuckets anhand von
TimebucketSchemas spezifiziert.
Allgemein müssen alle genutzten Timebuckets an dieser Stelle konfiguriert werden. Die TimebucketKonfiguration wird vom Bookkeeper und dem Timebucket-Switcher benötigt.
5.1.5.1 RuleConfig – Job-Konfiguration
Die Konfiguration der Jobs innerhalb der RuleConfig verknüpft folgende Aspekte:
 Job-Klasse: Verknüpfung der konkreten Job-Implementierung aus den Teilkomponenten qsfzg und qs-erh (z.B. Job MobaEventCreator im Package
de.tollcollect.zme.qs.erh.rule). Hinweis: Die Jobs müssen das Trait
RuleJobExecutorHeavy direkt oder abgeleitete Traits (z.B. RuleJobExecutorLight)
implementieren.
 Ausführungsintervall: Ausführungsintervall für den Job (truncationDimension)
 Vorbedingung/en (preConditions): Vorbedingungen, die als Voraussetzung für die
Ausführung eines Jobs angesehen werden. Es handelt sich dabei um ein oder mehrere
Timebuckets, die per TimebucketSchema in der RuleConfigTimebuckets konfiguriert
werden.
 Nachbedingung/en (postConditions): Nachbedingungen, die als Ergebnisse der JobAusführung erfüllt sind und die als Vorbedingungen für Folge-Jobs in der Verarbeitungskette 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 77 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
(Jobkette) fungieren. Es handelt sich dabei um ein oder mehrere Timebuckets, die per
TimebucketSchema in der RuleConfigTimebuckets konfiguriert werden.
Wichtige Hinweise: Eine Tabelle (= ein TimebucketSchema) darf jeweils immer nur für
einen RuleJob als Ausgangstabelle fungieren.
Weitere Informationen zu Arten von Timebuckets bzw. Timebucket-Schemata finden Sie im
Bookkeeper Glossar (Kapitel 5.1.2).
Konfiguration eines RuleSchema (via Klasse RuleConfig)
Parameter Typ Beschreibung
name String Eindeutige Bezeichnung für den Job, der auch für das
Zustands-Log verwendet wird.
truncationDimension TruncationDimension Ausführungsintervall/-zeitpunkt des RuleJobs. Kann wie folgt
angegeben werden:
 Ein Mal täglich zur vollen Stunde:
FixedHourInDay(<Stunden, z.B. 23>)“
 Intervall – alle x Stunden:
Hours(<x Stunden, z.B. 1>)“
 Intervall – alle y Minuten:
Minutes(<y Minuten, z.B. 15>)“
preConditions Seq[TimebucketSchema] Vorbedingung/en als Liste von TimebucketSchemata
Es handelt sich dabei um ein oder mehrere Timebuckets, die
per TimebucketSchema in der
RuleConfigTimebuckets konfiguriert werden.
Die Konfiguration von preConditions führt dazu, dass der
Job erst gestartet wird, wenn alle Vorbedingungen erfüllt
sind.
postConditions Seq[TimebucketSchema] Optionale Nachbedingung/en als Liste von
TimebucketSchemata
Es handelt sich dabei um ein oder mehrere Timebuckets, die
per TimebucketSchema in der
RuleConfigTimebuckets konfiguriert werden.
Die Angabe von postConditions dient dazu, dass
Folge-Jobs innerhalb der gleichen Jobkette direkt im
Anschluss an die erfolgreiche Ausführung des
dazugehörigen RuleJobExecutors gestartet werden
können, sofern damit alle Voraussetzungen
(preConditions) für die betreffenden Folge-Jobs erfüllt
sind.
ruleExec RuleJobExecutorHeavy Klasse des konkret zu startenden Jobs (RuleJob) aus
qs-fzg oder qs-erh, der das Trait
RuleJobExecutorHeavy oder eine Ableitung
implementiert und die Ausführungslogik enthält
forceRun Boolean Flag, das angibt, ob der Job ausgeführt werden soll, auch
wenn nicht alle Voraussetzungen erfüllt sind. Default ist
false
Tabelle 46: Bookkeeper RuleSchema-Konfiguration – RuleConfig
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 78 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Zusätzliche Funktionen
 Das TimebucketSchema stellt die Funktion explode(INT) bereit. Sie erwartet eine Zahl
vom Typ INT, die angibt, wie viele Timebuckets des TimebucketSchemas (gemäß der
truncationDimension des TimebucketSchemas) vom Anchor-Timebucket aus in die
Zukunft blickend betrachtet werden sollen. Wird z.B. ein RuleSchema definiert, welches
täglich um 0 Uhr läuft und die letzten 24 Stunden auswerten soll, dann muss bei einem
TimebucketSchema mit einer Partitionierung von 15 Minuten ein explode von 96 angegeben
werden (<TimebucketSchema>.explode(96).toList()).
 Um sicherzustellen, dass alle Daten des Eingangsdatenstroms eines Jobs erfasst und
einmalig verarbeitet werden können, müssen die als preConditions genutzten
Timebuckets mit dem Ausführungsintervall des Jobs übereinstimmen bzw. durch die Nutzung
von explode() angeglichen werden.
Beispiele
RuleSchema("fzg/dm/mdnParser/15m", Minutes(15), List(timebucketSchemaMdn),
List(timebucketSchemaDmMdn), MdnParser),
RuleSchema("erh/edm/metricVerdichtung/24h", FixedHourInDay(0),
timebucketSchemaEdmMetrik.explode(96).toList, List(timebucketMetrikOut24h), new
GenericMetricVerdichtung(), forceRun = true)
Die aktuelle Konfiguration für die QS-AV Applikation ist im Kapitel 7 Ablaufsicht grafisch
dargestellt.
5.1.5.2 RuleConfigTimebuckets – Timebucket-Konfiguration
Die grafische Darstellung finden Sie in der Abbildung 24: Bookkeeper – Klassen und Objekte
(Konfiguration).
Konfiguration eines TimebucketSchema (innerhalb der Klasse RuleConfigTimebuckets)
Das Timebucket-Schema beschreibt ein Timebucket, das sich aus dem Namen des Eingangs-
/Ausgangsdatenstroms, dem Intervall für die Partitionierung und optional einem konkreten Timebucket
(vom Anchor-Zeitpunkt aus gesehen) zusammensetzt.
Die unterschiedlichen Arten von Timebuckets sind im Glossar-Eintrag zum Timebucket-Schema
beschrieben (siehe Kapitel 5.1.2 Bookkeeper Glossar). Die Unterscheidung ist nur für die
Konfiguration relevant und nicht für den Bookkeeper oder den Timebucket-Switcher, die jedes zur
Verfügung stehende Timebucket gleich behandeln.
Das TimebucketSchema spezifiziert ein Timebucket über folgende Parameter:
Parameter Typ Beschreibung
tableName String Der tableName kann:
 Name einer realen Eingangs-/Ausgangs-Tabelle sein,
die mit diesem Timebucket verknüpft ist
 Eine frei gewählte Bezeichnung für ein TimingTimebucket sein, das über das Zustands-Log hinweg
keine weitere Relevanz für den Ablauf hat
 Name eines Signal-Ereignisses sein
Hinweise: Aus Gründen der Übersichtlichkeit sollte der
Tabellenname bei Timer-Timebuckets immer mit dem
Präfix „timingtb-“ und der von Signal-Timebuckets mit
„signal-“ beginnen.
truncationDimension TruncationDimension Intervall für die Segmentierung der Daten eines
Datenstroms (Partitionierung). Kann wie folgt angegeben 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 79 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
werden:
 Ein Mal täglich zur vollen Stunde:
FixedHourInDay(<Stunden, z.B. 23>)“
 Intervall – alle x Stunden:
Hours(<x Stunden, z.B. 1>)“
 Intervall – alle y Minuten:
Minutes(<y Minuten, z.B. 15>)
anchorDistanceInTru
ncationSizeMultiple
Int Wert, der angibt, auf das wievielte Timebucket in der
Vergangenheit sich vom Anchor des RuleSchemas
bezogen werden soll.
Beispiel für einen Job, der jede Stunden ausgeführt werden
soll und ein TimebucketSchema mit einer
Partitionierung von 15 Minuten verwendet: Wird als
anchorDistanceInTruncationSizeMultiple
ein Wert von 3 angegeben, würde nur das jeweils dritte
Timebucket jeder Stunde, also äquivalent 00:45, 01:45,
02:45 usw. verarbeitet werden.
Soll jedes Timebucket ausgewertet werden, ist ein Wert
von 0 anzugeben.
Tabelle 47: Bookkeeper TimebucketSchema-Konfiguration - RuleConfigTimebuckets
Wichtiger Konfigurationshinweis: Für alle Timebuckets, die durch den Timebucket-Switcher
geswitcht werden sollen (Timer- und Tabellen-Timebuckets), müssen zusätzlich folgende
Konfigurationstätigkeiten durchgeführt werden:
1) TimeBucketSchema zur Liste ldsEnQtimebuckets in der Klasse RuleConfigTimebuckets
hinzufügen.
2) Timebucket im Timebucket-Katalog (Tabelle en_q_timebuckets) per CQL-insert registrieren:
INSERT INTO en_q_timebuckets ( table_name, curr_timebucket ) VALUES
('<tableName>', '19700101_0000');
Beispiele
TimebucketSchema(enQmdn, Minutes(15), 0)
TimebucketSchema(„timingTb-RseKontaktDeletion“, FixedHourInDay(4), 0)
Die aktuelle Konfiguration für die QS-AV Applikation ist im Kapitel 7 Ablaufsicht grafisch
dargestellt.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 80 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.1.6 Zustands-Log
Der Bookkeeper verwaltet ein Zustands-Log bestehend aus den drei Cassandra-Tabellen
bkpr_eventlog, bkpr_waiting_timebuckets und bkpr_rule_jobs.
Abbildung 25: Zustands-Log (Bookkeeper)
5.1.6.1 Speicherung und Protokollierung
Im Zustands-Log erfolgt die Speicherung und Protokollierung (Historie) von:
 Protokolleinträgen (Events und Statusübergänge)
 Job-Statusinformationen und -Daten
 Timebucket-Zuordnungen zu Jobs (Vorbedingungen) und deren Status
5.1.6.2 Einsatz
Das Zustands-Log wird benötigt für:
 Die Funktionsfähigkeit des Bookkeepers, der die Job-Ausführung in Abhängigkeit des
Zustands-Logs steuert.
 Den Wiederaufsatz nach geordnetem Shutdown oder Applikationscrash.
 Den Neustart von Jobs, die nicht erfolgreich ausgeführt werden konnten, per Admin-Interface.
 Die Nachvollziehbarkeit der Ablaufsteuerung (Protokollierung der Job-Status).
5.1.6.3 Nutzung
Das Zustands-Log wird von folgenden Teil-/Sub-/Komponenten genutzt:
 Bookkeeper
 Timebucket-Switcher
 Admin-Interface
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 81 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.1.6.4 Datenbanktabellen
Im Folgenden sind die Zustands-Log-Tabellen im Detail beschrieben.
Datenmodell: Die detaillierte Dokumentation der Attribute/Spalten finden Sie im
Referenzdokument [13] Datenmodell.
bkpr_eventlog: Protokolleinträge (Events und Statusübergänge)
Bei der Tabelle bkpr_eventlog handelt es sich um die zentrale Tabelle für die Bookkeeper-Events
und Statusübergänge. Jede Zeile entspricht einem atomaren und konsistenten Protokolleintrag des
Bookkeeper-Ablaufes. Jeder Eintrag ist durch eine zum Insert-Zeitpunkt generierte UUID (event_id)
eindeutig identifizierbar.
Zu jedem Protokolleintrag können zusätzlich benötigte Daten in den Sekundärtabellen
bkpr_waiting_timebuckets und bkpr_rule_jobs gespeichert und via UUIDs verknüpft
werden. Die Schreibvorgänge erfolgen immer in Form von Cassandra-Atomic-Batches. Dadurch soll
sichergestellt werden, dass Einträge im Zustands-Log immer komplett über alle betreffenden Tabellen
persistiert werden.
Jeder Protokolleintrag ist eindeutig typisiert. Die folgende Tabelle beschreibt zu jedem verfügbaren
Typen (event_type:INT) die typabhängig beschriebenen Datenfelder innerhalb der Tabelle sowie
die per Atomic-Batch beschriebenen Sekundärtabellen:
Typ Name (Info) Event-Typ spezifische Spalten in
bkpr_eventlog
Beschriebene
Sekundärtabellen
1 LdsEnqTimebucketSwitched new_timebuckets = set<TEXT>
Menge von Timebuckets kodiert als
String im Format:
<tabellenname>;<timebucket>
Beispiel
“lds_dm_en_q_xy;201601170915“
-
2 RuleJobExecutedSuccess new_timebuckets = set<TEXT>
Menge von Timebuckets kodiert als
String im Format:
<tabellenname>;<timebucket>
Beispiel
“lds_dm_en_q_xy;201601170915“
-
3 RuleJobExecutedFailed new_timebuckets = set<TEXT>
Menge von Timebuckets kodiert als
String im Format:
<tabellenname>;<timebucket>
Beispiel
“lds_dm_en_q_xy;201601170915“
-
4 NewRuleJobsCreated rule_jobs_created = set<TIMEUUID>
Menge mit Referenzen auf Tabelle
bkpr_rule_Jobs per TIMEUUID
bkpr_waiting_timebuckets
bkpr_rule_jobs
5 RuleJobReadyToExecute rule_job_to_exec = TIMEUUID mit
Referenz auf Tabelle
bkpr_rule_Jobs per TIMEUUID
bkpr_waiting_timebuckets
bkpr_rule_jobs
6 NoJobReadyToExecute - bkpr_waiting_timebuckets
bkpr_rule_jobs
Tabelle 48: Zustands-Log-Tabelle bkpr_eventlog – Typen von Protokolleinträgen
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 82 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Jeder Protokolleintrag weist einen der vier Zustände (state) von 0 bis 3 auf:
 0=„created“
 1=„outdated“
 2=„aPrioriOutdated“
 3=„restarted“
Der Zustand spielt eine zentrale Rolle für den konsistenten Wiederaufsatz-Prozess. Bei Erstellung
wird der Zustand initial auf den Wert 0 (created) gesetzt. Ausnahme ist der Event-Typ 6
(NoJobReadyToExecute), der initial auf den Wert 2 (aPrioriOutdated) gesetzt wird, weil dieses
Event nicht zum Wiederaufsatz nötig ist und nur zur besseren Überprüfbarkeit der Korrektheit aller
Bookkeeper-Entscheidungen geschrieben wird.
Bis auf den Event-Typ LdsEnqTimebucketSwitched (1) hat jeder Protokolleintrag einen
eindeutigen Vorgängereintrag im Log, der in der Spalte event_id_outdated referenziert wird. Der
Zustand dieses referenzierten Events wird beim INSERT des Nachfolge-Events auf den Zustand 1
(outdated) gesetzt und damit obsolet für den Wiederaufsatz, weil ein neuer konsistenter Zustand für
dieses Blatt des baumartigen Job-Ausführungs-Graphen erreicht ist.
bkpr_waiting_timebuckets: Timebucket-Zuordnungen zu Jobs
Beim Einstellen von neuen RuleJobs wird für alle TimebucketSchemata, die als Vorbedingung für
das Ausführen dieses RuleJobs konfiguriert wurden, in dieser Tabelle ein neuer Key angelegt
(tablename_timebucket) bzw. ein schon bestehender Key um die UUID des wartenden RuleJobs
erweitert. Auf diese Weise ist bei "Eintreffen" neuer fertiger Timebuckets (Abschluss eines
Timebuckets durch switch) eine effiziente Entscheidung darüber möglich, welche RuleJobs auf das
Timebucket warten. Die eingetroffenen Timebuckets werden aus dieser Tabelle wieder gelöscht,
nachdem die Bookkeeper-Entscheidung konsistent als Protokolleintrag in der Tabelle
bkpr_eventlog geloggt worden ist.
bkpr_rule_jobs: Job-Statusinformationen und –Daten
In dieser Tabelle werden alle wartenden RuleJobs vermerkt. Via dem PartitionKey rule_job_id (TIMEUUID) wird von den beiden anderen Tabellen auf diese Tabelle verwiesen.
Beim Eintreffen von neuen Timebuckets werden diese aus der Spalte needed_timebuckets gelöscht.
Sobald dieses Set leer geworden ist, kann der entsprechende RuleJob ausgeführt werden.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 83 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.1.7 Admin-Interface (REST-API)
Für den Wiederaufsatz wurde eine Jetty-basierte Administrations-REST-API erstellt, die per Default
unter http://127.0.0.1:8092 erreichbar ist. Hinweis: Host und Port sind über die Umgebungsvariablen
QSAV_ADMIN_HOST und QSAV_ADMIN_PORT beim Deployment konfigurierbar.
Über die API lassen sich abgebrochene Jobs ausgeben und neu starten. Die Jobs werden bei der
Job-Ausführung je Zyklus bevorzugt behandelt.
Die zur Verfügung gestellten Services sind im Folgenden beschrieben.
5.1.7.1 Service: Usage (Bedienungsanleitung)
Aufruf GET http://127.0.0.1:8092/
Beschreibung Gibt die Bedienungsanleitung zurück
Tabelle 49: Administrations-API (Bookkeeper) - Service für die Bedienungsanleitung
5.1.7.2 Service: Abgebrochene Jobs
Aufruf GET http://127.0.0.1:8092/admin/failed_jobs.json
Beschreibung Gibt eine Liste aller abgebrochenen Jobs zurück
Beispiel curl http://127.0.0.1:8092/admin/failed_jobs.json
[{
 "jobName" : "fzg/dm/mdnParser/15m",
 "errorMessage" : "Artificial, I don't want to know",
 "failTime" : "2017-03-23T11:32:41.526",
 "logId" : "0b3e6d60-0fb4-11e7-86e3-7178fe8bf3d4",
 "jobId" : "92abf363-0fb1-11e7-8ac7-7178fe8bf3d4",
 "timebucketsProvided" : "fzg_dm_mdn;20170323_1000",
 "timebucketsNeeded" : "",
 "timebucketsNeededOriginally" : "lds_dm_en_q_mdn;20170323_1000"
}, {
 "jobName" : "fzg/dm/mdnParser/15m",
 "errorMessage" : "Artificial, I don't want to know",
 "failTime" : "2017-03-23T11:32:41.534",
 "logId" : "0b3fa5e0-0fb4-11e7-86e3-7178fe8bf3d4",
 "jobId" : "ac3904b0-0fb3-11e7-b1d6-7178fe8bf3d4",
 "timebucketsProvided" : "fzg_dm_mdn;20170323_1015",
 "timebucketsNeeded" : "",
 "timebucketsNeededOriginally" : "lds_dm_en_q_mdn;20170323_1015"
} ]
Tabelle 50: Administrations-API (Bookkeeper) - Service zur Auflistung abgebrochener Jobs
5.1.7.3 Service: Job neu starten
Aufruf POST http://127.0.0.1:8092/admin/job_restart?id=<Log-ID>
Beschreibung Startet einen Job mit gegebener Log-ID neu
Beispiel curl -X POST "http://localhost:8092/admin/job_restart?id=c066c1f0-
0fb0-11e7-a5af-7178fe8bf3d4"
Job Restart requested
Tabelle 51: Administrations-API (Bookkeeper) - Service zum Neustart eines Jobs
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 84 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.1.7.4 Service: Mehrere Jobs neu starten
Aufruf POST http://127.0.0.1:8092/admin/job_restart_all?name=<Name>&maxHours=<hours>
Beschreibung Startet alle Jobs mit dem gegebenen namen, die in den letzten maxHours fehlgeschlagen
sind neu.
Beispiel curl -X POST
"http://127.0.0.1:8092/admin/job_restart_all?name=fzg/dm/mdnParser/1
5m&maxHours=2"
Requested restart of following jobs Vector(0b3e6d60-0fb4-11e7-86e3-
7178fe8bf3d4, 0b3fa5e0-0fb4-11e7-86e3-7178fe8bf3d4, 0c604c90-0fbe11e7-9a4c-7178fe8bf3d4)
Tabelle 52: Administrations-API (Bookkeeper) - Service zum Neustart aller Jobs
Hinweis: Beim Aufruf ist auf korrektes HTTP-Escaping zu achten.
5.1.8 Fehlerbehandlung
Die Fehlerbehandlung ist im Referenzdokument [17] [A_QSAV_HBB_00] QS-AV
Betriebshandbuch dokumentiert.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 85 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2 QS-AV Applikation
Die Realisierung der Geschäftslogik der Bereiche QS-AV erfolgt gekapselt innerhalb entsprechender
Teilkomponenten: qs-fzg und qs-erh. Diese sind zusammen mit weiteren technischen
Komponenten in den übergeordneten Applikationskontext eingebettet.
Hinweis: Technisch sind die Teilkomponenten als Maven-Projekte realisiert.
Maven (lokale Abhängigkeiten)
qs-fzg qs-boot qs-erh
qs-parent <<parent>>
qs-common qs-common-test <<compile>> <<test>> <<compile>> <<test>> <<compile>> <<compile>>
qs-timebucketswitcher <<compile>> <<compile>> <<compile>>
Abbildung 26: Überblick Software-Komponenten QS-AV Applikation inkl. Timebucket-Switcher
Die Aufteilung der Maven-Projekte basiert auf folgenden Aspekten:
1. Rahmen-Paket, das alle funktionalen Subkomponenten enthält (qs-parent)
2. Applikationskontext für den Programmstart (qs-boot)
3. Kapselung gemeinsam genutzter Basisfunktionalitäten (z.B. Bookkeeping) (qs-common)
4. Aufteilung der Geschäftslogik nach QS-FzG und QS-Erh (qs-fzg und qs-erh)
5. Kapselung gemeinsam genutzter Basisfunktionalitäten und Utilities der Komponententests
(qs-common-test)
6. Timebucket-Switcher (qs-timebucketswitcher)
Die einzelnen Projekte werden in den folgenden Abschnitten näher beschrieben.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 86 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.1 qs-parent: Parent-POM (technisch)
Das Maven-Modul qs-parent aggregiert folgende Maven-Module:
<modules>
 <module>../qs-common</module>
 <module>../qs-common-test</module>
 <module>../qs-fzg</module>
 <module>../qs-erh</module>
 <module>../qs-boot</module>
 <module>../qs-timebucketswitcher</module>
</modules>
Listing 1: qs-parent Module (Auszug aus der pom.xml)
Das Parent-POM dient der Zusammenfassung der Software-Module im Build-Prozess. Darüber hinaus
bietet es u.a. Möglichkeiten zur Vererbung gemeinsamer Merkmale, wie dem Quell-Encoding (UTF-8),
zur globalen Definition von Compiler-Versionen, zum zentralen Management transienter
Dependencies oder zum Version-Alignment.
Das Modul qs-parent enthält keine Anwendungslogik.
5.2.2 qs-boot: Hauptapplikation (run)
Das Maven-Modul qs-boot fasst die Subkomponenten qs-fzg und qs-erh in einem gemeinsamen
Applikationskontext zusammen.
Hinweis: Mit dem Start von qs-boot wird das Scheduling und die Ausführung der
Qualitätssicherungs-Jobs aus den Teilkomponenten qs-fzg und qs-erh durch den Bookkeeper
gestartet. Siehe auch Kapitel 7 Ablaufsicht.
5.2.3 qs-common: Basiskomponenten und Utilities
In der Subkomponente qs-common sind gemeinsam genutzte Basisfunktionalitäten gekapselt:
 Bookkeeper inklusive abstrakten Jobs und Utility-Funktionen zur Benutzung in konkreten
Jobimplementierungen innerhalb der Packages de.tollcollect.zme.qs.common.jobs
und de.tollcollect.zme.qs.common.rule.
Der Bookkeeper ist als Komponente zur Programmablaufsteuerung gesondert im Kapitel 5.1
Schwerpunktkapitel: Bookkeeper (qs-common) beschrieben.
 Basiskonfiguration de.tollcollect.zme.qs.common.conf.tech:
o Die Klasse Config bietet Zugriff auf die in der Cassandra-DB gespeicherte
Applikationskonfiguration (Tabelle tech_config).
o Die Klasse SchwellwertProvider bietet Zugriff auf die in der Cassandra-DB
gespeicherten Schwellwerte (Tabelle schwellwert).
o Context: Applikationskontext u.a. mit SparkContext, CassandraSQLContext,
HiveContext , Config, SchwellwertProvider, Cassandra-Host-, CassandraSession- und –Cassandra-Keyspace-Konfiguration
 Utils: Allgemeine Utilities finden sich im Package
de.tollcollect.zme.qs.common.util
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 87 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4 qs-fzg: Qualitätssicherung der Fahrzeuggeräte
Die Teilkomponente qs-fzg enthält die Anwendungslogik inklusive der
Qualitätssicherungsalgorithmen für die Qualitätssicherung der Fahrzeuggeräte. Die Implementierung
ist komplett in Scala gehalten und erfolgt innerhalb einzelner Jobs, die durch den Bookkeeper
gesteuert werden.
5.2.4.1 Jobs
In den nachfolgenden Unterkapiteln werden die Jobs für qs-fzg beschrieben. Der Ablauf der
Jobsteuerung kann dem Kapitel 7 Ablaufsicht entnommen werden.
Datenmodell: In den folgenden Unterkapiteln werden Tabellen und Attribute genannt, die im
Datenmodell gelistet sind. Das Datenmodell (Tool-Export) [13] wird zusammen mit den SoftwareLieferungen zur Verfügung gestellt.
Die Ablaufsicht aller Jobs inklusive ihrer Namen, Ausführungsintervalle und Abhängigkeiten ist im
Bookkeeper-Graph dargestellt. Siehe Anhang [BK-Graph].
5.2.4.1.1 [DSJ-QSFzG-0010] BatchImportFzgStammdaten
Paket: de.tollcollect.zme.qs.fzg.masterdata
Die Scala-Klasse BatchImportFzgStammdaten, behandelt die Persistierung von FzGStammdatenänderungen, sowie der Persistierung neuer FzG-Stammdaten. Sie enthält die Logik zum
FULL-Import der FzG-Stammdaten aus der Quelltabelle der DM Oracle DB View. Der Job ist wie folgt
implementiert:
Abbildung 27: Job-Ablauf (schematisch): BatchImportFzgStammdaten
Trigger: Der FULL-Import wird 1x täglich vom Bookkeeper gestartet.
Ablauf
Der Zugriff auf die FzG-Stammdaten erfolgt mittels Oracle JDBC-Treiber. Folgende Schritte werden
durchgeführt:
1. Laden der Daten aus der DM Oracle View in einen Spark Dataframe
2. Mapping und Transformation der Daten der Quelltabelle FZG_STAMMDATEN_VIEW auf die
Struktur der Zieltabelle lds_dm_fzg_stammdaten
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 88 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3. Speicherung der transformierten Stammdaten in der Cassandra-Datenbank.
Wichtig: Je FzG existiert ein Eintrag in der Cassandra-Zieltabelle lds_dm_fzg_stammdaten.
Beim FULL-Import werden die Daten aller vorhandenen FzG überschrieben (PRIMARY KEY
ehk_id), neue FzG werden automatisch hinzugefügt. In der Zieltabelle werden alle FzG seit dem
ersten Import-Zeitpunkt geführt, also ggf. auch nicht mehr eingesetzte FzG. Der Status der FzG ist in
der Spalte status_usecycle vermerkt (mögliche Werte: 0 = Eingebaut, 1 = Ausgebaut, 2 = Im
Einbau, 3 = Einbauabbruch).
Die Persistierung der zusätzlich von DM via LDS-Client übertragenen Stammdaten-Events (EinbauEvents) erfolgt in derselben Cassandra-Zieltabelle, die auch für die FULL-Importe verwendet wird
(lds_dm_fzg_stammdaten). Siehe dazu auch Kapitel 6.1.3 SST 830 Device Management (DM) -
QS-AV.
Konfiguration
Der Zugriff über den Oracle JDBC-Treiber wird in folgenden Umgebungsvariablen im Rahmen der
Erstellung der Deployments konfiguriert:
Variable Beschreibung
QSAV_EXTERNAL_LDS_DM_ORACLE_JDBC_CONNECTION_URI URI der Oracle-DB
QSAV_EXTERNAL_LDS_DM_ORACLE_SCHEMA Das zu verwendende Oracle Schema
Tabelle 53: BatchImportFzgStammdaten – Umgebungsvariablen
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 89 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.2 [DSJ-QSFzG-0020] FsmNormierung
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job FsmNormierung dient der Normierung der über den LDS-Client eingehenden FSM
(Fahrzeuggeräte-Statusmeldungen). Die Normierung liefert vier Ergebnisse:
1. Erfassung von FSM Zustandsänderungen: Der Zustandswechsel einer FSM wird durch
Vergleich der FSM mit ihrem direkten Vorgänger ermittelt und gespeichert.
2. Erfassung von Regelbits: Auswertung des FSM und Setzen der Regelbits (boolean-Werte).
3. Aktualisierung der FSM in der Cassandra-Tabelle: Aktualisierung vorhandener Einträge in der
Cassandra-Tabellen fzg_fsm_eingang, um sicherzustellen, dass zu jeder Zeit nur ein
aktueller Eintrag pro FzG existiert.
4. Anonymisierung und Löschung: Daten der Ausgangstabellen werden im Zuge der
Normierung anonymisiert und mit TTLs versehen. Hierzu wird ein DataFrame geteilt und mit
unterschiedlichen TTLs nach Cassandra geschrieben.
Wichtig: Das Sub-DataFrame muss immer den Primärschlüssel der Zieltabelle enthalten.
Der Job ist in der Klasse FsmNormierung, wie folgt implementiert:
Abbildung 28: Job-Ablauf (schematisch): FsmNormierung
Trigger: Die FSM-Normierung wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
1. Einlesen der FSM des aktuellen 15-Minuten-Timebuckets aus der Quelltabelle
(lds_es_en_q_fsm).
2. FzG-spezifisch (in Abhängigkeit der EHK-Id) prüfen, ob mit den eingelesenen FSM
Zustandsänderungen gegenüber der letzten in QS-AV gespeicherten erfolgt sind.
3. Für FSM mit Zustandsänderung: Ermittlung der Regelbits im Rahmen der FSM-Normierung
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 90 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4. Speicherung der Ergebnisse in den drei Ausgangstabellen:
a. fzg_fsm_zustandswechsel: Speicherung des Zustandswechsels für jede
geänderte FSM (attributweise).
b. fzg_fsm_normalisierung_ergebnis: Speicherung der Ergebnisse der
Normierung für jede geänderte FSM.
c. fzg_fsm_eingang: Speicherung bzw. Aktualisierung des Eingangszeitpunkts der
ersten und der letzten FSM eines Fahrzeuggeräts.
5. Löschung des verarbeiteten Timebuckets in der LDS-Inputtabelle (lds_es_en_q_fsm).
Allgemeine Hinweise
Im Zuge der FSM-Normierung wird eine Anonymisierung und Löschung der Zeitstempel-Attribute
mittels TTLs durchgeführt:
 fzg_fsm_zustandswechsel:
o Der Zeitstempel wird nach einer konfigurierbaren Anzahl an Sekunden
(zustandswechsel_day_anonymization_interval_sec) auf den Tag
aggregiert (0 Uhr). Siehe auch Konfiguration.
o Der Zeitstempel wird nach einer konfigurierbaren Anzahl an Sekunden
(zustandswechsel_year_anonymization_interval_sec) auf das Jahr
aggregiert (1.Januar 0 Uhr). Siehe auch Konfiguration.
 fzg_fsm_normalisierung_ergebnis:
o Der Zeitstempel wird nach einer konfigurierbaren Anzahl an Sekunden
(ergebnis_day_anonymization_interval_sec) auf den Tag aggregiert (0 Uhr).
Siehe auch Konfiguration.
o Der Zeitstempel wird nach einer konfigurierbaren Anzahl an Sekunden
(ergebnis_deletion_interval_sec) gelöscht. Siehe auch Konfiguration.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
fzg.sw.aktuelle_version Unconfigured Versionsbezeichnung der aktuellsten produktiv
ausgerollten EtcApp; hier benötigt für die
Ermittlung, ob die in der FSM angegebene
Version der aktuellen entspricht
fzg.fsm_normierung.ergebnis_day_
anonymization_interval_sec
3628800 Sekunden nach dem der Zeitstempel in
lds_dm_data_primary_fsm_ergebnis
gelöscht wird.
fzg.fsm_normierung.ergebnis_deletion_
interval_sec
10368000 Sekunden nach dem Einträge in
lds_dm_data_primary_fsm_ergebnis
gelöscht werden.
fzg.fsm_normierung.zustandswechsel_
day_anonymization_interval_sec
3628800 Sekunden nach dem Einträge in
lds_dm_data_primary_fsm_ergebnis
gelöscht werden.
fzg.fsm_normierung.zustandswechsel_
year_anonymization_interval_sec
31536000 Sekunden nach dem der auf den Tag gerundete
Zeitstempel in
lds_dm_data_primary_fsm_zustandswec
hsel gelöscht wird.
Tabelle 54: FSMNormierung – Konfiguration in Tabelle tech_config
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 91 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.3 [DSJ-QSFzG-0030] MdnParser
Paket: de.tollcollect.zme.qs.fzg.rule
Das Parsen und Dekodieren der Monitoring-Nachrichten (MDN) ist in der Klasse MdnParser
implementiert.
Abbildung 29: Job-Ablauf (schematisch): MdnParser
Trigger: Der MdnParser wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
1. Einlesen der Monitoring-Nachrichten (MDN) für das jeweilige Timebucket aus der Tabelle
lds_dm_en_q_mdn.
2. Dekodieren der Detailinformationen je Monitoring-Nachricht, die als base64-kodierter String
im Attribut daten_base64 geliefert werden.
3. Speichern der MDN mit den dekodierten Informationen in der Tabelle fzg_dm_mdn. Dabei
werden Formate wie Zeitangaben etc. an die QS-AV-Datenformate angeglichen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 92 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.4 [DSJ-QSFzG-0040] MdnNormalizer
Paket: de.tollcollect.zme.qs.fzg.rule
Die Normierung inklusive der Anwendung der Normierungsregeln ist in der Klasse MdnNormalizer,
implementiert.
Abbildung 30: Job-Ablauf (schematisch): MdnNormaliser
Trigger: Der MdnNormaliser wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
1. Einlesen der durch den MDNParser aufbereiteten neuen und geänderten MDN-Daten der
Tabelle fzg_dm_mdn.
2. Zwischenspeichern der MDN-Daten in der Hilfstabelle
fzg_mdn_normalisieurng_intermediate_prv für die Durchführung der
Normalisierung.
3. Anwenden der Normalisierungsregeln auf die zwischengespeicherten Daten. Dazu werden
u.a. zum Teil herstellerbezogene Schwellwertprüfungen durchgeführt. Für die Durchführung
der Normalisierung werden Daten aus den folgenden beiden Hilfstabellen herangezogen:
a. fzg_hersteller
b. tech_rule_schwellwert
4. Speichern der Ergebnisse in der Tabelle fzg_mdn_normalisierung_ergebnis
5. Entfernen von Einträgen aus der Hilfstabelle
fzg_mdn_normalisieurng_intermediate_prv.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 93 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
fzg.mdn.1_batt 100 Minimalwert der Batteriespannung für Hersteller mit
ID 1 (Grundig).
fzg.mdn.2_batt 101 Minimalwert der Batteriespannung für Hersteller mit
ID 2 (Siemens).
fzg.mdn.3_batt 102 Minimalwert der Batteriespannung für Hersteller mit
ID 3 (Grundig Pilot).
fzg.mdn.4_batt 103 Minimalwert der Batteriespannung für Hersteller mit
ID 4 (Siemens Pilot).
fzg.mdn.5_batt 104 Minimalwert der Batteriespannung für Hersteller mit
ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_batt 105 Minimalwert der Batteriespannung für Hersteller mit
ID 6 (Grundig 16MB).
fzg.mdn.7_batt 106 Minimalwert der Batteriespannung für Hersteller mit
ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_batt 107 Minimalwert der Batteriespannung für Hersteller mit
ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_batt 108 Minimalwert der Batteriespannung für Hersteller mit
ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_batt 110 Minimalwert der Batteriespannung für Hersteller mit
ID 10 (Siemens 1373++).
fzg.mdn.11_batt 111 Minimalwert der Batteriespannung für Hersteller mit
ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_batt 112 Minimalwert der Batteriespannung für Hersteller mit
ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_batt 113 Minimalwert der Batteriespannung für Hersteller mit
ID 13 (Bosch 2G).
fzg.mdn.14_batt 114 Minimalwert der Batteriespannung für Hersteller mit
ID 14 (Bosch 2G Pilot).
fzg.mdn.1_capa_flash 200 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 1 (Grundig).
fzg.mdn.2_capa_flash 201 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 2 (Siemens).
fzg.mdn.3_capa_flash 202 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_capa_flash 203 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_capa_flash 204 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_capa_flash 205 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 6 (Grundig 16MB).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 94 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key Default Beschreibung
fzg.mdn.7_capa_flash 206 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_capa_flash 207 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_capa_flash 208 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_capa_flash 210 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_capa_flash 211 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_capa_flash 212 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_capa_flash 213 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_capa_flash 214 Minimalwert der freien Speicherkapazität (Flash) für
Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 1 (Grundig).
fzg.mdn.2_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 2 (Siemens).
fzg.mdn.3_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 3 (Grundig Pilot).
fzg.mdn.4_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 4 (Siemens Pilot).
fzg.mdn.5_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 6 (Grundig 16MB).
fzg.mdn.7_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 10 (Siemens 1373++).
fzg.mdn.11_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit
ID 13 (Bosch 2G).
fzg.mdn.14_sat_fs 40 Minimalwert der Satellitenfeldstärke für Hersteller mit 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 95 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key Default Beschreibung
ID 14 (Bosch 2G Pilot).
fzg.mdn.1_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 1 (Grundig).
fzg.mdn.2_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 2 (Siemens).
fzg.mdn.3_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_sat_fs_sprung 10 Maximalwert der Satellitenfeldstärkensprünge für
Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 1 (Grundig).
fzg.mdn.2_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 2 (Siemens).
fzg.mdn.3_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 3 (Grundig Pilot).
fzg.mdn.4_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 4 (Siemens Pilot).
fzg.mdn.5_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 6 (Grundig 16MB).
fzg.mdn.7_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 7 (Siemens DIN-Schacht Pilot).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 96 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key Default Beschreibung
fzg.mdn.8_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 10 (Siemens 1373++).
fzg.mdn.11_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 13 (Bosch 2G).
fzg.mdn.14_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für Hersteller
mit ID 14 (Bosch 2G Pilot).
Tabelle 55: MdnNormalizer - Konfiguration in Tabelle tech_rule_schwellwert
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 97 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.5 [DSJ-QSFzG-0050] FzgVerdichtung
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job FzgVerdichtung hat die Aufgabe FzG-bezogene Daten aus der FSM-Normierung, MDNNormierung und DM-Fehlerevents auf Basis aller Timebuckets eines Tages zu verdichten. Die
Korrelation wird im gleichen Zug durchgeführt, jedoch nicht als Zwischenergebnis persistiert.
Die folgende Abbildung zeigt schematisch die Implementierung des Jobs.
Abbildung 31: Job-Ablauf (schematisch): FzgVerdichtung
Trigger: Die FzG-bezogene Verdichtung (FzgVerdichtung) wird 1x pro Tag durch den
Bookkeeper gestartet.
Ablauf
1. Laden der Daten aus den Quelltabellen lds_dm_en_q_fehler,
fzg_fsm_normalisierung_ergebnis und fzg_mdn_normalisierung_ergebnis.
2. Durchführung der Korrelation für die MDN-Ergebnisse und DM-Fehler
3. Aggregation der Daten
4. Zusammenführung der Aggregationsergebnisse aller Quelltabellen (Verdichtung)
5. Speicherung der Verdichtung in den Zieltabellen fzg_verdichtung_ergebnis und
fzg_verdichtung_ergebnis_by_ehk_id
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 98 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.6 [DSJ-QSFzG-0060] FzgVerdichtungPopulation
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job FzgVerdichtungPopulation erfüllt im Wesentlichen dieselben Aufgaben wie der Job
FzgVerdichtung mit dem Unterschied, dass die Verdichtung nicht für einzelne FzG, sondern gruppiert
nach den FzG-Typen erfolgt (=plattformspezifische Verdichtung).
Die folgende Abbildung zeigt schematisch die Implementierung des Jobs.
Abbildung 32: Job-Ablauf (schematisch): FzGgVerdichtungPopulation
Trigger: Die FzG-Populations-bezogene Verdichtung wird alle 15 Minuten vom Bookkeeper
gestartet.
Ablauf
1. Laden der Daten aus den Quelltabellen lds_dm_en_q_fehler,
fzg_fsm_normalisierung_ergebnis und fzg_mdn_normalisierung_ergebnis.
2. Ermittlung des FzG-Typs aus der Seriennummer und Speicherung in Spalte fzg_typ
3. Durchführung der Korrelation für die MDN-Ergebnisse und DM-Fehler
4. Aggregation der Daten
5. Zusammenführung der Aggregationsergebnisse aller Quelltabellen (Verdichtung)
6. Speicherung der Verdichtung in den Zieltabellen fzg_verdichtung_pop_all_by_zeit,
fzg_verdichtung_pop_fzg_typ und
fzg_verdichtung_pop_fzg_typ_by_fzg_typ
Allgemeine Hinweise
Die Verdichtung basiert auf dem Timebucket und nicht dem Zeitstempel der Daten. Der Job bekommt
vom Bookkeeper nur das Anfangs-Timebucket und berechnet selbst alle 96 Timebuckets für den
ganzen Tag. Die Timebucket-Größe von 15 Minuten ist im Job fest hinterlegt.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 99 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.7 [DSJ-QSFzG-0070] FspProcessor
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job FspProcessor dient der Verarbeitung der aus DM in QS-AV eingehenden FSP-EingangsEvents (Fahrspurdatei-Eingangs-Events). Für jedes FzG soll der Zeitstempel des letzten Empfangs
einer FSP-Datei persistiert werden. Der zuvor letzte Zeitstempel wird dabei überschrieben. Die FSPDatei selbst wird nicht übertragen.
Abbildung 33: Job-Ablauf (schematisch): FspProcessor
Trigger: Der Job wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
1. Einlesen der FSP-Eingangs-Events (15min Timebucket) aus lds_dm_en_q_fsp.
2. Ermittlung des aktuellsten Eintrags pro EHK-Id (FzG) und speichern/überschreiben in
fzg_fsp_eingang. (Es existiert immer nur ein Eintrag pro EHK-Id in der Tabelle
fzg_fsp_eingang).
3. Löschen aller Daten des bearbeiteten Timebucket aus lds_dm_en_q_fsp.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 100 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.8 [DSJ-QSFzG-0080] FzgBewerter
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job FzgBewerter bewertet die Ergebnisse der täglichen FzG-Verdichtung, in der das Auftreten
definierter Fehlerbilder (wie z.B. Geringe Batteriespannung) je EHK-Id gezählt wird. Im Rahmen der
Bewertung wird anhand der konfigurierten Schwellwerte beurteilt, ob die Häufigkeit mit der ein
bestimmtes Fehlerbild auftritt, eine Auffälligkeit darstellt.
Das Ergebnis der Bewertung wird pro FzG-Typ verdichtet. Hierbei wird die Häufigkeit der Fehlerbilder
für jeden FzG-Typ gezählt.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 34: Job-Ablauf (schematisch): FzgBewerter
Trigger: Der FzgBewerter-Job wird 1x am Tag vom Bookkeeper getriggert.
Ablauf
Die Bewertung wird je Fehlerbild folgendermaßen durchgeführt:
1. Laden der Daten der FzG-Verdichtung. Je Fehlerbild werden dabei die aktiven Tage
betrachtet, die via Schwellwert konfiguriert sind
(fzg.bewertung.<Fehlerbild>.aktiveTage)
2. Für jeden aktiven Tag pro Fehlerbild wird geprüft, ob der Fehlerwert >= dem Schwellwert für
die min. Anzahl an fehlerhaften Tagen ist (fzg.bewertung.<Fehlerbild>.fehler). In
diesen Fällen wird der Tag als „auffälliger Tag“ bewertet.
3. Prüfung ob die Anzahl der auffälligen aktiven Tage >= dem Schwellwert der min. Anzahl an
fehlerhaften Tagen im Zeitraum der aktiven Tage ist
(fzg.bewertung.<Fehlerbild>.aktiveTageFehler). Sofern dies gegeben ist, wird
das Fehlerbild in der Verdichtung als positiv (TRUE = auffällig) gewertet, andernfalls als
negativ (FALSE = nicht auffällig)
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 101 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4. Verdichtung der FzG-Bewertung pro FzG-Typ durch Aufsummierung der Fehlerbilder.
5. Speicherung der Ergebnisse in folgenden Cassandra-Tabellen:
a. fzg_bewertung_ergebnis (Tabelle für die Bewertungsergebnisse pro EHK-Id)
b. fzg_bewertung_ergebnis_fehlerfrei (Tabelle die je EHK-Id dokumentiert, ob
mehr als ein Fehlerbild mit TRUE bewertet wurde)
c. fzg_bewertung_verdichtung_pop (Verdichtungen pro fzg_typ)
Allgemeine Hinweise
In der Ausgangstabelle der Verdichtung der FzG-Bewertung fzg_bewertung_verdichtung_pop
kennzeichnet der Wert "alle" eine Verdichtung über alle FzG-Typen und der Wert "kein" eine
Verdichtung über Einträge ohne FzG-Seriennummer (da sich der FzG-Typ aus dieser ableitet).
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert. Sie setzen sich zusammen aus 19 FzG-Fehlerbildern zu je folgenden 3 Kategorien:
 Kategorie aktiveTage: Anzahl der aktiven Tage (aktiver Tag = Eintrag in FzGVerdichtungstabelle existiert), die pro ehk_id bei der Bewertung berücksichtigt werden
 Kategorie fehler: Min. Anzahl an Fehlern pro aktivem Tag, damit der Tag als fehlerhaft
bewertet wird
 Kategorie aktiveTageFehler: Min. Anzahl an fehlerhaften Tagen im Zeitraum der aktiven
Tage, damit die FzG-Bewertung das Fehlerbild mittels TRUE als „auffällig“ bewertet.
Die Schwellwert-Keys die sich daraus ergeben haben folgendes Muster:
fzg.bewertung.<Fehlerbild>.<Kategorie>
Die folgende Tabelle enthält alle Defaultwerte für die Schwellwerte (Mapping von Fehlerbild auf
Kategorie):
Fehlerbild aktiveTage fehler aktiveTageFehler
fallback 1 1 1
dienst_de_aktiv_defekt 1 1 1
sw_version_nicht_aktuell 1 1 1
batt_spannung_gering 5 1 3
dauerplus 1 1 1
keine_tachodaten 5 1 4
keine_gyrodaten 5 1 4
tachodaten_unplausibel 5 1 4
gyrodaten_unplausibel 5 1 4
capa_flash 5 1 4
uw_errorflag 5 1 4
probefahrt_nicht_bestanden 5 1 1
ehk_fehler 5 1 4
gps_fehler 5 1 4
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 102 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Fehlerbild aktiveTage fehler aktiveTageFehler
zeitsprung 5 1 4
wtr_sprung 5 1 4
sat_fs 5 1 4
mac_fehler 1 1 1
loop_fehler 1 1 1
Tabelle 56: FzgBewerter - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Ist der aus dem Start-Timebucket abgeleitete Verarbeitungstag ungültig, da die Zeitangabe nicht
Mitternacht entspricht, wird eine IllegalArgumentException geworfen und der Job abgebrochen.
Ist einer der drei Schwellwerte je Fehlerbild ungültig, wird das Fehlerbild nicht bewertet (Wert null in
Ergebnistabelle) und eine Warnung geloggt (Applikationslog). Folgendes wird als „ungültig“
angesehen:
 Schwellwert ist nicht vorhanden
 Schwellwert ist keine Ganzzahl
 Schwellwert ist kleiner 1
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 103 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.9 [DSJ-QSFzG-0090] DeklarationEventsNormierung
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job DeklarationEventsNormierung ist für die Persistierung und Normierung der via SST 833 von
ES übermittelten Deklarations-Events zuständig.
Persistierung
Je nach Art des Deklarations-Events (Attribut: deklarationsart) gelten unterschiedliche
Persistierungsregeln:
 initial: Werden historisiert abgespeichert
 aktiv: Werden historisiert abgespeichert
 passiv: Dürfen NICHT historisiert abgespeichert werden. Pro FzG und Event-Typ darf es nur
einen Wert geben, alte Werte werden von neuen überschrieben.
Hinweis: Die Eingangsdaten in der LDS-Tabelle lds_es_en_q_deklaration_events werden via
TTL nach 24 Stunden gelöscht.
Normierung: Plausibilitätsprüfung der passiven Events / Setzen von Regelbits
Passive Deklarations-Events werden auf Plausibilität geprüft. Die Ergebnisse der Prüfung werden
persistiert.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 35: Job-Ablauf (schematisch): DeklarationEventsNormierung
Trigger: Der DeklarationEventsNormierung-Job wird alle 15 Minuten vom Bookkeeper gestartet.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 104 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Ablauf
1. Einlesen der Timebuckets gemäß Konfiguration im Job-Schema aus der Eingangstabelle
lds_es_en_q_deklaration_events.
2. Persistierung der Deklarations-Events in folgenden Tabellen:
 fzg_deklaration_events_initial: Historisierte Speicherung der DeklarationsEvents mit deklarationsart=initial
 fzg_deklaration_events_aktiv: Historisierte Speicherung der Deklarations-Events
mit deklarationsart=aktiv
 fzg_deklaration_events_passiv: Speicherung des jeweils neuesten DeklarationsEvents mit deklarationsart=passiv je FzG-Seriennummer und Event-Typ
3. Überprüfung der Plausibilität der Deklarations-Events:
Die Ergebnisse der Plausibilitätsprüfung werden mit FzG-Seriennummer und FzG-Typ aus der
Tabelle lds_dm_fzg_stammdaten angereichert, sofern vorhanden.
Es werden folgende Regelbits berechnet:
 rb_a_p_inkonsistent: Abweichung des deklarierten Werts zum letzten aktiven Event
 rb_p_p_inkonsistent: Abweichung des deklarierten Werts zum letzten passiven
Event
 rb_dekl_p_gewicht_ohne_achs: Passive Gewichtsdeklaration ohne nachfolgende
passive Achsdeklaration (in konfigurierbarem Zeitintervall)
4. Speicherung der Ergebnisse in der Zieltabelle
fzg_deklaration_events_interpretation.
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
fzg.deklaration_events.interpretation.dekl_
p_gewicht_ohne_achs.max_zeitdifferenz_
sekunden
10000 Maximale Zeitdifferenz in Sekunden zwischen
passivem Gewichtsdeklarations-Event und
passivem Achsdeklarations-Event, damit das
Regelbit
rb_dekl_p_gewicht_ohne_achs den Wert
FALSE hat.
Beachte: Obere Grenze nicht eingeschlossen.
Wichtig: Dieser Schwellwert darf nicht größer
sein als die Timebucket-Größe der
Eingangstabelle lds_es_en_q_
deklaration_events. Andernfalls muss die
Bookkeeper-Konfiguration angepasst werden.
Tabelle 57: DeklarationEventsNormierung – Konfiguration in Tabelle tech_rule_schwellwert
Wichtige Hinweise
Der Job wird alle 15 Minuten vom Bookkeeper gestartet. Der Start erfolgt jedoch um 15 Minuten
versetzt, weil für das Regelbit rb_dekl_p_gewicht_ohne_achs in die Zukunft geschaut werden
muss. Der Job erhält somit immer ein Timebucket mehr (neuer) als er zu verarbeiten hat. Es gibt
somit ein Sliding-Window. Das neuste Timebucket wird nicht verarbeitet, sondern dient nur zur
Prüfung des Regelbits. Das Sliding-Window kann über die Bookkeeper-Konfiguration angepasst
werden.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 105 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Beispiele
1. Timebucket-Größe entspricht Job-Aufrufintervall:
 Job bekommt Timebuckets T1, T2 und verarbeitet T1
 Job bekommt Timebuckets T2, T3 und verarbeitet T2
 Job bekommt Timebuckets T3, T4 und verarbeitet T3
 usw.
2. Timebucket- Größe ist 15 Minuten, Job-Aufrufintervall ist 1 Stunde:
 Job bekommt Timebuckets T1, T2, T3, T4, T5 und verarbeitet T1, T2, T3, T4
 Job bekommt Timebuckets T5, T6, T7, T8, T9 und verarbeitet T5, T6, T7, T8
 Job bekommt Timebuckets T9, T10, T11, T12, T13 und verarbeitet T9, T10, T11, T12
 usw.
Fehlerbehandlung
Sollte der Schwellwert
fzg.deklaration_events.interpretation.dekl_p_gewicht_ohne_achs.max_zeitdifferenz_se
kunden nicht gesetzt worden sein, wird eine Exception (SchwellwertException) geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 106 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.10 [DSJ-QSFzG-0100] MonpFzgEventEmitter
Paket: de.tollcollect.zme.qs.fzg.rule.monp
Der Job MonpFzgEventEmitter erzeugt FzG-spezifische Metrik-Events für die Monitoring-Plattform
(MonP) und reiht diese an die QS-AV-interne globale Versand-Warteschlange ein, bevor die Events
asynchron per SST 819 an MonP übertragen werden.
Einen allgemeinen Überblick über die Schnittstelle und die beteiligten Programmteile finden Sie im
Kapitel 6.1.6 SST 819 QS-AV  MonP unter Technische Realisierung der Schnittstelle innerhalb der
QS-AV Applikation.
Informationen zu den zu übertragenen Events entnehmen Sie bitte der Schnittstellenspezifikation im
Referenzdokument [8] [S_819_SST_00] SST 819 SST-Spezifikation QS-AV – MonP.
Die folgende Tabelle listet die Input-Tabellen die für die Event-Erstellung durch den
MonpFzgEventEmitter herangezogen werden.
Input-Tabelle Events
fzg_verdichtung_pop_fzg_typ_by_fzg_typ Populationsanzahlen betreffend (z.B.:
PopAnzDienstDeAktivDefekt)
Tabelle 58: Input-Tabellen der FzG-spezifischen Metrik-Events
Bei der Einreihung der Events in die Versand-Warteschlange werden folgende Output-Tabellen
beschrieben, um den Versand durch den ScheduledMonpSender (siehe auch Kapitel 6.1.6 SST 819
QS-AV  MonP) zu ermöglichen:
Output-Tabelle Aufgabe
tech_monp_pending_metric_event Warteschlange zu versendender Events.
Tabelle 59: Output-Tabellen der FzG-spezifischen der Metrik-Events
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 36: Job-Ablauf (schematisch): MonpFzgEventEmitter
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 107 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Trigger: Der MonpFzgEventEmitter-Job wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
1. Einlesen der Eingangsdaten aus den oben genannten Input-Tabellen, zur Ermittlung des
Event-Inhalts, der für das Monitoring verwendet werden soll (siehe Tabelle 58: Input-Tabellen
der FzG-spezifischen Metrik-Events).
2. Generierung der Metrik-Events gemäß Schnittstellenspezifikation. Dafür werden die Daten in
ein vorgegebenes JSON-Format eingebettet.
3. Einreihung der erstellten Events in die globale Versand-Warteschlange (Tabelle
tech_monp_pending_metric_event).
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
erh.monp.plattform_prefix Plattform Präfix für den FzG Plattformnamen, der im
Event-Inhalt von einigen MonP-Events
verwendet wird. Beispiel: Der Präfix "Plattform"
wird mit dem Herstellernamen „Bosch1G“
zusammengesetzt zu "Plattform Bosch1G".
Tabelle 60: MonpErhEventEmitter – Konfiguration in Tabelle tech_config
Für allgemeine Konfigurationseinstellungen bezüglich der MonP-Schnittstelle siehe Kapitel 6.1.6 SST
819 QS-AV  MonP unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Wichtige Hinweise
Der Inhalt der Metrik-Events ist abhängig von der Befüllung der Tabelle fzg_hersteller. Die
Spalte SENDEN_AN_MONP bestimmt, ob ein FzG-Typ als Gruppierungskriterium und somit Daten für
diesen FzG-Typ in das Event mit aufgenommen werden oder nicht. Die Schreibweise des Herstellers
muss in der Spalte HERSTELLER_NAME_MONP definiert werden. Ist diese Spalte nicht gesetzt (null),
so wird anstelle dessen eine Zusammensetzung aus HERSTELLER_NAME_ANZEIGE und
HERSTELLER_NAME verwendet. Dem ermittelten Herstellernamen wird in jedem Fall das
plattform_prefix aus der Konfiguration vorangestellt, um auf die im Event verwendete
Schreibweise der Keys zu kommen. Aus "Bosch1G" wird somit "Plattform Bosch1G" (siehe
Konfiguration).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 108 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.11 [DSJ-QSFzG-0110] CognosFzgMiscUpdater
Paket: de.tollcollect.zme.qs.fzg.rule.cognos
Der Job stellt FzG-bezogene Kennzahlen zur Anzahl der aktiven FzG, dem Erhebungsmodus
(zME/dME) und zum Kontrollmodus für die Cognos Schnittstelle (SST 884) zur Erstellung des
FzG-Reports bereit.
Einen allgemeinen Überblick über die Schnittstelle und die beteiligten Programmteile finden Sie im
Kapitel 6.1.7 SST 884 QS-AV  Cognos unter Technische Realisierung der Schnittstelle innerhalb
der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 37: Job-Ablauf (schematisch): CognosFzgMiscUpdater
Trigger: Der CognosFzgMiscUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
1. Laden und Zusammenführen (Join) der FSM-Normierungsergebnisse
(fzg_fsm_normalisierung_ergebnis) und der FzG-Stammdaten
(lds_dm_fzg_stammdaten). Betrachtet werden dabei alle Daten aus dem konfigurierten
Betrachtungszeitraum (siehe Konfigurations-Key
fzg.cognos.datenumfang_tage_fzg_misc) ausgehend von den FSMNormierungsergebnissen.
2. Aggregation der Daten je Tag und FzG-Typ:
a. Anzahl aller FzG, die an diesem Tag eine FSM abgesetzt haben
b. Anzahl aller FzG im zME-Modus
c. Anzahl aller FzG im Kontrollmodus CCC_AKTIV
3. Löschen aller Daten aus der Zieltabelle tech_cognos_fzg_misc.
(Hintergrund: Die Daten werden immer für den gesamten Betrachtungszeitraum neu
generiert.)
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 109 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4. Speicherung der neu ermittelten Daten in der Tabelle tech_congos_fzg_misc.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
fzg.cognos.datenumfang_tage_fzg_misc 5 Anzahl der Tage (rückwirkend), für die
FzG-Daten für Cognos bereitgestellt werden
sollen; definiert den Betrachtungszeitraum.
Tabelle 61: CognosFzgMiscUpdater – Konfiguration in Tabelle tech_config
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 110 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.12 [DSJ-QSFzG-0120] CognosFzgMapUpdater
Paket: de.tollcollect.zme.qs.fzg.rule.cognos
Der Job stellt das Mapping zwischen FzG-Typencodes (z.B. 0202) und FzG-Typennamen für die
Cognos Schnittstelle (SST 884) bereit. Das Mapping wird auf Seiten Cognos bei der
Reportgenerierung benötigt, um nicht nur FzG-Typencodes sondern auch die FzG-Typennamen
anzeigen zu können.
Einen allgemeinen Überblick über die Schnittstelle und die beteiligten Programmteile finden Sie im
Kapitel 6.1.7 SST 884 QS-AV  Cognos unter Technische Realisierung der Schnittstelle innerhalb
der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 38: Job-Ablauf (schematisch): CognosFzgMapUpdater
Trigger: Der CognosFzgMapUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
1. Laden der FzG-Hersteller aus der Tabelle fzg_hersteller, die an Cognos übertragen
werden sollen. Sie sind in der Tabelle mit dem Flag senden_an_cognos versehen.
2. Löschen aller Daten aus der Zieltabelle tech_cognos_fzg_map. Es handelt sich bei der
Tabelle um eine Mappingtabelle, bei der immer nur der aktuellste Stand benötigt wird.
3. Erweiterung des Herstellernamens (Attribut hersteller_name_cognos) um das
konfigurierte Präfix (siehe Konfigurations-Key fzg.cognos.fzg_plattform_prefix).
4. Speicherung des Mappings in der Tabelle tech_congos_fzg_map.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
fzg.cognos.fzg_plattform_prefix Plattform Plattform-Präfix für FzG-Herstellernamen, der
bei der Reportgenerierung auf Seiten Cognos
verwendet wird.
Tabelle 62: CognosFzgMiscUpdater – Konfiguration in Tabelle tech_config
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 111 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.13 [DSJ-QSFzG-0130] CognosFzgStatusFehlerUpdater
Paket: de.tollcollect.zme.qs.fzg.rule.cognos
Der Job stellt Kennzahlen zu FzG-bezogenen Fehlerbildern (zME) für die Cognos Schnittstelle (SST
884) bereit.
Einen allgemeinen Überblick über die Schnittstelle und die beteiligten Programmteile finden Sie im
Kapitel 6.1.7 SST 884 QS-AV  Cognos unter Technische Realisierung der Schnittstelle innerhalb
der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 39: Job-Ablauf (schematisch): CognosFzgStatusFehlerUpdater
Trigger: Der CognosFzgStatusFehlerUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
1. Laden der Kennzahlen zu FzG-bezogenen Fehlerbildern aus der Tabelle
fzg_bewertung_verdichtung_pop für den konfigurierten Betrachtungszeitraum (definiert
durch Konfigurations-Key fzg.cognos.datenumfang_tage_fzg_status_fehler).
2. Löschen aller Daten aus der Zieltabelle tech_cognos_fzg_status_fehler.
(Hintergrund: Die Daten werden immer für den gesamten Betrachtungszeitraum neu
generiert.)
3. Speicherung der neu ermittelten Daten in der Tabelle tech_congos_fzg_status_fehler.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
fzg.cognos.datenumfang_tage_fzg_status_fehler 5 Anzahl der Tage (rückwirkend), für die
Kennzahlen zu FzG-Fehlerbilder für Cognos
bereitgestellt werden sollen; definiert den
Betrachtungszeitraum.
Tabelle 63: CognosFzgStatusFehlerUpdater – Konfiguration in Tabelle tech_config
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 112 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.14 [DSJ-QSFzG-0140] CognosRotschaltungenUpdater
Paket: de.tollcollect.zme.qs.fzg.rule.cognos
Der Job stellt Kennzahlen über FzG-bezogene Rotschaltungen (aus technischen Gründen) für die
Cognos Schnittstelle (SST 884) bereit.
Einen allgemeinen Überblick über die Schnittstelle und die beteiligten Programmteile finden Sie im
Kapitel 6.1.7 SST 884 QS-AV  Cognos unter Technische Realisierung der Schnittstelle innerhalb
der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 40: Job-Ablauf (schematisch): CognosRotschaltungenUpdater
Trigger: Der CognosRotschaltungenUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
1. Laden der stündlichen Rotschaltungsverdichtungsdaten für die konfigurierte Anzahl der
vergangenen Tage aus der Tabelle fzg_grund_led_rot_verdichtung_fzgtyp_stunde
2. Aggregation der Verdichtungsdaten auf Nutzungstage: Anzahl der Rotschaltungen je Tag,
FzG-Typ und Rotschaltungsgrund
3. Anreichern des Ergebnisses aus Schritt 2. mit dem Beschreibungstext für den jeweiligen
Rotschaltungsgrund aus der Tabelle fzg_rot_tne_grund (gemappt per
Rotschaltungsgrund-ID)
4. Löschen aller Daten aus der Zieltabelle tech_cognos_rotschaltungen
5. Speicherung der neu ermittelten Daten in der Tabelle tech_congos_rotschaltung
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 113 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
fzg.cognos.datenumfang_tage_rotschaltung 5 Anzahl der Tage für die Cognos Daten aus der
Rotschaltung Verdichtung liefern soll.
Tabelle 64: CognosFzgStatusFehlerUpdater – Konfiguration in Tabelle tech_config
Fehlerbehandlung
Kann für eine Rotschaltungsgrund-ID kein Rotgrundbeschreibungstext aus der Tabelle
fzg_rot_tne_grund ermittelt werden, wird der Ergebnisdatensatz nicht in die Zieltabelle
geschrieben.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 114 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.15 [DSJ-QSFzG-0150] GrundLedRotVerdichtungen
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job verdichtet die Rotschaltungsgründe anhand der Informationen der RsE-Kontakte des Vortags.
Die Verdichtung erfolgt nach FzG-Typ, Rotschaltungsgrund und Nutzungsstunde für RsE-Kontakte,
die einen Kontakt am Vortag verzeichneten. RsE-Kontakte, die verspätet eintreffen, werden ignoriert.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 41: Job-Ablauf (schematisch): GrundLedRotVerdichtungen
Trigger: Der GrundLedRotVerdichtung-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
1. Laden der RsE-Kontakte des Vortags aus der Tabelle lds_es_en_q_rse_kontakt.
2. Verdichtung der Vortagesdaten nach FzG-Typ, Rotschaltungsgrund und Nutzungsstunde.
3. Berechnung folgender Werte basierend auf der Verdichtung:
a. Anzahl der RsE-Kontakte je Rotschaltungsgrund (anz_events)
b. Summe aller RsE-Kontakte für aktive FzGs (anz_aktive_fzg)
c. Prozentualer Anteil von Rotschaltungsgründen basierend auf der Summer aller RsEKontakte für aktive FzG (anteil_grund_led_rot)
4. Speicherung der Verdichtungsergebnisse sowie der Berechnungen in der Tabelle
fzg_grund_led_rot_verdichtung_fzgtyp_stunde
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 115 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.1.16 [DSJ-QSFzG-0160] GrundTneVerdichtung
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job dient der stundenweisen TNE-Grund-bezogenen Verdichtung von MDN (MonitoringNachrichten) des Vortags (TNE=technisch bedingte Nichterfassung). Die Verdichtung erfolgt nach
FzG-Typ, TNE-Grund und Nutzungsstunde für MDN, die am Vortag eingegangen sind. MDN die
verspätet eintreffen, werden ignoriert.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 42: Job-Ablauf (schematisch): GrundTneVerdichtung
Trigger: Der GrundTneVerdichtung-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
1. Laden der geparsten MDN des Vortags aus der Tabelle fzg_dm_mdn.
2. Verdichtung der Vortagesdaten je FzG-Typ, TNE-Grund und Nutzerungsstunde
3. Berechnung folgender Werte basierend auf der Verdichtung:
a. Anzahl MDN je TNE-Grund (anz_events)
b. Summe aller MDN für aktive FzGs (anz_aktive_fzg)
c. Prozentualer Anteil von TNE-Gründen basierend auf der Summer aller MDN für aktive
FzG (anteil_grund_tne)
4. Speicherung der Verdichtungsergebnisse sowie der Berechnungen in der Tabelle
fzg_grund_tne_verdichtung_fzgtyp_stunde
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 116 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.4.2 Anonymisierung, Archivierung und Löschung
Die Anonymisierung, Archivierung und Löschung von personenbezogenen Daten der Komponente
qs-fzg erfolgt via TTLs (Time-To-Live) und durch den Job FzGDeletion.
Vorgaben und Job-Details sind dem Systemlöschkonzept (Referenzdokument [14]) zu entnehmen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 117 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5 qs-erh: Qualitätssicherung der Erhebung
Die Subkomponente qs-erh enthält die Anwendungslogik inklusive der
Qualitätssicherungsalgorithmen für die Qualitätssicherung der Erhebung. Die Implementierung ist
komplett in Scala gehalten und erfolgt innerhalb einzelner Jobs, die durch den Bookkeeper gesteuert
werden.
5.2.5.1 Jobs
In den nachfolgenden Unterkapiteln werden die Jobs für qs-fzg beschrieben. Der Ablauf der
Jobsteuerung kann dem Kapitel 7 Ablaufsicht entnommen werden.
Datenmodell: In den folgenden Unterkapiteln werden Tabellen und Attribute genannt, die im
Datenmodell gelistet sind. Das Datenmodell (Tool-Export) [13] wird zusammen mit den SoftwareLieferungen zur Verfügung gestellt.
Die Ablaufsicht aller Jobs inklusive ihrer Namen, Ausführungsintervalle und Abhängigkeiten ist im
Bookkeeper-Graph dargestellt. Siehe Anhang [BK-Graph].
5.2.5.1.1 [DSJ-QSErh-0010] BetriebsdatenRestImporter
Paket: de.tollcollect.zme.qs.erh.rule.betriebsdaten
Im Zuge des Betriebsdaten-Imports werden die von GDBS bereitgestellten Betriebsdaten-Versionen in
die QS-AV Datenbank importiert. Die Daten werden gemäß [4] [S_814_SST_00] SST 814 SST
Spezifikation GDBS – QS-AV per REST-Schnittstelle abgerufen.
Folgende Betriebsdaten werden importiert:
 Tarifabschnittliste
 Abschnittssequenzen
 Erkennungsmodell
 Release-Historie
 Release-Notes
 Baustellen
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 118 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Abbildung 43: Job-Ablauf (schematisch): BetriebsdatenRestImporter
Trigger: Der Betriebsdaten-REST-Import wird alle 15 Minuten vom Bookkeeper getriggert.
Ablauf
1. URL für REST-Service aus der Umgebungsvariable QSAV_GDBS_URL ermitteln.
2. Folgendes wird für jeweils alle Instanzen (PRODUKTION, VALIDIERUNG) ausgeführt:
a. Release-Historie herunterladen und in Cassandra DB speichern
(erh_gdbs_betriebsdatenhistorie)
b. Fehlende Produkte aus erh_gdbs_betriebsdaten_importer_state ermitteln.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 119 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
c. Fehlende Produkte via REST-Service abrufen, Daten auf Cassandra-Tabellen
mappen und persistieren in Tabellen:
i. erh_gdbs_release_notes
ii. erh_gdbs_mautobjekt
iii. erh_gdbs_mautpunkt
iv. erh_gdbs_abschnittssequenz
v. erh_gdbs_erkennungsobjekte
d. Sofern eine BD-Version mit allen zugehörigen Tarifabschnittslisten erfolgreich
importiert wurde:
i. MonP-Information-Event in MonP-Warteschlange schreiben (Tabelle
tech_monp_pending_information_event).
ii. Änderungshistorie in Tabelle erh_mo_aenderungsstatus schreiben
(geänderte Mautobjekte je BD-Version).
e. Importer-Status in Cassandra DB aktualisieren:
erh_gdbs_betriebsdaten_importer_state
3. Aktualität der Baustellenliste anhand Zeitstempel per REST-Call prüfen und bei Bedarf neue
Baustellenliste via REST-Service abrufen und Daten in Cassandra DB persistieren
(erh_gdbs_baustelle und erh_gdbs_baumassnahmeart).
Allgemeine Hinweise
Für das Caching der Betriebsdaten in der Applikation zur Performance-Optimierung der Regel-Zugriffe
werden die vorhandenen Spark Caching-Mechanismen (LAZY caching) eingesetzt.
Konfiguration
Die Konfiguration erfolgt mittels Umgebungsvariablen. Weitere Details sind im Referenzdokument [16]
[A_QSAV_HBI_00] QS-AV Installationshandbuch beschrieben.
Umgebungsvariable Beschreibung
QSAV_GDBS_URL URL zum GDBS-Rest-Service (SST 814 GDBS – QS-AV)
z.B. http://tem2r68fd.e2e.ux.tc.corp:8443/rest
QSAV_GDBS_USERNAME Benutzername für die Anmeldung am GDBS-Rest-Service
QSAV_GDBS_PASSWORD Passwort zur Anmeldung am GDBS für den GDBS-Benutzer
(QSAV_GDBS_USERNAME)
QSAV_GDBS_ABRUFER Feld ‘Abrufer’ für den Aufruf des GDBS-Rest-Service
(SST 814 GDBS – QS-AV)
z.B. QSAV
GDBS_IGNORE_CERTIFICATE GDBS-Rest-Service (TLS, SST 814 GDBS – QS-AV):
true: Gültigkeit des Zertifikats wird nicht geprüft
false: Gültigkeit des Zertifikats wird geprüft
QSAV_EXTRA_CERTIFICATES Semikolon-separierte Liste mit (Aussteller-) Zertifikatsdateien, die zur
Prüfung des GDBS-Serverzertifikats verwendet werden. Relevant, wenn
der Parameter GDBS_IGNORE_CERTIFICATE=false ist.
Format:
‚/apps/secret-volume/cert1.p12;/apps/secretvolume/cert2.p12;<etc.>’
Tabelle 65: BetriebsdatenRestImporter – Konfiguration der Umgebungsvariablen
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 120 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Weitere Betriebsdaten
 Bei folgenden Tabellen handelt es sich um Tabellen mit Basisdaten, die bei Erstinstallation mit
den Initialskripten in die Datenbank eingespielt werden: erh_mo_monitoring_parameter,
erh_mo_mautobjektklasse, erh_mo_fahrleistungsklassen, erh_mo_ausnahme,
erh_mo_regel. Im weiteren Verlauf werden die Daten manuell durch den Fachbereich
gepflegt.
 Folgende Tabelle dient dem Fachbereich dazu, später manuell zusätzliche Mautobjektklassen
hinzuzufügen: erh_mo_mautobjekt_mautobjektklasse
Fehlerbehandlung
Im Fehlerfall werden alle bislang konsistent eingelesenen Daten persistiert und eine entsprechende
Fehlermeldung geloggt (Applikationslog). Die betroffene Betriebsdaten-Version gilt als nicht
erfolgreich importiert (entsprechender Status in erh_gdbs_betriebsdaten_importer_state)
und wird beim nächsten Import erneut bearbeitet.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 121 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.2 [DSJ-QSErh-0011] GdbsProductsCreator
Paket: de.tollcollect.zme.qs.erh.rule.betriebsdaten
Dieser Job ist für die Erstellung der via SST 814 von QS-AV an GDBS übermittelten
Produktdokumente zuständig (siehe auch Kapitel 6.1.2 SST 814 Grunddaten Bereitstellungs-Server
(GDBS)  QS-AV).
Nach Erstellung reiht der GdbsProductsCreator die Produkte in eine FIFO-Warteschlange in der
Datenbank ein. Der eigentliche Upload nach GDBS erfolgt asynchron durch den Job [DSJ-QSErh0012] GdbsUploadPump.
In der aktuellen Implementierung werden folgenden Produkte erstellt:
 Auffälligkeiten (Modellauffälligkeiten)
 Fahrleistungen (Fahrleistungsklassen der Abschnitte)
Hinweis: Die in der SST-Spezifikation festgelegten Lückenschlussvorschläge werden (noch) nicht
erstellt.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 44: Job-Ablauf (schematisch): GdbsProductsCreator
Trigger: Die Erstellung der ausgehenden GDBS-Produkte wird 1x täglich vom Bookkeeper
getriggert.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 122 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Ablauf
Die Erstellung der Produkte wird innerhalb des Jobs nacheinander ausgeführt.
I. Auffälligkeiten
1. Mit Hilfe des Konfigurationswerts
erh_gdbs_auffaelligkeiten_document_creation_interval_days und des
letzten gespeicherten Produkterstellungszeitpunkts (Tabelle
erh_tech_gdbs_product_creation_state) prüfen, ob das Produkt für die
Auffälligkeiten in diesem Job-Durchlauf erstellt werden soll.
Falls nein, die weiteren Schritte im Abschnitt überspringen.
2. Produktdokument für die Auffälligkeiten anhand der Ergebnisse der Modellbewertung
(aus Tabelle erh_modellbewertung) für alle Nutzungstage seit dem letzten
Erstellungszeitpunkt generieren.
3. Das erstellte Produktdokument als ausstehenden Produkt-Upload in die DatenbankQueue für die asynchrone Verarbeitung durch den Job GdbsUploadPumpe einreihen
(Tabelle erh_tech_gdbs_pending_uploads).
Hinweise: Das Produktdokument wird dabei mit einer UUID versehen. Der Inhalt des
Produktdokuments wird als XML serealisiert und in der Spalte content als BLOB
gespeichert.
4. Den letzten Produkterstellungszeitpunkt in der Datenbank protokollieren (Tabelle
erh_tech_gdbs_product_creation_state).
II. Fahrleistungen
1. Hinweis: Das Produkt „Fahrleistungen“ soll immer am Monatsersten für den
abgelaufenen (Vor-)Monat neu generiert werden.
Anhand des aktuellen Datums prüfen, ob in diesem Job-Durchlauf eine Erstellung
durchgeführt werden soll.
Falls nein, endet der Job mit diesem Schritt.
2. Produktdokument für die Fahrleistungen anhand der Ergebnisse der MOBAVerdichtung (Tabelle erh_moba_mo_verdichtung_tag_by_tag_mo_mvw) in
Verbindung mit den Fahrleistungsklassen (Tabelle
erh_mo_fahrleistungsklassen) für alle Nutzungstage des letzten Monats
generieren.
3. Analog Schritt I.3.
4. Analog Schritt I.4.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
erh_gdbs_auffaelligkeiten_document_creation_
interval_days
1 Bestimmt, wie oft das Produkt „Auffälligkeiten“
erstellt wird. Bei einem Intervall von 1 Tag, sind
alle Auffälligkeiten eines Tages in dem
generierten Produktdokument enthalten.
Tabelle 66: GdbsProductsCreator – Konfiguration in Tabelle tech_config
Fehlerbehandlung
Unbekannte Produkte (alle außer Auffälligkeiten, Fahrleistungen und Lückenschlussvorschläge)
können nicht in die Warteschlage eingereiht werden. Eine entsprechende Log-Meldung wird
ausgegeben.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 123 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Produktdokumente, deren XML-Struktur nicht der XSD gemäß SST-Spezifikation 814 entsprechen,
werden ebenfalls nicht in die Warteschlange eingereiht. Eine Fehlermeldung wird geloggt und eine
IllegalArgumentException geworfen, die zum Job-Abbruch führt.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 124 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.3 [DSJ-QSErh-0012] GdbsUploadPump
Paket: de.tollcollect.zme.qs.erh.rule.betriebsdaten
Dieser Job ist für die Übermittlung der innerhalb QS-AV erstellten Produkte „Auffälligkeiten“,
„Fahrleistungen“ und „Lückenschlussvorschläge“ per Schnittstelle 814 an GDBS zuständig (siehe
auch 6.1.2 SST 814 Grunddaten Bereitstellungs-Server (GDBS)  QS-AV).
Die Produkte werden asynchron vom Job erstellt und in eine FIFO-Warteschlange eingereiht. Der Job
GdbsUploadPump holt fertig erstellte Produkte aus der Warteschlange und versendet sie an GDBS.
Die folgende Abbildung stellt dem Jobablauf schematisch dar.
Abbildung 45: Job-Ablauf (schematisch): GdbsUploadPump
Trigger: Der Job GdbsUploadPump wird stündlich vom Bookkeeper getriggert.
Ablauf
1. Ausstehende Produkt-Uploads aus der Warteschlange abfragen (Tabelle
erh_tech_gdbs_pending_uploads).
Falls keine Uploads ausstehen, endet der Job mit diesem Schritt.
2. Ausstehende Produkt-Uploads durchführen, siehe auch Referenzdokument [4]
[S_814_SST_00] SST 814 SST Spezifikation GDBS – QS-AV.
Hinweise: Fehlgeschlagene Uploads werden in der Warteschlange belassen, ein RetryCounter wird hochgezählt (num_trials).
Ist die laut Konfigurationseinstellung erh_gdbs_upload_max_retries maximale Anzahl
an Versuchen erreicht, wird der betreffende ausstehende Produkt-Upload aus der
Warteschlange entfernt.
Erneute Upload-Versuche nach einem Fehlschlag werden frühestens im Rahmen des
nächsten Job-Ausführungsintervalls und nach Verstreichen des konfigurierten Retry-Intervalls
(erh_gdbs_upload_retry_interval_s) unternommen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 125 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
erh_gdbs_upload_retry_interval_s 3600 Intervall nachdem ein nicht erfolgreicher Upload
frühestens wiederholt werden darf. Der Wert
muss immer in Verbindung mit dem
Ausführungsintervall des GdbsUploadPump
betrachtet werden.
 Retry frühestens nach letztem Uploadversuch
plus Verstreichen von
erh_gdbs_upload_retry_interval_s im
Rahmen einer regulären Jobausführung.
erh_gdbs_upload_max_retries 120 Maximale Anzahl an Versuchen bevor ein
Produkt-Upload verworfen wird.
Tabelle 67: GdbsUploadPump – Konfiguration in Tabelle tech_config
Fehlerbehandlung
Unbekannte Produkte (alle außer Auffälligkeiten, Fahrleistungen und Lückenschlussvorschläge)
werden nicht übertragen und entsprechende Produkt-Uploads aus der Warteschlagen entfernt. Eine
entsprechende Log-Meldung wird ausgegeben.
Technischen Übertragungsfehler werden je Produkt-Upload gefangen und als Warnung geloggt.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 126 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.4 [DSJ-QSErh-0020] ProcessONQ15Job
Paket: de.tollcollect.zme.qs.erh.rule.onq
Überblick ONQ
Die Berechnung der Operativen Nichterkennungsquote (ONQ) und die Verdichtung der Ergebnisse
erfolgt in zwei Stufen mit unterschiedlicher Taktung. Die folgende Abbildung soll dies
veranschaulichen.
Abbildung 46: Überblick ONQ
Über die Schnittstelle 833 empfängt QS-AV die vom Erkenner übermittelten RsE-Kontakte. Sie
werden durch den Job [DSJ-QSErh-0100] ProcessRSEIdKorrektur aus der Cassandra-Tabelle
lds_es_en_q_rse_kontakt gelesen, ggf. korrigiert und in der Tabelle
erh_onq_rse_kontakt_id_korrigiert gespeichert.
Der Bookkeeper startet den Job ProcessONQ15Job alle 15 Minuten. Der Job verarbeitet in jedem
Durchgang ein 15 Minuten umfassendes Timebucket an korrigierten RsE-Kontakten. Er bewertet für
jeden RsE-Kontakt anhand der Sollzuordnung (erh_onq_sollzuordnung_mo_tag und
erh_onq_sollzuordnung_eo_tag), ob eine Erkennung oder Nicht-Erkennung vorliegt und
berechnet im Anschluss die ONQ bezogen auf Maut- und Erkennungsobjekte und speichert diese in
der Tabelle erh_onq_15min_2. Als Zwischenergebnis aktualisiert der Job die Tabelle
erh_onq_soll_ist_vergleich_hlp, die zur Verdichtung der Quoten durch den Job
ProcessONQ24Job benötigt wird.
Einmal täglich startet der Bookkeeper den Job ProcessOnqSollzuordnungJob. Dieser verarbeitet das
24 Stunden umfassende Timebucket des vorigen Tages an RsE-Kontakten, aktualisiert die
Sollzuordnungen zwischen Maut- sowie Erkennungsobjekten und RsE anhand der gesammelten 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 127 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Daten und speichert sie in den Tabellen erh_onq_sollzuordnung_mo_tag und
erh_onq_sollzuordnung_eo_tag sowie erh_onq_sollzuordnung_eo_mo_tag (letztere wird
für die Verarbeitung innerhalb des ProcessDSRC15Job benötigt).
Einmal täglich startet der Bookkeeper den Job ProcessONQ24Job. Dieser verdichtet die Ergebnisse
aus dem Soll-Ist-Vergleich des Vortages und speichert sie in den Zieltabellen des Jobs.
Umsetzung
Die Berechnung der ONQ ist in der Klasse ProcessONQ15Job, wie folgt implementiert:
Abbildung 47: Job-Ablauf (schematisch): ProcessONQ15Job
Trigger: Die Berechnung der ONQ wird alle 15 Minuten vom Bookkeeper getriggert.
Ablauf
1. Lesen der RsE-Kontakte (15min Timebucket) aus der Quelltabelle
(erh_onq_rse_kontakt_id_korrigiert).
Hinweis: Es werden nur produktive RsE-Kontakte (BD-Instanz = Produktion) berücksichtigt.
Dubletten und gesperrte RsE-Kontakte werden ignoriert.
2. Soll-Ist-Bewertung für alle eingelesenen RsE-Kontakte mit Hilfe der Sollzuordnung
(erh_onq_sollzuordnung_mo_tag und erh_onq_sollzuordnung_eo_tag)
durchführen und in der Tabelle erh_onq_soll_ist_vergleich_hlp speichern.
3. Anhand der Soll-Ist-Bewertung der eingelesenen RsE-Kontakte werden die Gut- und
Schlechtfälle bei der Erkennung aufsummiert. Per Formel werden daraus die Operativen
Nichterkennungsquoten (ONQ) für das aktuelle Timebucket (15min) berechnet und in der
Tabelle erh_onq_15min_2 gespeichert.
Die Ergebnisse werden dabei gruppiert nach FzG-Typ, LED-Farbe (rot, grün oder gesamt),
Objekt-Typ (MO, EO oder alle) und Straßentyp (BAB, BS oder gesamt).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 128 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.onq.led_rot_grund_query 19028,
19478,
2620
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgründe (Nummern)
weisen auf manuelles Verfahren hin.
erh.onq.led_rot_grund_rule_sperre_query 33420,
35376
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgründe (Nummern)
indizieren gesperrte FzG.
erh.onq.bd_instanz_produktion 1 RsE-Kontakte mit bd_instanz=1 stammen aus
der produktiven BD-Instanz.
erh.onq.konau_nummernkreis_oben 5000 Schwellwert für den Nummernkreis der RsE-ID
(oben) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten 4000 Schwellwert für den Nummernkreis der RsE-ID
(unten) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben 8000 Schwellwert für den Nummernkreis der RsE-ID
(oben) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten 6000 Schwellwert für den Nummernkreis der RsE-ID
(unten) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.zeit_onq_sek 3600 Anzahl der Sekunden rückwirkend des RSEKontaktzeitpunkts, der für die Berechnung
betrachtet wird.
erh.onq.led_farbe_rot 0 LED-Farbe der RsE-Kontakte mit led_farbe=0
ist rot.
erh.onq.led_farbe_gruen 1 LED-Farbe der RsE-Kontakte mit led_farbe=1
ist grün.
Tabelle 68: ProcessONQ15 - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 129 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.5 [DSJ-QSErh-0021] ProcessOnqSollzuordnungJob
Paket: de.tollcollect.zme.qs.erh.rule.onq
Der Job ProcessOnqSollzuordnungJob berechnet anhand der RsE-Kontakte des Vortages die
Sollzuordnungen zwischen RsE-Id und Mautobjekten bzw. Erkennungsobjekten, die als Grundlage für
die Berechnung der ONQ sowie der DSRC-Quoten dienen.
Der Job ist in der Klasse ProcessOnqSollzuordnungJob, wie folgt implementiert:
Abbildung 48: Job-Ablauf (schematisch): ProcessOnqSollzuordnungJob
Trigger: Der Job ProcessOnqSollzuordnungJob wird 1x täglich vom Bookkeeper getriggert.
Ablauf
1. Einlesen der RsE-Kontakte des gestrigen Tages aus der Quelltabelle
(erh_onq_rse_kontakt_id_korrigiert).
Hinweis: Es werden nur produktive RsE-Kontakte (BD-Instanz = Produktion) berücksichtigt.
Dubletten und gesperrte RsE-Kontakte werden ignoriert.
2. Ermittlung der Sollzuordnungen: Die Sollzuordnungen werden unter Anwendung der
Schwellwerte berechnet: Die dabei jeweils am häufigsten auftretende Kombination von MO-ID
bzw. EO-ID und RsE-Id wird als Sollzuordnung gesetzt.
3. Die Sollzuordnungen in den Zieltabellen speichern.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 130 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.onq.led_rot_grund_query 19028,
19478,
2620
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgründe (Nummern)
indizieren manuelles Verfahren.
erh.onq.bd_instanz_produktion 1 RsE-Kontakte mit bd_instanz=1 stammen aus
der produktiven BD-Instanz.
erh.onq.konau_nummernkreis_oben 5000 Schwellwert für den Nummernkreis der RsE-ID
(oben) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten 4000 Schwellwert für den Nummernkreis der RsE-ID
(unten) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben 8000 Schwellwert für den Nummernkreis der RsE-ID
(oben) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten 6000 Schwellwert für den Nummernkreis der RsE-ID
(unten) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.anz_gruen 3 Schwellwert für die Mindestanzahl an RsEKontakten mit led_farbe gruen für eine RsEID.
erh.onq.ant_ausfahrt 20 Schwellwert für die erlaubte Quote an Ausfahrten
für eine RsE.
erh.onq. anz_rse_gesamt_eo 280 Anzahl der RSE-Kontakte, die mindestens
notwendig sind, um eine neue EO-Sollzuordnung zu
berechnen.
erh.onq. anz_rse_gesamt_mo 280 Anzahl der RSE-Kontakte, die mindestens
notwendig sind, um eine neue MO-Sollzuordnung
zu berechnen.
erh.onq. anz_rse_gesamt_eo_mo 280 Anzahl der RSE-Kontakte, die mindestens
notwendig sind, um eine neue EO-MOSollzuordnung zu berechnen.
erh.onq.led_farbe_gruen 1 LED-Farbe der RsE-Kontakte mit led_farbe=1
ist grün.
erh.onq.led_rot_grund_rule_sperre_query 33420,
35376
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgründe (Nummern)
indizieren gesperrte FzG.
erh.onq.erhebungsparameterabfahrt 2 RsE-Kontakte mit erhebungsparameter=2
indizieren Abfahrten.
Tabelle 69: ProcessOnqSollzuordnungJob - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 131 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.6 [DSJ-QSErh-0030] ProcessONQ24Job
Paket: de.tollcollect.zme.qs.erh.rule.onq
Der Job ProcessONQ24Job verdichtet die Ergebnisse des Jobs ProcessONQ15Job tageweise. Einen
Überblick über die Zusammenhänge der ONQ-Berechnung finden Sie im Kapitel 5.2.5.1.4 [DSJQSErh-0020] ProcessONQ15Job unter Überblick.
Der Job ist in der Klasse ProcessONQ24Job, wie folgt implementiert:
Abbildung 49: Job-Ablauf (schematisch): ProcessONQ24Job
Trigger: Der Job ProcessONQ24Job wird 1x täglich vom Bookkeeper getriggert.
Ablauf
1. Ergebnisse des Soll-Ist-Vergleichs des gestrigen Tages
(erh_onq_soll_ist_vergleich_hlp) einlesen.
2. Kennzahlen und Quoten für die letzten 24 Stunden berechnen, verdichten und in den
Zieltabellen speichern.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 132 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.onq.konau_nummernkreis_oben 5000 Schwellwert für den Nummernkreis der RsE-ID
(oben), zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten 4000 Schwellwert für den Nummernkreis der RsE-ID
(unten), zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben 8000 Schwellwert für den Nummernkreis der RsE-ID
(oben), zur Kennzeichnung der Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten 6000 Schwellwert für den Nummernkreis der RsE-ID
(unten), zur Kennzeichnung der Kontrollart KonSL.
erh.onq.led_farbe_rot 0 LED-Farbe der RsE-Kontakte mit led_farbe=0
ist rot.
erh.onq.led_farbe_gruen 1 LED-Farbe der RsE-Kontakte mit led_farbe=1
ist grün.
Tabelle 70: ProcessONQ24Job - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 133 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.7 [DSJ-QSErh-0040] ProcessDSRC15Job
Paket: de.tollcollect.zme.qs.erh.rule.onq
Überblick DSRC-Quotenberechnung
Die Berechnung der Dedicated Short Range Communication Quote (DSRC-Quote) und die
Verdichtung der Ergebnisse erfolgt in zwei Stufen mit unterschiedlicher Taktung. Die folgende
Abbildung soll dies veranschaulichen.
Abbildung 50: Überblick DSRC-Quotenberechnung
Über die Schnittstelle 833 empfängt QS-AV die vom Erkenner übermittelten Befahrungsparameter und
RsE-Kontakte. Die RsE-Kontakte werden durch den Job [DSJ-QSErh-0100] ProcessRSEIdKorrektur
aus der Cassandra-Tabelle lds_es_en_q_rse_kontakt gelesen, ggf. korrigiert und in der Tabelle
erh_onq_rse_kontakt_id_korrigiert gespeichert.
Der Bookkeeper startet den Job ProcessDSRC15Job alle 15 Minuten. Der Job verarbeitet in jedem
Durchgang ein 15 Minuten umfassendes Timebucket an Befahrungsparametern. Er bewertet für jeden
Befahrungsparameter anhand der Sollzuordnung (erh_onq_sollzuordnung_mo_tag und
erh_onq_sollzuordnung_eo_mo_tag), ob ein RsE-Kontakt erwartet wird und gleicht diesen
Erwartungswert mit den RsE-Kontakten ab (erh_onq_rse_kontakt_id_korrigiert). Aus dem
Ergebnis kann im Anschluss die DSRC-Quote berechnet werden. Dabei werden RsE-Kontakte aus
der konfigurierten Blacklist (erh_onq_dsrc_blacklist_rse) ignoriert. Als Zwischenergebnis
aktualisiert der Job die Tabelle erh_onq_dsrc_soll_ist_vergleich_2, die zur täglichen
Verdichtung der Quoten durch den Job ProcessDSRC24Job benötigt wird.
Einmal täglich um 0 Uhr startet der Bookkeeper den Job ProcessDSRC24Job. Dieser verarbeitet die
24 Stunden umfassenden Timebuckets des vorigen Tages aus der Tabelle
erh_onq_dsrc_soll_ist_vergleich_2 und verdichtet die DSRC-Quoten des vergangenen
Tages stündlich nach RsE-Id, nach EHK-Id. Auch hier werden RsE-Kontakte aus der konfigurierten
Blacklist (erh_onq_dsrc_blacklist_rse) ignoriert.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 134 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Abbildung 51: Job-Ablauf (schematisch): ProcessDSRC15Job
Trigger: Die DSRC-Quotenberechnung wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
1. Laden der Befahrungsparameter (in drei 5min Timebuckets) aus
lds_es_en_q_befahrungsparameter_eo.
2. Laden der aktuellen MO-Sollzuordnung (erh_onq_sollzuordnung_mo_tag) und EO/MORsE-Sollzuordnung (erh_onq_sollzuordnung_eo_mo_tag), sofern vorhanden, als
Referenz zu den Befahrungsparametern.
3. Für jeden Befahrungsparameter wird überprüft, ob für das darin gespeicherte letzte
Mautobjekt (MO) bzw. Erkennungsobjekt (EO) eine Sollzuordnung in den Tabellen
erh_onq_sollzuordnung_(eo_)mo_tag vorliegt. Dabei gilt die Einschränkung, dass nur
Befahrungsparameter betrachtet werden, deren Befahrungszeit nicht länger, als die mittels
des Schwellwerts erh.onq.schwellwert_erhebung_tage_zurueck konfigurierte
Zeitspanne zurück liegt. Für die weitere Quotenberechnung und Verdichtung werden nur
Befahrungsparameter aus dem konfigurierten Befahrungszeitraum verwendet, für die eine
Sollzuordnung ermittelt werden konnte.
4. Laden der RsE-Kontakte aus erh_onq_rse_kontakt_id_korrigiert.
5. Die in Schritt 3 indirekt ermittelten Befahrungsparameter-RsE-Soll-Zuordnungen werden mit
den tatsächlich registrierten RsE-Kontakten (erh_onq_rse_kontakt_id_korrigiert)
abgeglichen. Bei Übereinstimmung wird ein Gutfall gezählt, ansonsten wird ein Schlechtfall 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 135 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
gezählt. Die Ergebnisse werden in der Tabelle erh_onq_dsrc_soll_ist_vergleich_2
gespeichert.
6. Die Daten der Tabelle erh_onq_dsrc_soll_ist_vergleich_2 der letzten
Viertelstunde werden mit folgenden Gruppierungskriterien verdichtet: Anzahl der Erhebungen
per RsE-Id, Gut- und Schlechtfälle. Das Ergebnis der Verdichtung wird in der Tabelle
erh_onq_dsrc_15min gespeichert. Dabei werden RsE-Kontakte aus der konfigurierten
Blacklist (erh_onq_dsrc_blacklist_rse) ignoriert.
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.onq.led_rot_grund_query 19028,
19478,
2620
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgründe (Nummern)
indizieren manuelles Verfahren.
erh.onq.bd_instanz_produktion 1 RsE-Kontakte mit bd_instanz=1 stammen
aus der produktiven BD-Instanz.
erh.onq.konau_nummernkreis_oben 5000 Schwellwert für den Nummernkreis der RsE-ID
(oben) , zur Kennzeichnung der Kontrollart
KonAu.
erh.onq.konau_nummernkreis_unten 4000 Schwellwert für den Nummernkreis der RsE-ID
(unten) , zur Kennzeichnung der Kontrollart
KonAu.
erh.onq.konsl_nummernkreis_oben 8000 Schwellwert für den Nummernkreis der RsE-ID
(oben) , zur Kennzeichnung der Kontrollart
KonSL.
erh.onq.konsl_nummernkreis_unten 6000 Schwellwert für den Nummernkreis der RsE-ID
(unten) , zur Kennzeichnung der Kontrollart
KonSL.
erh.onq.led_rot_grund_rule_sperre_query 33420,
35376
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgründe (Nummern)
indizieren gesperrte FzG.
erh.onq.schwellwert_erhebung_tage_zurueck 5 Zeitraum in Tagen, wie lange ein
Befahrungsparameter rückwirkend für die
Berechnungen herangezogen werden darf.
erh.onq.rse_suchfenster_bs 3600 Sekunden rückwirkend zum RSEKontaktzeitpunkt: Festlegung des Starts des
Suchfensters für die Berechnung der DSRCQuote (Ende = Kontaktzeitpunkt).
Tabelle 71: ProcessDSRC15Job - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 136 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.8 [DSJ-QSErh-0050] ProcessDSRC24Job
Paket: de.tollcollect.zme.qs.erh.rule.onq
Der Job ProcessDSRC24Job dient der täglichen Verdichtung der Ergebnisse des Jobs
ProcessDSRC15Job. Einen Überblick über die DSRC-Quotenberechnung und die Abhängigkeiten der
Jobs finden Sie im Kapitel 5.2.5.1.7 [DSJ-QSErh-0040] unter Überblick DSRC.
Der Job ProcessDSRC24Job ist in der Klasse ProcessDSRC24Job, wie folgt implementiert:
Abbildung 52: Job-Ablauf (schematisch): ProcessDSRC24Job
Trigger: Die Verdichtung wird 1x täglich vom Bookkeeper gestartet.
Ablauf
1. Laden aller viertelstündlichen Ergebnisse des Jobs ProcessDSRC15Job
aus erh_onq_dsrc_soll_ist_vergleich_2 für den vergangenen Tag sowie der
Blacklist mit den auszuschließenden RsE-IDs (erh_onq_dsrc_blacklist_rse).
Wichtig: RsE-Kontakte, die in der Blacklist konfiguriert sind, werden für die folgenden
Verdichtungen ausgeschlossen.
2. Verdichtung der Daten per RsE-ID und je FzG-Typ-Aspekten (alle/prod/pilot),
Berechnung der DSRC-Quoten für den gesamten Tag und Speicherung in der Tabelle
erh_onq_dsrc_verdichtung_rse_2.
3. Verdichtung der Daten per EHK-ID, Berechnung der DSRC-Quote für den gesamten Tag und
Speicherung in der Tabelle erh_onq_dsrc_verdichtung_fzg_2. Dabei ist der
Schwellwert erh.onq.schwellwert_dsrc_quote zu beachten: Für die Verdichtung dürfen
nur Daten zu RsEs mit einer DSRC-Quote größer des angegebenen Schwellwertes
berücksichtigt werden, um die FzG-bezogenen Verdichtungsergebnisse nicht zu verfälschen.
4. Verdichtung der Daten per FzG-Typ und Stunde und Speicherung in der Tabelle
erh_onq_dsrc_verdichtung_stunde.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 137 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.onq.dsrc_quote 98 Mindestwert für die DSRC-Quote einer RsE,
damit sie für die FzG-bezogene DSRC-QuotenVerdichtung berücksichtigt wird.
Hintergrund: Für die Verdichtung dürfen nur
Daten zu RsEs mit einer DSRC-Quote größer
des angegebenen Schwellwertes berücksichtigt
werden, um die FzG-bezogenen
Verdichtungsergebnisse nicht zu verfälschen.
erh.onq.konau_nummernkreis_oben 5000 Schwellwert für den Nummernkreis der RsE-ID
(oben), zur Kennzeichnung der Kontrollart
KonAu.
erh.onq.konau_nummernkreis_unten 4000 Schwellwert für den Nummernkreis der RsE-ID
(unten), zur Kennzeichnung der Kontrollart
KonAu.
erh.onq.konsl_nummernkreis_oben 8000 Schwellwert für den Nummernkreis der RsE-ID
(oben), zur Kennzeichnung der Kontrollart
KonSL.
erh.onq.konsl_nummernkreis_unten 6000 Schwellwert für den Nummernkreis der RsE-ID
(unten), zur Kennzeichnung der Kontrollart
KonSL.
Tabelle 72: ProcessONQ24 - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 138 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.9 [DSJ-QSErh-0060] MobaEventCreator
Paket: de.tollcollect.zme.qs.erh.rule
Es werden MOBA-Events zur Aufdeckung von Ein-Abschnitts-Erhebungslücken und
Fehlvergebührungen erzeugt. Die Events der Klasse MobaEventCreator werden durch einen
Vergleich erhobener Abschnitte (Befahrungsparameter vom Erkenner) mit der Abschnittstopologie
aus der Abschnittssequenz (aus den Betriebsdaten) aufgedeckt. Befahrungsparameter, wie z.B.
Befahrungszeit des Abschnittes und Erhebungsparameter (Einfahrt, Durchfahrt, Ausfahrt) fließen
dabei mit ein.
Abbildung 53: Job-Ablauf (schematisch): MobaEventCreator
Trigger: Die Erzeugung der MOBA-Events wird alle 5 Minuten vom Bookkeeper in Abhängigkeit
der eingehenden Befahrungsparameter vom ES getriggert.
Ablauf
1. Befahrungsparameter des aktuellen Timebuckets aus der Quelltabelle
(lds_es_en_q_befahrungsparameter_eo) lesen.
Hinweis: Alle Befahrungsparameter mit aktuellemAbschnitt.Befahrungszeit älter als
dem Betrachtungszeitraum (erh.moba.betrachtungszeitraum_tage) werden ignoriert.
2. MOBA-Regeln ausführen und potentielle MOBA-Events generieren:
a. Regel 120 – Ungültiger Streckenabschnitt
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 139 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
b. Regel 131 – Erhebungsparameter Einfahrt an Streckenabschnitt ohne
Startmöglichkeit
c. Regel 132 – Erhebungsparameter Ausfahrt an Streckenabschnitt ohne Zielmöglichkeit
d. Regel 240 – Doppelt gebuchte Abschnitte
e. Regel 241 – Doppelt gebuchter Abschnitt, ED
f. Regel 310 – Ein-Abschnitts-Lücken mit falscher Aus-/Einfahrtskennung
g. Regel 313 – Ein-Abschnitts-Lücke mit doppelt gebuchtem Abschnitt ED
h. Regel 318 – Ein-Abschnitts-Lücke mit doppelt gebuchtem Abschnitt DD
i. Regel 323 – Sichere Ein-Abschnitts-Lücke mit falscher Kennung ED
j. Regel 328 – Sichere Ein-Abschnitts-Lücke mit falscher Kennung DD
k. Regel 350 – Falsche Aus-Einfahrtskennung, AE
3. Potentielle MOBA-Events aus Schritt 2) gemäß Ausnahmeliste (erh_mo_ausnahme) filtern.
4. Finale MOBA-Events mit Daten aus Fzg-Stammdaten (lds_dm_fzg_stammdaten) und
Regelliste (erh_mo_regel) anreichern, Ausnahmestatistik aktualisieren.
5. MOBA-Events in Cassandra DB speichern (erh_moba_event: unter Verwendung des QuellTimebuckets der Befahrungsparameter), Ausnahmestatistik in Cassandra DB aktualisieren
(erh_moba_ausnahme_statistik).
Hinweis: Die Tabelle erh_moba_ausnahme_statistik_historized ist eine Hilfstabelle zur
Aktualisierung der Ausnahmestatistik (Idempotenz).
Allgemeine Hinweise
 Nutzungstag und Nutzungsstunde eines MOBA Events werden aus Befahrungszeit des ersten
aktuellen Abschnitts berechnet.
 Zu Gültigkeitsangaben in den Regeln siehe auch Kapitel 10.12.1 Gültigkeitsbereiche.
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.moba.betrachtungszeitraum_tage 5 Konfiguration des Betrachtungszeitraums. Es dürfen
nur Befahrungen in MOBA und in der Verdichtung
berücksichtigt werden, die gemessen an der
Befahrungszeit nicht älter sind, als die konfigurierbare
Anzahl von Tagen (Default-Wert).
Tabelle 73: MobaEventCreator - Konfiguration in Tabelle tech_rule_schwellwert
Die Konfiguration einer Default-Mautobjektklasse erfolgt in der Cassandra-Tabelle tech_config
mittels des folgenden Keys:
Key Default Beschreibung
default.mautobjektklasse 1 Default-Wert, der als Mautobjektklasse gesetzt wird,
wenn für ein Mautobjekt keine Mautobjektklasse
ermittelt werden konnte (in Tabelle
erh_mo_mautobjekt_mautobjektklasse).
Tabelle 74: MobaEventCreator - Konfiguration in Tabelle tech_config
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 140 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Fehlerbehandlung
 Folgender Fehler führt zum Abbruch der MOBA-Event-Erzeugung für das aktuelle Timebucket
(Exception in Richtung Bookkeeper):
Formatfehler für die BD-Versionen in den Befahrungsparametern.
 Folgende Inkonsistenz führt zur Verwerfung eines Befahrungsparameters für die MOBAEvent-Erzeugung:
Betriebsdaten gemäß BD-Version des Befahrungsparameters sind nicht in QS-AV vorhanden.
 Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 141 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.10 [DSJ-QSErh-0070] MobaVerdichtungJob
Paket: de.tollcollect.zme.qs.erh.rule
Der Job MobaVerdichtungJob führt in unterschiedlichen Granularitäten die Verdichtung der MOBAEreignisse basierend auf den Befahrungsparameter und den MOBA-Events durch. Die
Befahrungsparameter werden dabei jeweils nach Befahrungszeitpunkt der Abschnitte gefiltert. Ziel
der Verdichtung ist es eine Bewertung des Optimierungspotenzials des Erkennungsmodells
vornehmen zu können.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 54: Job-Ablauf (schematisch): MobaVerdichtungJob
Trigger: Die MOBA-Verdichtung wird stündlich vom Bookkeeper gestartet.
Ablauf
1. Laden der 12 abgeleiteten Timebuckets von lds_es_en_q_befahrungsparameter_eo
und erh_moba_event.
2. Es werden nur Befahrungsparameter weiterverarbeitet, deren Befahrungszeit des aktuellen
Abschnitts nicht mehr Tage zurückliegt, als der Schwellwert
erh.moba.betrachtungszeitraum_tage vorgibt. Der aktuelle Tag wird vom InputTimebucket abgeleitet (bereitgestellt durch den Job MobaEventCreator).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 142 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3. Durchführung der FzG-bezogenen Verdichtungen mit Befahrungsparametern gefiltert nach
BD-Instanz „Produktion“ und Speicherung der Ergebnisse in den entsprechenden Zieltabellen.
4. Durchführung der Regelverdichtung für Erkennungsobjekte pro FzG und pro MO und
Speicherung der Ergebnisse in den entsprechenden Zieltabellen.
5. Durchführung der Erkennungsobjektverdichtung pro Stunde und pro Tag und Speicherung der
Ergebnisse in den entsprechenden Zieltabellen.
Hinweise:
Alle FzG-bezogenen Verdichtungen werden mit einer TTL persistiert (gemäß Konfigurationswert
erh.fzg_verdichtung.deletion_interval_sec).
Bei den Tabellen *_hist handelt es sich um Hilfstabellen zur Aktualisierung der Verdichtungen
(Idempotenz). Auch die täglichen Verdichtungen werden stündlich aktualisiert.
Allgemeine Hinweise
 Defaultwerte für einzelne Mautobjekte werden vom Job
MoVerdichtungDefaultValuesSetter zu Beginn eines Tages geschrieben und von
diesem überschrieben.
 Da die Verdichtungen der MOBA- und der EOBA-Events ähnliche Abläufe und Strukturen
aufweisen, wurde durch die Klasse ModellVerdichter eine gemeinsame Basisklasse
implementiert, welche die jeweilige Verdichtung (EOBA/MOBA) steuert.
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.moba.betrachtungszeitraum_tage 5 Konfiguration des Betrachtungszeitraums. Es
dürfen nur Befahrungen in MOBA und in der
Verdichtung berücksichtigt werden, die gemessen
an der Befahrungszeit nicht älter sind, als die
konfigurierbare Anzahl von Tagen (Default-Wert).
Tabelle 75: MobaVerdichtungJob - Konfiguration in Tabelle tech_rule_schwellwert
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
erh.fzg_verdichtung.deletion_interval_sec 10368000
(120 Tage)
Sekunden nach denen Werte in
Verdichtungstabellen von EOBA/MOBA und ONQ
mit FzG-Bezug via EHK-ID gelöscht werden.
Tabelle 76: MobaVerdichtungJob – Konfiguration in Tabelle tech_config
Fehlerbehandlung
 Existiert einer der Schlüssel erh.fzg_verdichtung.deletion_interval_sec
oder erh.moba.betrachtungszeitraum_tage nicht in der Konfiguration, sind keine
Werte gesetzt oder sind die Werte nicht vom Typ Integer, wird eine Exception in Richtung
Bookkeeper geworfen und der Job abgebrochen.
 Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 143 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.11 [DSJ-QSErh-0080] MoVerdichtungDefaultValueSetter
Paket: de.tollcollect.zme.qs.erh.rule
Dieser Job hat die Aufgabe zur Vorbereitung der MOBA-Verdichtung in den Tabellen
erh_moba_mo_verdichtung_tag und erh_moba_mo_verdichtung_stunde für den
kommenden Tag Defaultwerte für alle Mautobjekte zu hinterlegen. Der Job
MoVerdichtungDefaultValuesSetter muss aus diesem Grund jeden Tag vor dem
MobaVerdichter laufen.
Der Job ist in der Klasse MoVerdichtungDefaultValuesSetter wie folgt implementiert:
Abbildung 55: Job-Ablauf (schematisch): MoVerdichtungDefaultValueSetter
Trigger: Der MoVerdichtungDefaultValuesSetter wird 1x täglich vom Bookkeeper
gestartet.
Ablauf
1. Aktuellen Tag aus dem Start-Timebucket des Jobs ermitteln – für diesen Tag sollen die
Default-Werte gesetzt werden.
2. Anhand von erh_gdbs_betriebsdatenhistorie werden alle gültigen
Tarifabschnittslisten-Versionen (ta_version) für den aktuellen Tag ermittelt.
3. Alle Mautobjekte der ermittelten TAL-Versionen anhand der Tabelle erh_gdbs_mautobjekt
ermitteln und mit ihrem Änderungsstatus aus der Tabelle erh_mo_aenderungsstatus
anreichern (Defaultwert = unveraendert).
4. Für alle ermittelten Mautobjekte Default-Einträge für den aktuellen Tag in die Tabellen
erh_moba_mo_verdichtung_tag und erh_moba_mo_verdichtung_stunde schreiben.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 144 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
 bd_aenderungsstatus: <siehe Schritt 3>
 anz_ausfahrten: 0
 anz_einfahrten: 0
 anz_durchfahrten: 0
 anz_erhebungen_lueckenschluss: 0
 anz_events: 0
 anz_events_erhebungsluecken: 0
 moba_quote: 100
 erhebungsquote: 100
 fahrleistung: 0
Allgemeine Hinweise
Der MobaVerdichter (siehe Kapitel 5.2.5.1.10 [DSJ-QSErh-0070] MobaVerdichtungJob) verdichtet
Befahrungsparameter, die bis zu fünf Tage alt sind. Bei Jobstart auf einer leeren Datenbank kann es
für die vier vorangegangenen Tage keine Default-Werte und keinen BD-Änderungsstatus geben, da
an diesen Tagen der MoVerdichtungDefaultValuesSetter -Job nicht ausgeführt würde. Aus
diesem Grund werden beim blanken Deployment automatisch per Skript entsprechende Defaultwerte
gesetzt.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 145 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.12 [DSJ-QSErh-0090] GenericMetrikVerdichtung
Paket: de.tollcollect.zme.qs.erh.rule.edm
Der Tarifierer (EDM) liefert zyklisch alle 15 Minuten die Anzahl der Tarifierungen metrisch je
Betriebsdaten-Release und Betriebsdateninstanz an QS-Erh. Diese Eingangswerte werden auf
Verarbeitungstag und -stunde hin durch den Job GenericMetrikVerdichtung verdichtet.
Abbildung 56: Job-Ablauf (schematisch): GenericMetrikVerdichtung
Trigger: Der Job wird vom Bookkeeper mit zwei Verdichtungs-Granularitäten gestartet: Die
Stundenverdichtung wird stündlich, die Tagesverdichtung 1x täglich.
Ablauf
1. Einlesen der Timebuckets der Metrik-Events aus lds_edm_en_q_metriken.
2. Verdichtung nach Stunden respektive Tag pro BD-Release, BD-Instanz und Event-ID.
Hinweis: Bei der Verdichtung werden ausschließlich Datensätze mit gültigen BD-Releases
und -Instanzen berücksichtigt. Die von EDM übermittelte Aggregation „restliche“ wird ignoriert.
3. Speicherung der Verdichtung in erh_edm_metriken_verdichtung_1h bzw.
erh_edm_metriken_verdichtung_24h.
Konfiguration
Die Aggregations-Granularität des Jobs GenericMetricVerdichtung wird vollständig über die
Bookkeeper gesteuert (siehe auch Abschnitt Fehlerbehandlung). Eine variable Konfiguration (ohne
Deployment) ist nicht vorhanden.
Fehlerbehandlung
Spark-DataFrame bzw. SQL-Fehler führen zu einem Abbruch des gesamten Jobs.
Bei der Job-Konfiguration muss beachtet werden, dass prinzipiell mehrere Timebucket-Schemata als
Vorbedingungen definiert werden können, sie sich jedoch immer auch auf genau eine Quell-Tabelle
beziehen müssen. Gleichzeitig muss genau ein Timebucket-Schema als Nachbedingung konfiguriert
werden, das auch die Ziel-Tabelle für die konfigurierte Verdichtung festlegt. Der Job scheitert, wenn
diese Randbedingungen nicht erfüllt werden.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 146 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.13 [DSJ-QSErh-0100] ProcessRSEIdKorrektur
Paket: de.tollcollect.zme.qs.erh.rule.onq
Der Job ProcessRSEIdKorrektur dient dazu, die vom Erkennungsservice (ES) gelieferten RsEKontakte vorzuselektieren, um sicherzustellen, dass ausschließlich RsE-Kontakte der Kontrollart
KonAu (automatische Kontrolle) für die ONQ/DSRC-Berechnung verwendet werden.
Die Selektion wird anhand der RsE-ID vorgenommen. Sie wird als Dezimalwert übermittelt und setzt
sich zusammen aus der 13Bit Manufacturer-ID und der 27Bit Individual-ID. Die Manufacturer-ID gibt
Auskunft über den Hersteller, die Individual-ID bezieht sich auf die konkrete RsE. Für KonAu wird eine
Individual-ID im Nummernkreis von 4000 bis 4999 (durch den Fachbereich festgelegt) erwartet. Die
aktuell übermittelte RsE-ID ist länger. Sie soll auf Seiten KonAu geändert werden. Um den Zeitraum
der Umstellung zu überbrücken, wurde ein Workaround umgesetzt, der die gelieferten RsE-IDs auf die
von QS-AV erwarteten Werte mappt.
Abbildung 57: Job-Ablauf (schematisch): ProcessRSEIdKorrektur
Trigger: Der ProcessRSEIdKorrektur-Job wird alle 15 Minuten vom Bookkeeper getriggert.
Ablauf
1. Prüfe anhand des Schwellwerts erh.onq.schwellwert_use_onq_workaround, ob der
Workaround des RsE-ID-Mappings für die Verarbeitung des/der aktuellen Timebuckets
verwendet werden soll.
a. Falls nein, führe die Selektierung der RsE-IDs durch:
i. Überprüfe ob RsE-Kontakte des Herstellers erlaubt sind und weiterverarbeitet
werden sollen. Dazu wird geprüft, ob die Manufacturer-ID den Wert „10101“
(binär) bzw. „21“ (dezimal) aufweist. Sofern die ID nicht übereinstimmt, wird
der RsE-Kontakt nicht weiter verarbeitet.
ii. Prüfe, ob die 4-stellige Zahl für die Kontrollart der Individual-ID innerhalb der
konfigurierten Schwellwertgrenzen (erh.onq.nummernkreis_oben und 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 147 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
erh.onq.nummernkreis_unten) liegt. Andernfalls wird der RsE-Kontakt
nicht weiter verarbeitet.
b. Falls ja, führe die Selektierung anhand der mitgelieferten RsE-ID und der MappingTabelle durch:
i. Überprüfe, ob RsE-Kontakte des Herstellers erlaubt sind und
weiterverarbeitet werden sollen. Dazu wird geprüft, ob die Manufacturer-ID
den Wert „10101“ (binär) bzw. „21“ (dezimal) aufweist. Sofern die ID nicht
übereinstimmt, wird der RsE-Kontakt nicht weiter verarbeitet.
ii. Ermittle die korrekte RsE-ID anhand der Mapping-Tabelle
erh_onq_rse_mapping_table und prüfe, ob die 4-stellige Zahl für die
Kontrollart innerhalb der konfigurierten Schwellwertgrenzen
(erh.onq.nummernkreis_oben und erh.onq.nummernkreis_unten)
liegt. Andernfalls wird der RsE-Kontakt nicht weiter verarbeitet.
2. Persistiere zutreffende RsE-Kontakte aus 1.a bzw. 1.b in der Tabelle
erh_onq_rse_kontakt_id_korrigiert.
Allgemeine Hinweise
Die Tabelle erh_onq_rse_kontakt_id_korrigiert ist analog zur Tabelle
lds_es_en_q_rse_kontakte aufgebaut und wird auf Grund des Workarounds verwendet, um die
RsE-Kontakte zu speichern. Bei RsE-Kontakten, bei denen ein Mapping der RsE-ID erforderlich war,
wird die aus dem Mapping ermittelte korrigierte RsE-ID gespeichert.
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.onq.konau_nummernkreis_oben 5000 Schwellwert für den Nummernkreis der RsE-ID
(oben), zur Kennzeichnung der Kontrollart
KonAu.
erh.onq.konau_nummernkreis_unten 4000 Schwellwert für den Nummernkreis der RsE-ID
(unten), zur Kennzeichnung der Kontrollart
KonAu.
erh.onq.konsl_nummernkreis_oben 8000 Schwellwert für den Nummernkreis der RsE-ID
(oben), zur Kennzeichnung der Kontrollart
KonSL.
erh.onq.konsl_nummernkreis_unten 6000 Schwellwert für den Nummernkreis der RsE-ID
(unten), zur Kennzeichnung der Kontrollart
KonSL.
erh.onq.use_onq_workaround true Gibt an ob der Workaround des RsE-IDMappings für die Verarbeitung verwendet
werden soll.
true= Workaround soll verwendet werden,
false= Workaround soll nicht verwendet
werden.
Tabelle 77: ProcessRSEIdKorrektur - Konfiguration in Tabelle tech_rule_schwellwert
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 148 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.14 [DSJ-QSErh-0110] EobaEventCreator
Paket: de.tollcollect.zme.qs.erh.rule
Der EobaEventCreator erzeugt EOBA-Events auf Basis des Erkennungsmodells und der
Befahrungsparameter.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 58: Job-Ablauf (schematisch): EobaEventCreator
Trigger: Der EobaEventCreator-Job wird alle 5 Minuten vom Bookkeeper getriggert.
Ablauf
1. Laden der Befahrungsparameter für das aktuelle Timebucket aus
lds_es_en_q_befahrungsparameter_eo.
2. Befahrungsparameter und Betriebsdaten (Erkennungsmodell) werden mit Hilfe der
Betriebsdatenhistorie (erh_gdbs_betriebsdatenhistorie) über eine übereinstimmende
Version und ID miteinander verknüpft. Hierbei werden Datensätze verworfen, deren
Ausfahrtszeitpunkt mehr als erh_eoba_betrachtungszeitraum_tage (siehe
Konfiguration) zurückliegt.
3. Ausführung aller statischen EOBA-Regeln basierend auf den gefilterten Eingangsdaten:
a. Regel 510: Lücke bei Teilbefahrung
b. Regel 511: Lücke mit überlanger Distanz
c. Regel 512: Lücke mit Transexzentrizität
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 149 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
d. Regel 522: Vergebührung mit knapper EO-Befahrung
e. Regel 530: Winkel der Befahrung nicht gemäß Erkennungsmodell
f. Regel 540: EO-Doppelbefahrung
g. Regel 571: Lücke bei fehlendem EO mit Vergebührungskonsequenz
h. Regel 582: Sichere Ein-Abschnitts-Lücke (Lückenschlussvorschlag)
Als Ergebnis bleiben alle Befahrungsparameter-Erkennungsobjekt-Kombinationen übrig,
bei denen mindestens eine der Regeln gegriffen hat = potentielle EOBA-Events.
Hinweis: Wenn diese Menge leer ist, sind die nachfolgendenSchritte obsolet.
4. Filterung der potentiellen EOBA-Events mit Hilfe der Ausnahmeliste (erh_eo_ausnahme).
Das Ergebnis spaltet die potenziellen EOBA-Events in folgende Mengen:
a. Als solche zu speichernde EOBA-Events (keine Ausnahme greift)
b. Ausgenommene EOBA-Events (mindestens eine Ausnahme greift)
5. Anreicherung der zu speichernden EOBA-Events (4.a) mit Daten aus den FzG-Stammdaten
und der Regelliste, Persistierung in erh_eoba_event.
6. Aggregation (Zählung) der ausgenommenen EOBA-Events (4.b) und Verwendung der Events
zum Update der Ausnahmestatistik in der Tabelle erh_eoba_ausnahme_statistik.
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.eoba.betrachtungszeitraum_tage 5 Gibt die Anzahl der Tage an, die die
Befahrungszeit des aktuellen Mautobjekts des
Befahrungsparameter-Datensatzes in der
Vergangenheit liegen darf, um noch im
Betrachtungszeitraum zu liegen.
Tabelle 78: EobaEventCreator - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 150 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.15 [DSJ-QSErh-0120] EobaModellBewerter
Paket: de.tollcollect.zme.qs.erh.rule.eoba
In der EOBA-Modellbewertung werden Anomalien in der Erhebung zu einzelnen Erkennungsobjekten
mit Auswirkung auf die zu erreichende Erhebungsqualität identifiziert. Dabei werden die Ergebnisse
aus der vorangegangenen EOBA-Verdichtung anhand eines Regelwerkes automatisch bewertet und
Modellbewertungsergebnisse und Modellauffälligkeiten gespeichert. Hinweis: Die Übermittlung der
Auffälligkeiten an GDBS erfolgt durch die Jobs [DSJ-QSErh-0011] GdbsProductsCreator und [DSJQSErh-0012] GdbsUploadPump.
Abbildung 59: Job-Ablauf (schematisch): EobaModellBewerter
Trigger: Der EobaModellBewerter-Job wird 1x m Tag vom Bookkeeper getriggert.
Ablauf
1. Laden der Daten aus den Eingangstabellen erh_eoba_eo_verdichtung_tag,
erh_eoba_regel_verdichtung_eo_tag und erh_eoba_event.
2. Filtern der Tabellen nach Betrachtungstag (Timebucket) und BD-Instanz „Produktion“.
3. Anwenden der statischen EOBA-Modellbewertungsregeln:
a. Regel 2000: Bewertung Modellauffälligkeit Teilbefahrung EO
b. Regel 2001: Bewertung Modellauffälligkeit Erhebungsquote EO
c. Regel 2002: Bewertung Modellauffälligkeit Fehlvergebührung EO
d. Regel 2003: Bewertung Modellauffälligkeit Befahrungsänderung EO
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 151 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Hinweis: Die Regeln 2000 und 2001 benötigen zur Einschätzung der Priorität einer
Auffälligkeit die Fahrleistungsklasse. Diese wird anhand der mo_id_eo aus dem
erh_eoba_event durch Bestimmung der Fahrleistung und Fahrleistungsklasse mit Hilfe
der Tabellen erh_moba_mo_verdichtung_tag (Fahrleistung für den aktuellen Tag)
und erh_mo_fahrleistungsklassen (Zuordnung der Fahrleistung zu einer
Fahrleistungsklasse) ermittelt.
4. Zusammenführen der Modellbewertungsergebnisse und Modellauffälligkeiten der einzelnen
Regeln.
5. Persistieren der Modellbewertungsergebnisse in
erh_eoba_modellbewertungs_ergebnisse und der Auffälligkeiten in
erh_eoba_modellbewertungs_incidents.
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.modellbewertung.grenzwert_anzahl_
teilbefahrung
20 Grenzwert für die Mindestanzahl an Events pro
EO, die als Teilbefahrungs-Event klassifiziert
wurden, damit eine Auffälligkeit für das EO
generiert wird.
erh.modellbewertung.grenzwert_anzahl_
erhebungsquote
20 Grenzwert für die Mindestanzahl an Events pro
EO, die als Erhebungsquoten-Event klassifiziert
wurden, damit eine Auffälligkeit für das EO
generiert wird.
erh.modellbewertung.grenzwert_anzahl_
fehlvergebuehrung
20 Grenzwert für die Mindestanzahl an Events pro
EO, die als Fehlvergebührungs-Event
klassifiziert wurden, damit eine Auffälligkeit für
das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
befahrungsaenderung
20 Grenzwert für die Mindestanzahl an Events pro
EO, die als Befahrungsänderungs-Event
klassifiziert wurden, damit eine Auffälligkeit für
das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_stunden 5 Grenzwert für die Mindestanzahl
unterschiedlicher Nutzungsstunden aller zu
einer EO-ID zugehörigen EOBA-Events an
diesem Tag, damit eine Auffälligkeit generiert
wird.
erh.modellbewertung.grenzwert_anzahl_benutzer 5 Grenzwert für die Mindestanzahl
unterschiedlicher Benutzer aller zu einer EO-ID
zugehörigen EOBA-Events an diesem Tag,
damit eine Auffälligkeit generiert wird.
erh.modellbewertung.grenzwert_anzahl_events 30 Grenzwert für die Mindestanzahl an Events
gegen das Erkennungsobjekt in der täglichen
Erkennungsobjektverdichtung, damit eine
Auffälligkeit für das EO generiert wird.
erh.modellbewertung.grenzwert_eoba_quote 98 Schwellwert für die EOBA-Quote, unterhalb
derer die Verdichtungsergebnisse überhaupt
weiter untersucht werden.
Tabelle 79: EobaModellBewerter - Konfiguration in Tabelle tech_rule_schwellwert
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 152 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Fehlerbehandlung
 Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
 Bei anderen auftretenden Fehlern wird kein Ergebnis produziert.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 153 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.16 [DSJ-QSErh-0130] EoVerdichtungDefaultValuesSetter
Paket: de.tollcollect.zme.qs.erh.rule
Der Job EoVerdichtungDefaultValueSetter hat die Aufgabe, in der Tabelle
erh_eoba_eo_verdichtung_tag für den aktuellen Tag Defaultwerte für alle Erkennungsobjekte zu
setzen. Er wird daher zu Beginn jedes Tages (0 Uhr) aufgerufen. Die Defaultwerte werden vom Job
EobaVerdichter überschrieben, sofern die Erkennungsobjekte verarbeitet werden.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 60: Job-Ablauf (schematisch): EoVerdichtungDefaultValuesSetter
Trigger: Der EoVerdichtungDefaultValuesSetter-Job wird 1x täglich vom Bookkeeper getriggert.
Ablauf
1. Aktuellen Tag aus dem Start-Timebucket des Jobs ermitteln. Für diesen werden die
Defaultwerte gesetzt.
2. Anhand der Betriebsdatenhistorie aus der Tabelle erh_gdbs_betriebsdatenhistorie
alle für den aktuellen Tag gültigen Erkennungsmodell-Versionen (em_version) ermitteln.
3. Laden aller Erkennungsobjekte aus erh_gdbs_erkennungsobjekte, die mit den
ermittelten aktuell gültigen EM-Versionen übereinstimmen.
4. Für alle ermittelten Erkennungsobjekte die Defaultwerte für den aktuellen Tag in die Tabelle
erh_eoba_eo_verdichtung_tag schreiben.
 anz_befahrungen: 0
 anz_events: 0
 anz_events_erhebungsluecken: 0
 anz_eoba_quote: 0 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 154 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.17 [DSJ-QSErh-0140] EobaVerdichtungJob
Paket: de.tollcollect.zme.qs.erh.rule
Der Job EobaVerdichtungJob führt in unterschiedlichen Granularitäten die Verdichtung der EOBAEreignisse basierend auf den Befahrungsparametern und den EOBA-Events durch. Die
Befahrungsparameter werden dabei jeweils nach Ausfahrtszeitpunkt (EO) gefiltert. Ziel der
Verdichtung ist es, eine Bewertung des Optimierungspotenzials des Erkennungsmodells vornehmen
zu können.
Die FzG-bezogenen EOBA-Verdichtungen werden ausschließlich für Befahrungsparameter der BDInstanz „Produktion“ angefertigt.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 61: Job-Ablauf (schematisch): EobaVerdichtungJob
Trigger: Der EobaVerdichtungJob wird 1x pro Stunde vom Bookkeeper getriggert.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 155 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Ablauf
1. Laden der Daten der abgeleiteten 12 Timebuckets von
lds_es_en_q_befahrungsparameter_eo (Befahrungsparameter) und erh_eoba_event
(EOBA-Events erstellt durch EobaEventCreator).
2. Filterung der Befahrungsparameter nach Ausfahrzeitpunkt. Der Ausfahrzeitpunkt (EO) darf
nicht mehr Tage, als der Schwellwert erh.eoba.betrachtungszeitraum_tage vorgibt,
zurück liegen.
3. Durchführung der FzG-bezogenen Verdichtungen mit Befahrungsparametern gefiltert nach
BD-Instanz „Produktion“ und Speicherung der Ergebnisse in den entsprechenden Zieltabellen.
Nach folgenden Merkmalen wird gruppiert:
a. FzG und Tag
b. Fzg, Tag und Regel
c. FzG und EO
d. FzG, EO und Regel
4. Durchführung der Regelverdichtung für Erkennungsobjekte pro Stunde und pro Tag mit den
gefilterten Befahrungsparametern und den EOBA-Events und Speicherung der Ergebnisse in
den entsprechenden Zieltabellen.
5. Durchführung der Erkennungsobjektverdichtung pro Stunde und pro Tag mit den gefilterten
Befahrungsparametern und den EOBA-Events und Speicherung der Ergebnisse in den
entsprechenden Zieltabellen.
Hinweis: Bei den Tabellen *_hist handelt es sich um Hilfstabellen zur Aktualisierung der
Verdichtungen (Idempotenz). Auch die täglichen Verdichtungen werden stündlich aktualisiert.
Allgemeine Hinweise
 Defaultwerte je Erkennungsobjekt werden vom Job EoVerdichtungDefaultValuesSetter zu
Beginn eines Tages geschrieben und vom EobaVerdichtungJob überschrieben.
Der EobaVerdichtungJob verdichtet Befahrungsparameter die bis zu x Tage alt sind. Bei
Jobstart auf einer leeren Datenbank kann es für die vier vorangegangenen Tage keine
Defaultwerte und keinen BD-Änderungsstatus geben, da an diesen Tagen der
EoVerdichtungDefaultValuesSetter -Job nicht ausgeführt wurde.
 Da die Verdichtungen der MOBA- und der EOBA-Events ähnliche Abläufe und Strukturen
aufweisen, wurde durch die Klasse ModellVerdichter eine gemeinsame Basisklasse
implementiert, welche die jeweilige Verdichtung (EOBA/MOBA) steuert.
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.eoba.betrachtungszeitraum_tage 5 Gibt die Anzahl der Tage an, die die
Befahrungszeit des aktuellen
Erkennungsobjekts des BefahrungsparameterDatensatzes in der Vergangenheit liegen darf,
um noch im Betrachtungszeitraum zu liegen (es
gilt der Ausfahrtszeitpunkt vom EO).
Tabelle 80: EobaVerdichtungJob - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 156 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.18 [DSJ-QSErh-0150] ModellVerdichterWeekly
Paket: de.tollcollect.zme.qs.erh.rule
Der Job ModellVerdichterWeekly führt auf Basis der stündlichen Modell-Verdichtung (Ergebnisse der
Jobs EobaVerdichtungJob und MobaVerdichtungJob) eine wöchentliche Modellverdichtung durch.
Die in den MOBA- und EOBA-Verdichtungen integrierten FzG-Verdichtungen, werden ausschließlich
für BD-Instanz „Produktion“ angewendet.
Abbildung 62: Job-Ablauf (schematisch): ModellVerdichterWeekly
Trigger: Der ModellVerdichterWeekly-Job wird 1x pro Woche vom Bookkeeper getriggert.
Ablauf
Für alle vier Verdichtungen (MO, MO/Regel, EO und EO/Regel) ist der Ablauf jeweils
folgendermaßen:
1. Laden der Daten aus der Input-Tabelle für die Vorwoche. Für MOBA-Verdichtungen
zusätzliche Filterung nach BD-Instanz „Produktion“. Die EOBA-Verdichtungen liegen bereits
nur für die produktive BD-Instanz vor.
2. Verdichtung der geladenen Daten und Speicherung in der entsprechenden wöchentlichen
Verdichtungstabelle (Output-Tabelle).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 157 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.19 [DSJ-QSErh-0160] BetriebsdatenValidierung
Paket: de.tollcollect.zme.qs.erh.rule.bdv
Der Job vergleicht die Betriebsdaten-Instanzen „Produktion“ und „Validierung“ anhand der MOBA- und
EOBA-Verdichtungen sowie der EDM-Metrik-Events.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 63: Job-Ablauf (schematisch): BetriebsdatenValidierung
Trigger: Der BetriebsdatenValidierung-Job wird stündlich vom Bookkeeper getriggert.
Ablauf
1. Ermittlung des Referenzzeitpunkts anhand des aktuellen Timebuckets (Timebucket plus 1h),
sowie der zu diesem Referenzzeitpunkt gültigen Betriebsdaten-Version für die BetriebsdatenInstanzen „Validierung“ und „Produktion“ aus der Tabelle
erh_gdbs_betriebsdatenhistorie.
2. Regelbasierte Abgleich der BD-Instanzen hinsichtlich der EOBA- sowie MOBA-Verdichtungen
unter Verwendung der konfigurierten Schwellwerte (siehe Konfiguration):
a. Selektion der Verdichtungen je EO (erh_eoba_eo_verdichtung_stunde) bzw.
MO (erh_moba_mo_verdichtung_stunde) innerhalb des Referenzzeitraums,
bestimmt durch erh.bdval.betrachtungszeitraum_min relativ zum
Referenzzeitpunkt für „Produktion“ und „Validierung“.
b. EOs bzw. MOs mit Hilfe der Release-Notes (aus der Tabelle
erh_gdbs_release_notes) unterteilen nach "neu", "geändert" und "nicht
geändert".
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 158 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
c. Zählen der auffälligen EOs und MOs aus den Verdichtungstabellen. Regelbasierter
Abgleich der Anzahlen mit Hilfe der konfigurierten Schwellwerte. Bei
Schwellwertüberschreitung, werden entsprechende Auffälligkeiten gezählt und
vermerkt.
d. Speicherung der Auffälligkeiten und der aufsummierten Anzahlen, für „neu“,
„geändert“ und „nicht geändert“ in der Tabelle erh_bdv_bewertungen.
3. Regelbasierte Abgleich der BD-Instanzen hinsichtlich der EDM-Metrik-Events unter
Verwendung der konfigurierten Schwellwerte (siehe Konfiguration):
a. Ermittlung der EDM-Metrik-Events für die Betriebsdaten-Instanzen „Produktion“ und
„Validierung“ im Referenzzeitraum (definiert durch Schwellwert
erh.bdval.edm_betrachtungszeitraum_min)
b. Summierung der Events und Bewertung der summierten Events anhand der
Schwellwerte. Wenn sich die summierten Events (jeder Event-Typ enthält eine
Summe) um den Schwellwert in Prozent unterscheiden, werden diese als „auffällig“
eingestuft.
c. Speicherung der aufsummierten Ergebnisse für die BD-Instanzen „Produktion“ und
„Validierung“, sowie ggf. der Auffälligkeiten in der Tabelle erh_bdv_bewertungen.
Allgemeine Hinweise
 Um die aktuell gültige Produktions- und Validierungsversion für den Referenzzeitpunkt zu
bestimmen, müssen für die BD-Instanzen „Produktion“ und „Validierung“ mindestens die
Release-Historie und die Release-Notes der Betriebsdaten vorliegen.
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.bdval.ng_befahrungen_grenzwert 10000 Grenzwert Anzahl auffälliger, nicht geänderter
Abschnitte in Bezug auf Befahrungen
erh.bdval.ng_mobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger, nicht
geänderter Abschnitte in Bezug auf MOBAQuote
erh.bdval.ng_erhquote_grenzwert 10000 Grenzwert Anzahl auffälliger, nicht geänderter
Abschnitte in Bezug auf Erhebungsquote
erh.bdval.ng_mobaquote_toleranz 5 Toleranz Abweichung MOBA-Quote (nicht
geänderter Abschnitte)
erh.bdval.ng_erhquote_toleranz 5 Toleranz Abweichung EOBA-Quote (nicht
geänderter Abschnitte)
erh.bdval.ng_befahrungen_toleranz 5 Toleranz Abweichung Befahrungen (nicht
geänderter Abschnitte)
erh.bdval.g_befahrungen_grenzwert 10000 Grenzwert Anzahl auffälliger, geänderter
Abschnitte in Bezug auf Befahrungen
erh.bdval.g_mobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger,
geänderter Abschnitte in Bezug auf MOBAQuote
erh.bdval.g_erhquote_grenzwert 10000 Grenzwert Anzahl auffälliger, geänderter
Abschnitte in Bezug auf Erhebungsquote
erh.bdval.g_mobaquote_toleranz 5 Toleranz Abweichung MOBA-Quote geänderter 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 159 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key Default Beschreibung
MO
erh.bdval.g_erhquote_toleranz 5 Toleranz Abweichung Erhebungsquote
geänderter MO
erh.bdval.g_befahrungen_toleranz 5 Toleranz Abweichung Befahrungen veränderter
MO
erh.bdval.n_mobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger, neuer Abschnitte
in Bezug auf MOBA-Quote
erh.bdval.n_erhquote_grenzwert 10000 Grenzwert Anzahl auffälliger, neuer Abschnitte
in Bezug auf Erhebungsquote
erh.bdval.n_erhquote_toleranz 95 Toleranz Abweichung Erhebungsquote neue
MO
erh.bdval.n_mobaquote_toleranz 95 Toleranz Abweichung MOBA-Quote neue MO
erh.bdval.ng_eobaquote_toleranz 5 Toleranz Abweichung EOBA-Quote nicht
geänderter EO
erh.bdval.ng_eobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger nicht geänderter
EO in Bezug auf EOBA-Quote
erh.bdval.g_eobaquote_toleranz 5 Toleranz Abweichung EOBA-Quote geänderter
EO
erh.bdval.g_eobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger geänderter EO in
Bezug auf EOBA-Quote
erh.bdval.n_eobaquote_toleranz 95 Toleranz Abweichung EOBA-Quote neuer EO
erh.bdval.n_eobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger neuer EO in Bezug
auf EOBA-Quote
erh.bdval.tq_sum_maut_ok_toleranz 1 Schwellwerte für Tarfierungsqualität
erh.bdval.tq_sum_ext_kosten_toleranz 1 Schwellwerte für Tarfierungsqualität
erh.bdval.tq_anz_tarif_ok_toleranz 1 Schwellwerte für Tarfierungsqualität
erh.bdval.tq_anz_tarif_nok_toleranz 1 Schwellwerte für Tarfierungsqualität
Tabelle 81: BetriebsdatenValidierung - Konfiguration in Tabelle tech_rule_schwellwert
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
erh.bdval.betrachtungszeitraum_min 120 Betrachtungszeitraum für Stundenverdichtung in
Minuten
erh.bdval.edm_betrachtungszeitraum_min 120 Betrachtungszeitraum für EDM-Events in
Minuten
Tabelle 82: BetriebsdatenValidierung – Konfiguration in Tabelle tech_config
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 160 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Fehlerbehandlung
 Sind die Betriebsdaten inkonsistent (z.B. liegt zum aktuellen Zeitpunkt keine gültige
Produktions-/Validierungsversion vor), wird eine Exception
(InconsistentBetriebsdatenException) geworfen und der Job abgebrochen.
 Fehlende oder fehlerhafte Angaben (z.B. negative Zeiten) von Konfigurations- oder
Schwellwerten führen zu Exceptions (ConfigurationException bzw.
SchwellwertException) und zum Abbruch des Jobs.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 161 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.20 [DSJ-QSErh-0170] EobaKreisClusterBuilder
Paket: de.tollcollect.zme.qs.erh.rule.eoba
Der Job EobaKreisClusterBuilder erzeugt mittels dichtebasiertem Clusterverfahren (OPTICSAlgorithmus) Cluster vom Typ „Kreis“, die zur Analyse von Befahrungen kreisförmiger EOs eingesetzt
werden Das dichtebasierte Clusterverfahren bildet Cluster als Häufung von Datenpunkten im
d-dimensionalem Raum ab.
Hintergrund des Clustering: Für jedes Erkennungsobjekt können mehrere mögliche Befahrungen
existieren. Mittels statistischer Auswertung ist es möglich, die Befahrungen eines EOs- oder MOs
anhand der Befahrungsparameter (z. B. Distanzen, Richtungsänderungen) in Erwartungen aus
Befahrungsparametercluster zu gruppieren. Diese bilden für den Lückenschluss und die Auswertung
der Befahrungen von EOs zulässige Befahrungen ab. Zudem ist es möglich neue oder veränderte
Befahrungsmöglichkeiten zu registrieren.
Für die Kreis-Clusterbildung werden zu jedem vorhandenen EO-ID-Paar, bestehend aus „aktueller
EO-ID“ und „letzter EO-ID“, die Befahrungsparameter aus dem Befahrungsparameter-Archiv (kurz
BfpA) gesammelt, gezählt und dem Clustering unterzogen. Dabei werden folgende Informationen
(Dimensionen) betrachtet:
 Einfahrtswinkel
 Ausfahrtswinkel
 Distanz im EO
 Distanz zum letzten EO
 Richtungsänderung im EO
 Richtungsänderung zum letzten EO
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 64: Job-Ablauf (schematisch): EobaKreisClusterBuilder
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 162 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Trigger: Der EobaKreisClusterBuilder-Job wird 1x täglich vom Bookkeeper getriggert.
Ablauf
1. Einlesen der aktuellsten Cluster-Konfiguration für den Cluster-Typ „Kreis“ aus der Tabelle
erh_eoba_clusterconfig, die gültig ist.
2. Laden der BfpA für die entsprechenden Timebuckets aus der Tabelle
erh_befahrungsparameter_archiv_eo, gruppiert nach EO-ID-Paaren, BD-Instanz und
BD-Version. Je Gruppierung wird die Anzahl der BfpA gezählt und geprüft, ob die Anzahl
innerhalb der Grenzwerte anzahl_befahrungen_mo_min und
anzahl_befahrungen_mo_max der Cluster-Konfiguration liegt.
3. Sofern die Grenzwerte nicht über- bzw. unterschritten wurden, werden die Daten in der
Tabelle erh_eoba_anzahl_eo_bef persistiert. Ist die Anzahl kleiner wird an dieser Stelle
abgebrochen. Ist die Anzahl größer, werden zufällige BfpA entsprechend dem Max-Grenzwert
ermittelt (ohne Doppelungen) und ebenfalls persistiert.
4. Anwendung des generischen OPTICS-Algorithmus (gemäß der Cluster-Konfiguration, z.B.
zeitintervall, min_pts) auf die 6-dimensionalen Daten „Aus-/Einfahrtswinkel“, „Distanz
im/letztes EO“, „Richtungsänderung im/letztes EO“ je Gruppierung. Die Anzahl der zu
generierenden Cluster ist über anzahl_cluster_max konfigurierbar.
5. Speicherung der Min- und Maxwerte für jede der 6 Dimensionen je gefundenem Cluster in der
Tabelle erh_eoba_kreiscluster.
Konfiguration
Die benötigten Konfigurations- und Schwellwerte werden zusammen in der Cassandra-Tabelle
erh_eoba_clusterconfig konfiguriert:
Key Default Beschreibung
config_id 2 Eindeutige ID der Konfiguration (Ganzzahl)
cluster_typ 2 Clustertyp: 0 = Lückenschluss, 1 = Tor, 2 =
Kreis
anzahl_befahrungen_mo_min 50 Minimale Anzahl an Befahrungsparameter aus
dem Archiv je EO-ID, die für die Durchführung
des Clustering erforderlich ist.
anzahl_befahrungen_mo_max 10000 Maximale Anzahl an Befahrungsparameter
aus dem Archiv je EO-ID. Existieren mehr
BfpA als durch diesen Key konfiguriert,
werden genauso viele BfpA zufällig
ausgewählt.
zeitintervall 1 Anzahl an vergangenen Tagen, für die die
Befahrungsparamater aus dem
Befahrungsparameter-Archiv in die Analyse
einbezogen werden sollen.
min_pts 5 OPTICS-spezifischer Parameter:
Mindestanzahl von Punkten in einer
Nachbarschaft vom Radius Epsilon, ab der
eine Gruppe von Punkten als Cluster
betrachtet werden kann.
epsilon 1.0 OPTICS-spezifischer Parameter: Radius in
dem nach Punkten gesucht wird, damit diese
als Nachbarn gelten.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 163 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key Default Beschreibung
epsilon_cl 1.0 OPTICS-spezifischer Parameter:
Erforderlicher Mindestabstand zwischen den
Zentren von Clustern, damit diese noch als
eigenständige Cluster betrachtet werden.
anzahl_cluster_max 1 Maximal erlaubte Anzahl von Clustern.
Werden mehr Cluster erzeugt, werden die
Cluster nicht weiter verarbeitet.
gueltig_von 2006-12-31T23:59:59Z Startzeitpunkt des Gültigkeitszeitraums
(Timestamp) der Konfiguration.
gueltig_bis 2130-12-31T23:59:59Z Endzeitpunkt des Gültigkeitszeitraums
(Timestamp) der Konfiguration.
Tabelle 83: EobaKreisClusterBuilder – Konfiguration in Tabelle erh_eoba_clusterconfig
Fehlerbehandlung
Sollte keine Konfiguration für den Cluster-Typ „Kreis“ vorliegen, die entsprechend des
Gültigkeitszeitraums verwendet werden kann, wird der Job abgebrochen und eine Exception
(ConfigurationException) geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 164 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.21 [DSJ-QSErh-0180] EobaTorClusterBuilder
Paket: de.tollcollect.zme.qs.erh.rule.eoba
Der Job EobaTorClusterBuilder erzeugt mittels dichtebasiertem Clusterverfahren (OPTICSAlgorithmus) Cluster vom Typ „Tor“, die zur Analyse von Befahrungen torförmiger EOs eingesetzt
werden Das dichtebasierte Clusterverfahren bildet Cluster als Häufung von Datenpunkten im ddimensionalem Raum ab.
Hintergrund des Clustering: Siehe Kapitel 5.2.5.1.20 [DSJ-QSErh-0170] EobaKreisClusterBuilder.
Für die Tor-Clusterbildung werden zu jedem vorhandenen EO-ID-Paar, bestehend aus „aktueller EOID“ und „letzter EO-ID“, die Befahrungsparameter aus dem Befahrungsparameter-Archiv (kurz BfpA)
gesammelt, gezählt und dem Clustering unterzogen. Dabei werden folgende Informationen
(Dimensionen) betrachtet:
 Ausfahrtswinkel
 Exzentrizität
 ID letztes EO
 Distanz zum letzten EO
 Richtungsänderung zum letzten EO
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 65: Job-Ablauf (schematisch): EobaTorClusterBuilder
Trigger: Der EobaTorClusterBuilder-Job wird 1x täglich vom Bookkeeper getriggert.
Ablauf
1. Einlesen der aktuellsten Cluster-Konfiguration für den Cluster-Typ „Tor“ aus der Tabelle
erh_eoba_clusterconfig, die gültig ist.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 165 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
2. Laden der BfpA für die entsprechenden Timebuckets aus der Tabelle
erh_befahrungsparameter_archiv_eo, gruppiert nach EO-ID-Paaren, BD-Instanz und
BD-Version. Je Gruppierung wird die Anzahl der BfpA gezählt und geprüft, ob die Anzahl
innerhalb der Grenzwerte anzahl_befahrungen_mo_min und
anzahl_befahrungen_mo_max der Cluster-Konfiguration liegt.
3. Sofern die Grenzwerte nicht über- bzw. unterschritten wurden, werden die Daten in der
Tabelle erh_eoba_anzahl_eo_bef persistiert. Ist die Anzahl kleiner wird an dieser Stelle
abgebrochen. Ist die Anzahl größer, werden zufällige BfpA entsprechend dem Max-Grenzwert
ermittelt (ohne Doppelungen) und ebenfalls persistiert.
4. Anwendung des generischen OPTICS-Algorithmus (gemäß der Cluster-Konfiguration, z.B.
zeitintervall, min_pts) auf die 4 Datendimensionen „Ausfahrtswinkel“, „Exzentrizität“
„Distanz letztes EO“, „Richtungsänderung letztes EO“ je Gruppierung. Die Anzahl der zu
generierenden Cluster ist über anzahl_cluster_max konfigurierbar.
5. Speicherung der Min- und Maxwerte für jede der 4 Dimensionen je gefundenem Cluster in der
Tabelle erh_eoba_torcluster.
Konfiguration
Die benötigten Konfigurations- und Schwellwerte werden zusammen in der Cassandra-Tabelle
erh_eoba_clusterconfig konfiguriert:
Key Default Beschreibung
config_id 1 Eindeutige ID der Konfiguration (Ganzzahl)
cluster_typ 1 Clustertyp: 0 = Lückenschluss, 1 = Tor, 2 =
Kreis
anzahl_befahrungen_mo_min 50 Minimale Anzahl an Befahrungsparameter aus
dem Archiv je EO-ID, die für die Durchführung
des Clustering erforderlich ist.
anzahl_befahrungen_mo_max 10000 Maximale Anzahl an Befahrungsparameter
aus dem Archiv je EO-ID. Existieren mehr
BfpA als durch diesen Key konfiguriert,
werden genauso viele BfpA zufällig
ausgewählt.
zeitintervall 1 Anzahl an vergangenen Tagen, für die die
Befahrungsparamater aus dem
Befahrungsparameter-Archiv in die Analyse
einbezogen werden sollen.
min_pts 5 OPTICS-spezifischer Parameter:
Mindestanzahl von Punkten in einer
Nachbarschaft vom Radius Epsilon, ab der
eine Gruppe von Punkten als Cluster
betrachtet werden kann.
epsilon 1.0 OPTICS-spezifischer Parameter: Radius in
dem nach Punkten gesucht wird, damit diese
als Nachbarn gelten.
epsilon_cl 1.0 OPTICS-spezifischer Parameter:
Erforderlicher Mindestabstand zwischen den
Zentren von Clustern, damit diese noch als
eigenständige Cluster betrachtet werden.
anzahl_cluster_max 1 Maximal erlaubte Anzahl von Clustern.
Werden mehr Cluster erzeugt, werden die
Cluster nicht weiter verarbeitet.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 166 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key Default Beschreibung
gueltig_von 2006-12-31T23:59:59Z Startzeitpunkt des Gültigkeitszeitraums
(Timestamp) der Konfiguration.
gueltig_bis 2130-12-31T23:59:59Z Endzeitpunkt des Gültigkeitszeitraums
(Timestamp) der Konfiguration.
Tabelle 84: EobaTorClusterBuilder – Konfiguration in Tabelle erh_eoba_clusterconfig
Fehlerbehandlung
Sollte keine Konfiguration für den Cluster-Typ „Tor“ vorliegen, die entsprechend des
Gültigkeitszeitraums verwendet werden kann, wird der Job abgebrochen und eine Exception
(ConfigurationException) geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 167 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.22 [DSJ-QSErh-0190] EobaLueckenschlussClusterBuilder
Paket: de.tollcollect.zme.qs.erh.rule.eoba
Der Job EobaLueckenschlussClusterBuilder erzeugt mittels dichtebasiertem Clusterverfahren
(OPTICS-Algorithmus) Cluster vom Typ „Lückenschluss“, die zur Analyse von Befahrungen von EOs
mit Lückenschlussvorschlägen eingesetzt werden Das dichtebasierte Clusterverfahren bildet Cluster
als Häufung von Datenpunkten im d-dimensionalem Raum ab.
Hintergrund des Clustering: Siehe Kapitel 5.2.5.1.20 [DSJ-QSErh-0170] EobaKreisClusterBuilder
Für die Lückenschluss-Clusterbildung werden zu jedem vorhandenen MO-ID-Tripel die
Befahrungsparameter aus dem Befahrungsparameter-Archiv (kurz BfpA) gesammelt, gezählt und
dem Clustering unterzogen. Das MO-ID-Tripel besteht aus:
 Aktueller MO-ID (MO-ID des aktuellen Abschnitts einer 3-Abschnitts-Kombination),
 Letzter MO-ID (MO-ID des letzten Abschnitts (Lücke) einer 3-Abschnitts-Kombination),
 Vorletzter MO-ID (MO-ID des vorletzten Abschnitts einer 3-Abschnitts-Kombination)
Beim Clustering werden folgende Informationen (Dimensionen) betrachtet:
 Distanz zum vorletzten Mautobjekt (Summe aus „Letztes Mautobjekt“ und „Aktuelles
Mautobjekt“)
 Richtungsänderung zum vorletzten Mautobjekt (Summe aus Letztes Mautobjekt und Aktuelles
Mautobjekt)
 Befahrungszeit Differenz vorletztes MO (zwischen aktuellem Mautobjekt und vorletztem
Mautobjekt)
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 66: Job-Ablauf (schematisch): EobaLueckenschlussClusterBuilder
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 168 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Trigger: Der EobaLueckenschlussClusterBuilder-Job wird 1x täglich vom Bookkeeper getriggert.
Ablauf
1. Einlesen der aktuellsten Cluster-Konfiguration für den Cluster-Typ „Lückenschluss“ aus der
Tabelle erh_eoba_clusterconfig, die gemessen am Gültigkeitszeitraum gültig ist.
2. Laden der BfpA für die entsprechenden Timebuckets aus der Tabelle
erh_befahrungsparameter_archiv_mo, gruppiert nach MO-ID-Tripeln, BD-Instanz und
BD-Version. Je Gruppierung wird die Anzahl der BfpA gezählt und geprüft, ob die Anzahl
innerhalb der Grenzwerte anzahl_befahrungen_mo_min und
anzahl_befahrungen_mo_max der Cluster-Konfiguration liegt.
3. Sofern die Grenzwerte nicht über- bzw. unterschritten wurden, werden die Daten in der
Tabelle erh_eoba_anzahl_eo_bef persistiert. Ist die Anzahl kleiner wird an dieser Stelle
abgebrochen. Ist die Anzahl größer, werden zufällige BfpA entsprechend dem Max-Grenzwert
ermittelt (ohne Doppelungen) und ebenfalls persistiert.
4. Anwendung des generischen OPTICS-Algorithmus (gemäß der Cluster-Konfiguration, z.B.
zeitintervall, min_pts) auf die 3-dimensionalen Daten „Distanz vorletztes MO“,
„Richtungsänderung vorletztes MO“ „Distanz letztes EO“, „Befahrungszeit Differenz vorletztes
MO“ je Gruppierung. Die Anzahl der zu generierenden Cluster ist über
anzahl_cluster_max konfigurierbar.
5. Speicherung der Min- und Maxwerte für jede der 3 Dimensionen je gefundenem Cluster in der
Tabelle erh_eoba_lueckenschlusscluster.
Konfiguration
Die benötigten Konfigurations- und Schwellwerte werden zusammen in der Cassandra-Tabelle
erh_eoba_clusterconfig konfiguriert:
Key Default Beschreibung
config_id 0 Eindeutige ID der Konfiguration (Ganzzahl)
cluster_typ 0 Clustertyp: 0 = Lückenschluss, 1 = Tor, 2 =
Kreis
anzahl_befahrungen_mo_min 50 Minimale Anzahl an Befahrungsparameter aus
dem Archiv je EO-ID, die für die Durchführung
des Clustering erforderlich ist.
anzahl_befahrungen_mo_max 10000 Maximale Anzahl an Befahrungsparameter
aus dem Archiv je EO-ID. Existieren mehr
BfpA als durch diesen Key konfiguriert,
werden genauso viele BfpA zufällig
ausgewählt.
zeitintervall 1 Anzahl an vergangenen Tagen, für die die
Befahrungsparamater aus dem
Befahrungsparameter-Archiv in die Analyse
einbezogen werden sollen.
min_pts 5 OPTICS-spezifischer Parameter:
Mindestanzahl von Punkten in einer
Nachbarschaft vom Radius Epsilon, ab der
eine Gruppe von Punkten als Cluster
betrachtet werden kann.
epsilon 1.0 OPTICS-spezifischer Parameter: Radius in
dem nach Punkten gesucht wird, damit diese
als Nachbarn gelten.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 169 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key Default Beschreibung
epsilon_cl 1.0 OPTICS-spezifischer Parameter:
Erforderlicher Mindestabstand zwischen den
Zentren von Clustern, damit diese noch als
eigenständige Cluster betrachtet werden.
anzahl_cluster_max 1 Maximal erlaubte Anzahl von Clustern.
Werden mehr Cluster erzeugt, werden die
Cluster nicht weiter verarbeitet.
gueltig_von 2006-12-31T23:59:59Z Startzeitpunkt des Gültigkeitszeitraums
(Timestamp) der Konfiguration.
gueltig_bis 2130-12-31T23:59:59Z Endzeitpunkt des Gültigkeitszeitraums
(Timestamp) der Konfiguration.
Tabelle 85: EobaLueckenschlussClusterBuilder – Konfiguration in Tabelle erh_eoba_clusterconfig
Fehlerbehandlung
Sollte keine Konfiguration für den Cluster-Typ „Lückenschluss“ vorliegen, die entsprechend des
Gültigkeitszeitraums verwendet werden kann, wird der Job abgebrochen und eine Exception
(ConfigurationException) geworfen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 170 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.23 [DSJ-QSErh-0200] MonpErhEventEmitter
Paket: de.tollcollect.zme.qs.erh.rule.monp
Der Job MonpErhEventEmitter erzeugt erhebungsspezifische Metrik-Events für die MonitoringPlattform (MonP) und reiht diese an die QS-AV-interne globale Versand-Warteschlange ein.
Einen allgemeinen Überblick über die Schnittstelle und die beteiligten Programmteile finden Sie im
Kapitel 6.1.6 SST 819 QS-AV  MonP unter Technische Realisierung der Schnittstelle innerhalb der
QS-AV Applikation.
Informationen zu den zu übertragenen Events entnehmen Sie bitte der Schnittstellenspezifikation im
Referenzdokument [8] [S_819_SST_00] SST 819 SST-Spezifikation QS-AV – MonP.
Die folgende Tabelle listet die Input-Tabellen die für die Eventerstellung durch den
MonpErhEventEmitter herangezogen werden.
Input-Tabelle Events
erh_onq_15min_2 ONQ betreffend (z.B.: OnqGruenGesamt, OnqGruenBab)
erh_onq_dsrc_15min DSRC betreffend (z.B.: DsrcQuoteBS, DsrcQuoteBab)
Tabelle 86: Input-Tabellen der erhebungsspezifischen Metrik-Events
Bei der Einreihung der Events in die Versand-Warteschlange werden folgende Output-Tabellen
beschrieben, um den Versand durch den ScheduledMonpSender (siehe auch Kapitel 6.1.6 SST 819
QS-AV  MonP) zu ermöglichen:
Output-Tabelle Aufgabe
tech_monp_pending_metric_event Warteschlange zu versendender Events.
Tabelle 87: Output-Tabellen der der erhebungsspezifischen Metrik-Events
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 67: Job-Ablauf (schematisch): MonpErhEventEmitter
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 171 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Trigger: Der MonpErhEventEmitter-Job wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
1. Einlesen der Eingangsdaten aus den oben genannten Input-Tabellen, zur Ermittlung des
Event-Inhalts, der für das Monitoring verwendet werden soll (siehe Tabelle 86: Input-Tabellen
der erhebungsspezifischen Metrik-Events).
2. Generierung der Metrik-Events gemäß Schnittstellenspezifikation. Dafür werden die Daten in
ein vorgegebenes JSON-Format eingebettet.
3. Einreihung der erstellten Events in die globale Versand-Warteschlange (Tabelle
tech_monp_pending_metric_event).
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
erh.monp.plattform_prefix Plattform Präfix für den FzG Plattformnamen, der im
Event-Inhalt von einigen MonP-Events
verwendet wird. Beispiel: Der Präfix "Plattform"
wird mit dem Herstellernamen „Bosch1G“
zusammengesetzt zu "Plattform Bosch1G".
Tabelle 88: MonpErhEventEmitter – Konfiguration in Tabelle tech_config
Für allgemeine Konfigurationseinstellungen bezüglich der MonP-Schnittstelle siehe Kapitel 6.1.6 SST
819 QS-AV  MonP unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Wichtige Hinweise
Der Inhalt der Metrik-Events ist abhängig von der Befüllung der Tabelle fzg_hersteller. Die
Spalte SENDEN_AN_MONP bestimmt, ob ein FzG-Typ als Gruppierungskriterium und somit Daten für
diesen FzG-Typ in das Event mit aufgenommen werden oder nicht. Die Schreibweise des Herstellers
muss in der Spalte HERSTELLER_NAME_MONP definiert werden. Ist diese Spalte nicht gesetzt (null),
so wird anstelle dessen eine Zusammensetzung aus HERSTELLER_NAME_ANZEIGE und
HERSTELLER_NAME verwendet. Dem ermittelten Herstellernamen wird in jedem Fall das
plattform_prefix aus der Konfiguration vorangestellt, um auf die im Event verwendete
Schreibweise der Keys zu kommen. Aus "Bosch1G" wird somit "Plattform Bosch1G" (siehe
Konfiguration).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 172 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.24 [DSJ-QSErh-0210] MonpNeueSollzuordnungenRse
Paket: de.tollcollect.zme.qs.erh.rule.monp
Der Job MonpNeueSollzuordnungenRse dient der Erstellung der beiden MonP-Information-Events
NeueZuordRseMoBAB und NeueZuordRseEoBS. Die Events werden erzeugt, sobald sich die
Sollzuordnungen von RsE-EO (BS) bzw. RsE-MO (BAB) geändert haben (siehe dazu auch Job [DSJQSErh-0021] ProcessOnqSollzuordnungJob in Kapitel 5.2.5.1.5).
Einen allgemeinen Überblick über die Schnittstelle und die beteiligten Programmteile finden Sie im
Kapitel 6.1.6 SST 819 QS-AV  MonP unter Technische Realisierung der Schnittstelle innerhalb der
QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 68: Job-Ablauf (schematisch): MonpNeueSollzuordnungenRse
Trigger: Der MonpNeueSollzuordnungenRse-Job wird 1x täglich vom Bookkeeper getriggert.
Ablauf
1. Generierung der Info-Events für MO/BAB:
a. Einlesen der RsE-MO-Sollzuordnungen (erh_onq_sollzuordnung_mo_tag) und
der zugehörigen RsE-Kontakte (erh_onq_rse_kontakt_id_korrigiert)
b. Erzeugen eines Info-Events für jede neue Sollzuordnung des aktuellen Tages und
Einreihung des Events in die Warteschlange
(tech_monp_pending_information_event).
2. Generierung der Info-Events für EO/BS:
a. Einlesen der RsE-EO-Sollzuordnungen (erh_onq_sollzuordnung_eo_tag) und
der zugehörigen RsE-Kontakte (erh_onq_rse_kontakt_id_korrigiert)
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 173 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
b. Erzeugen eines Info-Events für jede neue Sollzuordnung des aktuellen Tages und
Einreihung des Events in die Warteschlange
(tech_monp_pending_information_event).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 174 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.25 [DSJ-QSErh-0220] CognosErhebungsquotenUpdater
Paket: de.tollcollect.zme.qs.erh.rule.cognos
Der Job stellt die Kennzahlen zu Mautobjekten für die Cognos Schnittstelle (SST 884) zur späteren
Erstellung des Erhebungsquotenreports bereit.
Einen allgemeinen Überblick über die Schnittstelle und die beteiligten Programmteile finden Sie im
Kapitel 6.1.7 SST 884 QS-AV  Cognos unter Technische Realisierung der Schnittstelle innerhalb
der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 69: Job-Ablauf (schematisch): CognosErebungsquotenUpdater
Trigger: Der CognosErhebungsquotenUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
1. Löschen aller Daten aus der Zieltabelle tech_cognos_erhebungsquoten
(Hintergrund: Die Daten werden immer für den gesamten Betrachtungszeitraum neu
generiert.)
2. Zusammenführung der Daten der Input-Tabellen (Join). Betrachtet werden dabei alle Daten,
die dem konfigurierten Betrachtungszeitraum entsprechen (definiert durch Konfigurations-Key
erh.cognos.datenumfang_tage_erh). Die Daten werden übernommen und ggf.
zusammengesetzt oder berechnet. Beispiele sind:
 zeitscheibe_start/ende  Nutzungstag als UNIX-Timestamp in Sekunden
 streckenname  Streckenname aus den GDBS-Tabellen für das Mautobjekt
 fahrleistungsklasse  Errechnet aus dem Nutzungstag und den
Fahrleistungsklassen der Tabelle erh_mo_fahrleistungsklassen
 bewertungsergebnis  Übernahme des Feldes beschreibung aus der Tabelle
erh_modellbewertungen für das erste passende Nicht-Null-Element, aufsteigend nach
regel_id und regel_sub_id
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 175 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
3. Speicherung der ermittelten Daten in der Tabelle tech_cognos_erhebungsquoten.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
erh.cognos.datenumfang_tage_erh 5 Anzahl der Tage (rückwirkend), für die die
Erhebungsquoten für Cognos bereitgestellt
werden sollen; definiert den
Betrachtungszeitraum.
Tabelle 89: CognosErhebungsquotenUpdater – Konfiguration in Tabelle tech_config
Fehlerbehandlung
Fehler während des Joins, die aufgrund von null-Werten der Attribute auftreten werden abgefangen,
sofern es sich bei dem Attribute nicht um einen Primärschlüssel handelt. Die Zieltabelle ist so
gestaltet, dass sämtliche Attribute, ausgenommen Primärschlüssel, null sein können.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 176 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.26 [DSJ-QSErh-0230] CognosOnqUpdater
Paket: de.tollcollect.zme.qs.erh.rule.cognos
Der Job stellt ONQ-spezifische Kennzahlen für die Cognos Schnittstelle (SST 884) bereit.
Einen allgemeinen Überblick über die Schnittstelle und die beteiligten Programmteile finden Sie im
Kapitel 6.1.7 SST 884 QS-AV  Cognos unter Technische Realisierung der Schnittstelle innerhalb
der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 70: Job-Ablauf (schematisch): CognosOnqUpdater
Trigger: Der CognosOnqUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
1. Zusammenführung der Daten der Input-Tabellen (Join). Betrachtet werden dabei alle Daten,
die dem konfigurierten Betrachtungszeitraum (definiert durch Konfigurations-Key
erh.cognos.datenumfang_tage_onq) entsprechen.
2. Löschen aller Daten aus der Zieltabelle tech_cognos_onq.
(Hintergrund: Die Daten werden immer für den gesamten Betrachtungszeitraum neu
generiert.)
3. Speicherung der neu ermittelten Daten in der Tabelle tech_congos_onq.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden
Keys:
Key Default Beschreibung
erh.cognos.datenumfang_tage_onq 5 Anzahl der Tage (rückwirkend), für die die
ONQ-Kennzahlen für Cognos bereitgestellt
werden sollen; definiert den
Betrachtungszeitraum.
Tabelle 90: CognosOnqUpdater – Konfiguration in Tabelle tech_config
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 177 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Fehlerbehandlung
Mögliche Fehlerquelle: In sehr seltenen Fällen, kann es zu null-Werten in der Output-Tabelle
kommen, sollten die FzG-Typen der beiden Input-Tabellen sich unterscheiden. Bei einem Join kann
somit keine Zuordnung vorgenommen werden, was dazu führt, dass null-Werte für die betroffenen
Attribute der Output-Tabelle gesetzt werden.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 178 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.27 [DSJ-QSErh-0240] CognosDsrcUpdater
Paket: de.tollcollect.zme.qs.erh.rule.cognos
Der Job stellt DSRC-spezifische Kennzahlen, welche die Basis für die DSRCFehlerquotenberechnung in QS-AV bilden, für die Cognos Schnittstelle (SST 884) bereit.
Einen allgemeinen Überblick über die Schnittstelle und die beteiligten Programmteile finden Sie im
Kapitel 6.1.7 SST 884 QS-AV  Cognos unter Technische Realisierung der Schnittstelle innerhalb
der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
Abbildung 71: Job-Ablauf (schematisch): CognosDsrcUpdater
Trigger: Der CognosDsrcUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
1. Laden der DSRC-Verdichtungswerte für den konfigurierten Zeitraum aus der Tabelle
erh_onq_dsrc_verdichtung_stunde gefiltert nach Straßentypen BS und BAB, um
sicherzustellen, dass keine Daten doppelt gezählt werden.
2. Bilden der Anzahlen von Gut- und Schlechtfällen, gruppiert nach Tag und FzG-Typ für den
konfigurierten Betrachtungszeitraum (siehe Konfigurations-Key
erh.cognos.datenumfang_tage_dsrc).
3. Leeren der Zieltabelle tech_cognos_dsrc.
(Hintergrund: Die Daten werden immer für den gesamten Betrachtungszeitraum neu
generiert.)
4. Speicherung der neu ermittelten Daten in der Tabelle tech_cognos_dsrc.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 179 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys:
Key Default Beschreibung
erh.cognos.datenumfang_tage_dsrc 5 Anzahl der Tage (rückwirkend), für die DSRCDaten für Cognos bereitgestellt werden sollen;
definiert den Betrachtungszeitraum.
Tabelle 91: CognosDsrcUpdater – Konfiguration in Tabelle tech_config
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 180 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.2.5.1.28 [DSJ-QSErh-0250] ModellBewertungenRuleJobExecutor
Paket: de.tollcollect.zme.qs.erh.rule.modellbewertung
Dieser Job bewertet die Qualität von Mautobjekten der produktiven Betriebsdaten. Dazu werden die
Ergebnisse der MOBA-Verdichtung regelbasiert ausgewertet und als Modellbewertungsergebnisse
und Modellauffälligkeiten gespeichert. Der Versand an den Grunddatenprozess ist nicht Teil dieses
Jobs, sondern erfolgt mittels der Jobs [DSJ-QSErh-0011] GdbsProductsCreator und [DSJ-QSErh0012] GdbsUploadPump.
Hinweis: Die Bewertung der Qualität von Erkennungsobjekten erfolgt im Job [DSJ-QSErh-0120]
EobaModellBewerter.
Abbildung 72: Job-Ablauf (schematisch): ModellBewertungenRuleJobExecutor
Trigger: Der ModellBewertungenRuleJobExecutor-Job wird 1x täglich vom Bookkeeper gestartet.
Ablauf
1. Prüfung anhand der Tabelle tech_kalender, ob für den vom Bookkeeper übergebenen
Bewertungstag eine Modellbewertung vorgenommen werden soll. Soll keine Bewertung
erfolgen, wird der Job beendet.
2. Einlesen folgender Daten für den Bewertungstag:
a. MOBA-Events (erh_moba_event_auswahl_fuer_bewertung)
b. MOBA-Verdichtungsergebnisse je Tag bzgl. MOBA- und Erhebungsquoten
(erh_moba_mo_verdichtung_tag_by_tag_mobaquote_mvw und
erh_moba_mo_verdichtung_tag_by_tag_erhquote_mvw)
c. Fahrleistungsklassen (erh_mo_fahrleistungsklassen)
3. Schwellwertbasierte Ausführung der Bewertungsregeln.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 181 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
4. Persistierung der Bewertungsergebnisse in der Tabelle erh_modellbewertungen.
Konfiguration
Die benötigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert
konfiguriert:
Key Default Beschreibung
erh.modellbewertung.max_erhebungsquote 99 Maximaler Schwellwert unterhalb (<=) dessen
weitere Bedingungen pro Mautobjekt geprüft
werden.
erh.modellbewertung.min_anzahl_erhebungen_
auffaelliges_mautobjekt
500 Minimaler Schwellwert ab dem (>=) weitere
Bedingungen pro Mautobjekt geprüft werden.
erh.modellbewertung.min_anzahl_
erhebungsluecken_auffaelliges_mautobjekt
50 Minimaler Schwellwert ab dem (>=) weitere
Bedingungen pro Mautobjekt geprüft werden.
Achtung: bei Regel 1001 begrenzt dieser
Schwellwert die minimale Anzahl von
Doppelerhebungen und nicht von
Erhebungslücken.
erh.modellbewertung.max_anzahl_benutzer_
indikator_baustellenverkehr
10 Maximaler Schwellwert Anzahl verschiedener
Nutzer unterhalb (<=) dessen eine
Erhebungsanomalie pro Mautobjekt als
Baustellenverkehr kategorisiert wird.
erh.modellbewertung.max_anzahl_stunden_fuer_
temporaere_erhebungsanomalie
5 Maximales Zeit-Intervall in Stunden eines
Bewertungstages innerhalb (<=) dessen das
Auftreten einer Erhebungsanomalie pro
Mautobjekt als temporär kategorisiert wird.
erh.modellbewertung.id_string_prod_bd_instanz PROD String-Konstante zur Identifizierung der
Produktions-Instanz.
erh.modellbewertung.max_mobaquote 98 Maximaler Schwellwert unterhalb (<=) dessen
weitere Bedingungen pro Mautobjekt geprüft
werden.
erh.modellbewertung.min_anzahl_events_
auffaelliges_mautobjekt
50 Minimale Anzahl an Events pro Mautobjekt, ab
der (>=) weitere Bedingungen geprüft werden.
Tabelle 92: ModellBewertungenRuleJobExecutor – Konfiguration in Tabelle tech_rule_Schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException
geworfen.
5.2.5.2 Anonymisierung, Archivierung und Löschung
Die Anonymisierung, Archivierung und Löschung von personenbezogenen Daten der Komponente
qs-erh erfolgen in der Regel via TTLs (Time-To-Live: Standardlöschintervalle, die bei Anlage der
Datenbanktabellen definiert werden). Vorgaben und Job-Details sind dem Systemlöschkonzept [14] zu
entnehmen.
Folgende Jobs werden dort zusätzlich beschrieben:
 BefahrungsparameterArchivCreation
5.2.6 qs-common-test: Testspezifische Basiskomponenten und Utilities
Dieses Paket enthält testspezifische Utilities und Tools. Es wird ausschließlich im Test-Scope als
Brücke zur Ausführung der Integrations- und Systemtests eingesetzt.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 182 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.3 Timebucket-Switcher Applikation
5.3.1 qs-timebuchetswitcher
Der Timebucket-Switcher ist eine proprietäre Applikation, welche die Timebuckets für die CassandraTabellen aktualisiert und somit die Datenverarbeitung durch den Bookkeeper taktet. Jede
Aktualisierung eines Timebuckets wird im Zustands-Log protokolliert. Anhand dieser Information kann
der Bookkeeper entscheiden, welche Jobs gestartet werden können.
Die Timebucket-Switcher Applikation ist single-threaded und switcht je Durchlaufzyklus alle
konfigurierten Timebuckets gemäß ihres festgelegten Intervalls. Nach jedem Zyklus wird 2 Sekunden
gewartet bevor ein neuer Zyklus gestartet wird.
5.3.1.1 Ablauf
Abbildung 73: Timebucket-Switcher – Klassen und Objekte (Ablauf)
 Die Klasse LongRunner wird bei Programmstart instanziiert und gestartet. Sie kontrolliert die
Abarbeitung der Timebucket-Switcher-Aufgaben über die Klasse Timebucket-Switcher,
die analog zur Klasse JobExecutor des Bookkeepers das Trait LongRunnable erweitert
und durch den LongRunner zyklisch ausgeführt wird.
 Der Timebucket-Switcher liest TimebucketSchemata von Timer- und TabellenTimebuckets aus der Konfiguration (RuleConfigTimebuckets und aktualisiert die
Timebucket-Zeitstempel dementsprechend in der Tabelle en_q_timebuckets (TimebucketKatalog) = „switch“. Die Ergebnisse der Verarbeitung werden im Zustands-Log
(bkpr_eventlog) durch neue Protokolleinträge vom Typ 1 (LDSTimebucketSwitched) mit
dem Zustand „created“ festgehalten (siehe dazu auch Kapitel 5.1.6.4 Datenbanktabellen).
5.3.1.2 Allgemeine Hinweise
Der Timebucket-Switcher erzeugt auch für Zeitabschnitte der Eingangsdatenströme, die z.B. durch
eine Downtime des Systems, beim Switch übersprungen wurden, Protokolleinträge im Zustands-Log
(bkpr_eventlog) an, um die Job-Verarbeitung im Bookkeeper nicht negativ zu beeinflussen.
5.3.1.3 Konfiguration
Die Konfiguration des Timebucket-Switchers erfolgt über die Klasse RuleConfigTimebuckets, die
TimebucketSchemas definiert. Details zu der Konfiguration sind dem Kapitel 5.1.5 Konfiguration im
Schwerpunktkapitel: Bookkeeper (qs-common) zu entnehmen.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 183 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Abbildung 74: Timebucket-Switcher – Klassen und Objekte (Konfiguration)
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 184 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.4 QS-AV Webservice-Applikation
5.4.1 qs-webservice
Die QS-AV Webservice-Applikation ist eine proprietäre Web-Applikation zur Exposition von
Webservices durch QS-AV. Des Weiteren bietet sie eine Plattform für die Bereitstellung von WebOberflächen beispielsweise zur Konfiguration der Schwellwerte und/oder technischen Konfigurationen,
die in der Cassandra DB gehalten werden.
Technisch basiert die Web-Applikation auf dem Play Framework9
. Sie ist in Scala implementiert, als
Buildsystem wird Scala sbt10 eingesetzt.
Die QS-AV Webservice-Applikation wird auf einem eigenen Pod (Webservice-Pod) in der AppAgileUmgebung deployed.
5.4.1.1 Authentifizierung am Webservice
Zur Authentifizierung und Autorisierung wird der QS-AV Webservice unter Nutzung des ShiroFrameworks mit dem TC-Active Directory verbunden. Dazu ist die Datei
Shiro_activedirectory.ini im Zuge des Deployment-Prozesses mit Hilfe der DeploymentKonfiguration anzupassen.
5.4.1.2 Konfiguration
Es gibt drei verschiedene Konfigurationsebenen:
1. Konfiguration mittels Umgebungsvariablen (aus der Deployment-Konfiguration). Weitere
Details dazu finden Sie im Referenzdokument [16] [A_QSAV_HBI_00] QS-AV
Installationshandbuch.
2. Applikationskonfigurationsdateien:
Play besitzt ein hierarchisches Konfigurationssystem.
Die übergeordnete Konfiguration findet sich in der Datei conf/application.conf.
Die Produktionskonfiguration ist in der Datei conf/application_production.conf
enthalten und referenziert die übergeordnete Konfiguration.
Achtung: Nur Unterschiede werden hier eingetragen (das eine Script includiert das andere).
3. Argumente (nur für die Entwicklung relevant):
Jeder Wert aus der Konfigurationsdatei kann auch als Kommandozeilenargument gesetzt
werden. Z.B. würde -Dauth.mode=ldap das gleiche bewirken, wie wenn der Schlüssel
auth.mode in der übergeordneten Konfigurationsdatei auf ldap gesetzt werden würde.

9 Weitere Informationen und die offizielle Dokumentation finden Sie unter:
https://www.playframework.com/
10 Weitere Informationen und die offizielle Dokumentation finden Sie unter: http://www.scalasbt.org/index.html
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 185 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.5 DSE-Ring (Datenhaltung)
In diesem Abschnitt werden die Soll- und Schwellwerte, die innerhalb der Persistenzschicht gehalten
werden, dokumentiert.
Informationen zum Aufbau und der Technologiebasis des DSE-Rings entnehmen Sie bitte dem
gleichnamigen Architekturkapitel 4.2.2 DSE-Ring (Datenhaltung).
Das Datenmodell (siehe Referenzdokument [13]) wird zusammen mit den Software-Lieferungen
zur Verfügung gestellt.
5.5.1 Soll- und Schwellwerte
Alle Soll- und Schwellwerte der QS-AV Applikation sowie der Timebucket-Switcher Applikation, mit
Ausnahme des EOBA Clustering (siehe Kapitel 5.5.1.9 EOBA Clustering), werden in der CassandraTabelle tech_rule_schwellwert verwaltet. Das EOBA Clustering wurde in die separate
Konfigurations-Tabelle erh_eoba_clusterconfig ausgelagert, um den Anforderungen der
Versionierung und Nachvollziehbarkeit der Konfiguration nachkommen zu können.
Die Tabellen sind im laufenden Betrieb konfigurierbar und können somit das Verhalten der
Applikationssteuerung und insbesondere der Qualitätssicherungsalgorithmen steuern.
Hinweis: Die Soll- und Schwellwerte sind durch den Fachbereich vorgegeben bzw. werden durch
den Fachbereich auch im Betrieb verwaltet und werden keiner inhaltlichen Prüfung unterzogen.
Konfigurationshinweis: Double-Werte müssen mit einem Punkt getrennt werden. Beispiel: 0.75
5.5.1.1 FzG (MDN-Normierung)
Key in Cassandra-Tabelle Default Beschreibung
fzg.mdn.1_batt 100 Minimalwert der Batteriespannung für
Hersteller mit ID 1 (Grundig).
fzg.mdn.2_batt 101 Minimalwert der Batteriespannung für
Hersteller mit ID 2 (Siemens).
fzg.mdn.3_batt 102 Minimalwert der Batteriespannung für
Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_batt 103 Minimalwert der Batteriespannung für
Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_batt 104 Minimalwert der Batteriespannung für
Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_batt 105 Minimalwert der Batteriespannung für
Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_batt 106 Minimalwert der Batteriespannung für
Hersteller mit ID 7 (Siemens DIN-Schacht
Pilot).
fzg.mdn.8_batt 107 Minimalwert der Batteriespannung für
Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_batt 108 Minimalwert der Batteriespannung für
Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_batt 110 Minimalwert der Batteriespannung für
Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_batt 111 Minimalwert der Batteriespannung für
Hersteller mit ID 11 (Bosch DIN-Schacht).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 186 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key in Cassandra-Tabelle Default Beschreibung
fzg.mdn.12_batt 112 Minimalwert der Batteriespannung für
Hersteller mit ID 12 (Bosch DIN-Schacht
Pilot).
fzg.mdn.13_batt 113 Minimalwert der Batteriespannung für
Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_batt 114 Minimalwert der Batteriespannung für
Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_capa_flash 200 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 1 (Grundig).
fzg.mdn.2_capa_flash 201 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 2 (Siemens).
fzg.mdn.3_capa_flash 202 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_capa_flash 203 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 4 (Siemens
Pilot).
fzg.mdn.5_capa_flash 204 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 5 (Grundig Pilot
16MB).
fzg.mdn.6_capa_flash 205 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 6 (Grundig
16MB).
fzg.mdn.7_capa_flash 206 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 7 (Siemens DINSchacht Pilot).
fzg.mdn.8_capa_flash 207 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 8 (Siemens DINSchacht).
fzg.mdn.9_capa_flash 208 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 9 (Siemens
1373++ Pilot).
fzg.mdn.10_capa_flash 210 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 10 (Siemens
1373++).
fzg.mdn.11_capa_flash 211 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 11 (Bosch DINSchacht).
fzg.mdn.12_capa_flash 212 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 12 (Bosch DINSchacht Pilot).
fzg.mdn.13_capa_flash 213 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_capa_flash 214 Minimalwert der freien Speicherkapazität
(Flash) für Hersteller mit ID 14 (Bosch 2G
Pilot).
fzg.mdn.1_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 1 (Grundig).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 187 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key in Cassandra-Tabelle Default Beschreibung
fzg.mdn.2_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 2 (Siemens).
fzg.mdn.3_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 7 (Siemens DIN-Schacht
Pilot).
fzg.mdn.8_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 12 (Bosch DIN-Schacht
Pilot).
fzg.mdn.13_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_sat_fs 40 Minimalwert der Satellitenfeldstärke für
Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 1 (Grundig).
fzg.mdn.2_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 2 (Siemens).
fzg.mdn.3_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 3 (Grundig Pilot).
fzg.mdn.4_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 4 (Siemens Pilot).
fzg.mdn.5_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 6 (Grundig 16MB).
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 188 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key in Cassandra-Tabelle Default Beschreibung
fzg.mdn.7_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 10 (Siemens 1373++).
fzg.mdn.11_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 13 (Bosch 2G).
fzg.mdn.14_sat_fs_sprung 10 Maximalwert der
Satellitenfeldstärkensprünge für Hersteller
mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 1 (Grundig).
fzg.mdn.2_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 2 (Siemens).
fzg.mdn.3_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 7 (Siemens DIN-Schacht
Pilot).
fzg.mdn.8_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 189 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key in Cassandra-Tabelle Default Beschreibung
Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 12 (Bosch DIN-Schacht
Pilot).
fzg.mdn.13_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_wtr_sprung 10 Maximalwert der Wheel-Tick-Sprungrate für
Hersteller mit ID 14 (Bosch 2G Pilot).
Tabelle 93: Soll- und Schwellwerte für FzG (MDN-Normierung)
5.5.1.2 FzG-Bewertung
Die Schwellwerte setzen sich zusammen aus 19 FzG-Fehlerbildern zu je folgenden 3 Kategorien:
 aktiveTage: Anzahl der aktiven Tage (aktiver Tag = Eintrag in FzG-Verdichtungstabelle
existiert), die pro ehk_id bei der Bewertung berücksichtigt werden
 fehler: Min. Anzahl an Fehlern pro aktivem Tag, damit der Tag als fehlerhaft bewertet wird
 aktiveTageFehler: Min. Anzahl an fehlerhaften Tagen im Zeitraum der aktiven Tage, damit
die FzG-Bewertung das Fehlerbild mittels TRUE als „auffällig“ bewertet.
Die Schwellwert-Keys die sich daraus ergeben haben folgendes Muster:
fzg.bewertung.<Fehlerbild>.<Kategorie>
Die folgende Tabelle enthält alle Defaultwerte für die Schwellwerte unter Berücksichtigungen des
Mapping von Fehlerbild auf Kategorie:
Fehlerbild aktiveTage fehler aktiveTageFehler
fallback 1 1 1
dienst_de_aktiv_defekt 1 1 1
sw_version_nicht_aktuell 1 1 1
batt_spannung_gering 5 1 3
dauerplus 1 1 1
keine_tachodaten 5 1 4
keine_gyrodaten 5 1 4
tachodaten_unplausibel 5 1 4
gyrodaten_unplausibel 5 1 4
capa_flash 5 1 4
uw_errorflag 5 1 4
probefahrt_nicht_bestanden 5 1 1
ehk_fehler 5 1 4
gps_fehler 5 1 4
zeitsprung 5 1 4
wtr_sprung 5 1 4
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 190 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Fehlerbild aktiveTage fehler aktiveTageFehler
sat_fs 5 1 4
mac_fehler 1 1 1
loop_fehler 1 1 1
Tabelle 94: Soll- und Schwellwerte für FzG-Bewertung
5.5.1.3 Eventnormierung
Key in Cassandra-Tabelle Default Beschreibung
fzg.deklaration_events.interpretation.dekl_
p_gewicht_ohne_achs.max_zeitdifferenz_
sekunden
10000 Maximale Zeitdifferenz in Sekunden
zwischen passivem GewichtsdeklarationsEvent und passivem AchsdeklarationsEvent, damit das Regelbit
rb_dekl_p_gewicht_ohne_achs den
Wert FALSE hat.
Beachte: Obere Grenze nicht
eingeschlossen.
Wichtig: Dieser Schwellwert darf
nicht größer sein als die Timebucket-Größe
der Eingangstabelle lds_es_en_q_
deklaration_events. Andernfalls muss
die Bookkeeper-Konfiguration angepasst
werden.
Tabelle 95: Soll- und Schwellwerte für Eventnormierung
5.5.1.4 ONQ/DSRC
Key in Cassandra-Tabelle Default Beschreibung
erh.onq.led_rot_grund_query 19028,
19478,
2620
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgründe
(Nummern) indizieren manuelles Verfahren.
erh.onq.led_rot_grund_rule_sperre_query 33420,
35376
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgründe
(Nummern) indizieren gesperrte FzG.
erh.onq.bd_instanz_produktion 1 RsE-Kontakte mit bd_instanz=1
stammen aus der produktiven BD-Instanz.
erh.onq.konau_nummernkreis_oben 5000 Schwellwert für den Nummernkreis der RsEID (oben) , zur Kennzeichnung der
Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten 4000 Schwellwert für den Nummernkreis der RsEID (unten) , zur Kennzeichnung der
Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben 8000 Schwellwert für den Nummernkreis der RsEID (oben) , zur Kennzeichnung der
Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten 6000 Schwellwert für den Nummernkreis der RsEID (unten) , zur Kennzeichnung der
Kontrollart KonSL.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 191 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key in Cassandra-Tabelle Default Beschreibung
erh.onq.zeit_onq_sek 3600 Anzahl der Sekunden rückwirkend des RSEKontaktzeitpunkts, der für die Berechnung
betrachtet wird.
erh.onq.led_farbe_rot 0 LED-Farbe der RsE-Kontakte mit
led_farbe=0 ist rot.
erh.onq.led_farbe_gruen 1 LED-Farbe der RsE-Kontakte mit
led_farbe=1 ist grün.
erh.onq.anz_gruen 3 Schwellwert für die Mindestanzahl an RsEKontakten mit led_farbe gruen für eine
RsE-ID.
erh.onq.ant_ausfahrt 20 Schwellwert für die erlaubte Quote an
Ausfahrten für eine RsE.
erh.onq. anz_rse_gesamt_eo 280 Anzahl der RSE-Kontakte, die mindestens
notwendig sind, um eine neue EOSollzuordnung zu berechnen.
erh.onq. anz_rse_gesamt_mo 280 Anzahl der RSE-Kontakte, die mindestens
notwendig sind, um eine neue MOSollzuordnung zu berechnen.
erh.onq. anz_rse_gesamt_eo_mo 280 Anzahl der RSE-Kontakte, die mindestens
notwendig sind, um eine neue EO-MOSollzuordnung zu berechnen.
erh.onq.erhebungsparameterabfahrt 2 RsE-Kontakte mit
erhebungsparameter=2 indizieren
Abfahrten.
erh.onq.schwellwert_erhebung_tage_zurueck 5 Zeitraum in Tagen, wie lange ein
Befahrungsparameter rückwirkend für die
Berechnungen herangezogen werden darf.
erh.onq.rse_suchfenster_bs 3600 Sekunden rückwirkend zum RSEKontaktzeitpunkt: Festlegung des Starts des
Suchfensters für die Berechnung der DSRCQuote (Ende = Kontaktzeitpunkt).
erh.onq.dsrc_quote 98 Mindestwert für die DSRC-Quote einer RsE,
damit sie für die FzG-bezogene DSRCQuoten-Verdichtung berücksichtigt wird.
Hintergrund: Für die Verdichtung dürfen nur
Daten zu RsEs mit einer DSRC-Quote
größer des angegebenen Schwellwertes
berücksichtigt werden, um die FzGbezogenen Verdichtungsergebnisse nicht zu
verfälschen.
erh.onq.use_onq_workaround true Gibt an ob der Workaround des RsE-IDMappings für die Verarbeitung verwendet
werden soll.
true= Workaround soll verwendet werden,
false= Workaround soll nicht verwendet
werden.
Tabelle 96: Soll- und Schwellwerte für ONQ/DSRC
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 192 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.5.1.5 MOBA
Key in Cassandra-Tabelle Default Beschreibung
erh.moba.betrachtungszeitraum_tage 5 Konfiguration des Betrachtungszeitraums. Es
dürfen nur Befahrungen in MOBA und in der
Verdichtung berücksichtigt werden, die
gemessen an der Befahrungszeit nicht älter
sind, als die konfigurierbare Anzahl von
Tagen (Default-Wert).
Tabelle 97: Soll- und Schwellwerte für MOBA
5.5.1.6 EOBA
Key in Cassandra-Tabelle Default Beschreibung
erh.eoba.betrachtungszeitraum_tage 5 Gibt die Anzahl der Tage an, die die
Befahrungszeit des aktuellen
Erkennungsobjekts des
Befahrungsparameter-Datensatzes in der
Vergangenheit liegen darf, um noch im
Betrachtungszeitraum zu liegen (es gilt der
Ausfahrtszeitpunkt vom EO).
Tabelle 98: Soll- und Schwellwerte für EOBA
5.5.1.7 Modellbewertung
Key in Cassandra-Tabelle Default Beschreibung
erh.modellbewertung.grenzwert_anzahl_
teilbefahrung
20 Grenzwert für die Mindestanzahl an Events
pro EO, die als Teilbefahrungs-Event
klassifiziert wurden, damit eine Auffälligkeit
für das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
erhebungsquote
20 Grenzwert für die Mindestanzahl an Events
pro EO, die als Erhebungsquoten-Event
klassifiziert wurden, damit eine Auffälligkeit
für das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
fehlvergebuehrung
20 Grenzwert für die Mindestanzahl an Events
pro EO, die als Fehlvergebührungs-Event
klassifiziert wurden, damit eine Auffälligkeit
für das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
befahrungsaenderung
20 Grenzwert für die Mindestanzahl an Events
pro EO, die als Befahrungsänderungs-Event
klassifiziert wurden, damit eine Auffälligkeit
für das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
stunden
5 Grenzwert für die Mindestanzahl
unterschiedlicher Nutzungsstunden aller zu
einer EO-ID zugehörigen EOBA-Events an
diesem Tag, damit eine Auffälligkeit generiert
wird.
erh.modellbewertung.grenzwert_anzahl_
benutzer
5 Grenzwert für die Mindestanzahl
unterschiedlicher Benutzer aller zu einer EOID zugehörigen EOBA-Events an diesem
Tag, damit eine Auffälligkeit generiert wird.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 193 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key in Cassandra-Tabelle Default Beschreibung
erh.modellbewertung.grenzwert_anzahl_events 30 Grenzwert für die Mindestanzahl an Events
gegen das Erkennungsobjekt in der täglichen
Erkennungsobjektverdichtung, damit eine
Auffälligkeit für das EO generiert wird.
erh.modellbewertung.grenzwert_eoba_quote 98 Schwellwert für die EOBA-Quote, unterhalb
derer die Verdichtungsergebnisse überhaupt
weiter untersucht werden.
erh.modellbewertung.max_erhebungsquote 99 Maximaler Schwellwert unterhalb (<=)
dessen weitere Bedingungen pro Mautobjekt
geprüft werden.
erh.modellbewertung.min_anzahl_erhebungen_
auffaelliges_mautobjekt
500 Minimaler Schwellwert ab dem (>=) weitere
Bedingungen pro Mautobjekt geprüft werden.
erh.modellbewertung.min_anzahl_
erhebungsluecken_auffaelliges_mautobjekt
50 Minimaler Schwellwert ab dem (>=) weitere
Bedingungen pro Mautobjekt geprüft werden.
Achtung: bei Regel 1001 begrenzt dieser
Schwellwert die minimale Anzahl von
Doppelerhebungen und nicht von
Erhebungslücken.
erh.modellbewertung.max_anzahl_benutzer_
indikator_baustellenverkehr
10 Maximaler Schwellwert Anzahl
verschiedener Nutzer unterhalb (<=) dessen
eine Erhebungsanomalie pro Mautobjekt als
Baustellenverkehr kategorisiert wird.
erh.modellbewertung.max_anzahl_stunden_fuer
_temporaere_erhebungsanomalie
5 Maximales Zeit-Intervall in Stunden eines
Bewertungstages innerhalb (<=) dessen das
Auftreten einer Erhebungsanomalie pro
Mautobjekt als temporär kategorisiert wird.
erh.modellbewertung.id_string_prod_bd_instanz PROD String-Konstante zur Identifizierung der
Produktions-Instanz.
erh.modellbewertung.max_mobaquote 98 Maximaler Schwellwert unterhalb (<=)
dessen weitere Bedingungen pro Mautobjekt
geprüft werden.
erh.modellbewertung.min_anzahl_events_
auffaelliges_mautobjekt
50 Minimale Anzahl an Events pro Mautobjekt,
ab der (>=) weitere Bedingungen geprüft
werden.
Tabelle 99: Soll- und Schwellwerte für Modellbewertung
5.5.1.8 Betriebsdatenvalidierung
Key in Cassandra-Tabelle Default Beschreibung
erh.bdval.ng_befahrungen_grenzwert 10000 Grenzwert Anzahl auffälliger, nicht
geänderter Abschnitte in Bezug auf
Befahrungen
erh.bdval.ng_mobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger, nicht
geänderter Abschnitte in Bezug auf MOBAQuote
erh.bdval.ng_erhquote_grenzwert 10000 Grenzwert Anzahl auffälliger, nicht
geänderter Abschnitte in Bezug auf
Erhebungsquote
erh.bdval.ng_mobaquote_toleranz 5 Toleranz Abweichung MOBA-Quote (nicht 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 194 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key in Cassandra-Tabelle Default Beschreibung
geänderter Abschnitte)
erh.bdval.ng_erhquote_toleranz 5 Toleranz Abweichung EOBA-Quote (nicht
geänderter Abschnitte)
erh.bdval.ng_befahrungen_toleranz 5 Toleranz Abweichung Befahrungen (nicht
geänderter Abschnitte)
erh.bdval.g_befahrungen_grenzwert 10000 Grenzwert Anzahl auffälliger, geänderter
Abschnitte in Bezug auf Befahrungen
erh.bdval.g_mobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger,
geänderter Abschnitte in Bezug auf MOBAQuote
erh.bdval.g_erhquote_grenzwert 10000 Grenzwert Anzahl auffälliger, geänderter
Abschnitte in Bezug auf Erhebungsquote
erh.bdval.g_mobaquote_toleranz 5 Toleranz Abweichung MOBA-Quote
geänderter MO
erh.bdval.g_erhquote_toleranz 5 Toleranz Abweichung Erhebungsquote
geänderter MO
erh.bdval.g_befahrungen_toleranz 5 Toleranz Abweichung Befahrungen
veränderter MO
erh.bdval.n_mobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger,
neuer Abschnitte in Bezug auf MOBA-Quote
erh.bdval.n_erhquote_grenzwert 10000 Grenzwert Anzahl auffälliger, neuer
Abschnitte in Bezug auf Erhebungsquote
erh.bdval.n_erhquote_toleranz 95 Toleranz Abweichung Erhebungsquote neue
MO
erh.bdval.n_mobaquote_toleranz 95 Toleranz Abweichung MOBA-Quote neue
MO
erh.bdval.ng_eobaquote_toleranz 5 Toleranz Abweichung EOBA-Quote nicht
geänderter EO
erh.bdval.ng_eobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger nicht
geänderter EO in Bezug auf EOBA-Quote
erh.bdval.g_eobaquote_toleranz 5 Toleranz Abweichung EOBA-Quote
geänderter EO
erh.bdval.g_eobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger geänderter EO
in Bezug auf EOBA-Quote
erh.bdval.n_eobaquote_toleranz 95 Toleranz Abweichung EOBA-Quote neuer
EO
erh.bdval.n_eobaquote_grenzwert 10000 Grenzwert Anzahl auffälliger neuer EO in
Bezug auf EOBA-Quote
erh.bdval.tq_sum_maut_ok_toleranz 1 Schwellwerte für Tarfierungsqualität
erh.bdval.tq_sum_ext_kosten_toleranz 1 Schwellwerte für Tarfierungsqualität
erh.bdval.tq_anz_tarif_ok_toleranz 1 Schwellwerte für Tarfierungsqualität
erh.bdval.tq_anz_tarif_nok_toleranz 1 Schwellwerte für Tarfierungsqualität
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 195 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Tabelle 100: Soll- und Schwellwerte für Betriebsdatenvalidierung
5.5.1.9 EOBA Clustering
Hinweis: Die Konfiguration der Schwellwerte erfolgt in der gesonderten Tabelle
erh_eoba_clusterconfig. Sie ist keine reine Schwellwerttabelle, sondern enthält eine
Kombination aus Konfigurations- und Schwellwerten. Je Clustertyp wird ein initialer
Konfigurationsdatensatz angelegt, der anhand des Keys cluster_typ einem Cluster zugeordnet
werden kann. (Für die Defaultwerte zu config_id und cluster_typ in der folgenden Tabelle sind
daher nur Platzhalter dokumentiert.)
Key in Cassandra-Tabelle Default Beschreibung
config_id <entspricht
initial dem
cluster_typ>
Eindeutige ID der Konfiguration (Ganzzahl)
cluster_typ <0,1 oder 2> Clustertyp: 0 = Lückenschluss, 1 = Tor, 2 =
Kreis
anzahl_befahrungen_mo_min 50 Minimale Anzahl an Befahrungsparameter
aus dem Archiv je EO-ID, die für die
Durchführung des Clustering erforderlich ist.
anzahl_befahrungen_mo_max 1000 Maximale Anzahl an Befahrungsparameter
aus dem Archiv je EO-ID. Existieren mehr
BfpA als durch diesen Key konfiguriert,
werden genauso viele BfpA zufällig
ausgewählt.
zeitintervall 1 Anzahl an vergangenen Tagen, für die die
Befahrungsparamater aus dem
Befahrungsparameter-Archiv in die Analyse
einbezogen werden sollen.
min_pts 5 OPTICS-spezifischer Parameter:
Mindestanzahl von Punkten in einer
Nachbarschaft vom Radius Epsilon, ab der
eine Gruppe von Punkten als Cluster
betrachtet werden kann.
epsilon 1.0 OPTICS-spezifischer Parameter: Radius in
dem nach Punkten gesucht wird, damit diese
als Nachbarn gelten.
epsilon_cl 1.0 OPTICS-spezifischer Parameter:
Erforderlicher Mindestabstand zwischen den
Zentren von Clustern, damit diese noch als
eigenständige Cluster betrachtet werden.
anzahl_cluster_max 1 Maximal erlaubte Anzahl von Clustern.
Werden mehr Cluster erzeugt, werden die
Cluster nicht weiter verarbeitet.
gueltig_von 2006-12-
31T23:59:59Z
Startzeitpunkt des Gültigkeitszeitraums
(Timestamp) der Konfiguration.
gueltig_bis 2006-12-
31T23:59:59Z
Endzeitpunkt des Gültigkeitszeitraums
(Timestamp) der Konfiguration.
Tabelle 101: Soll- und Schwellwerte für EOBAClustering
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 196 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.6 Gesundheitsakte
Die Gesundheitsakte stellt eine GUI zur Verfügung, die dem Anwender auf einfachem und schnellen
Weg alle Informationen zu einem Fahrzeuggerät (FzG) anzeigt.
Dabei soll neben dem gültigen Rollen-Rechte-Konzept und einer hohen Verfügbarkeit auch eine
maximale Skalierbarkeit möglich sein.
Dieses Kapitel dient der Dokumentation und Anleitung zur Bedienung und Wartung der
Gesundheitsakte. Speziell wird auf die einzelnen Funktionalitäten, Möglichkeiten und Grenzen
eingegangen.
5.6.1 Aufbau und Funktionsweise
Bei der Gesundheitsakte handelt es sich um eine browserbasierte Server-Client Applikation. Der
Client sendet einen Post-Request an den Server, dieser antwortet als Response in Form von HTMLCode. Der Server kann sowohl über http als auch über https angesprochen werden. Der Zugriff auf
die Gesundheitsakte erfolgt über gängige Browser mit einem benutzerspezifischen Login, das über
das Toll-Collect Berechtigungsmanagement beantragt werden muss.
Die Gesundheitsakte verfügt über eine statische Seitenstruktur (Hauptmenü und weitere statische
Elemente) sowie über dynamische Inhalte (berechtigungsabhängige Menüpunkte und Unterseiten).
Die Applikation gliedert sich in folgende Bereiche:
 FzG-Suche
 ÜPA
 Schwellwerte
 Admin
Das Berechtigungs- und Rollenkonzept für die Gesundheitsakte ist im Referenzdokument [12]
[A_QSAV_BRK_00] QS-AV (QS-FzG/QS-Erh) Benutzer- Rollenkonzept beschrieben.
Der technische Aufbau ist im Architektur-Kapitel 4.2.6 Gesundheitsakte (Präsentation) beschrieben.
5.6.1.1 Aufbau der Seitenstruktur
Die Main.cgi erhält nach erfolgreicher Authentifizierung die Anfragen(Requests) des Benutzers
(Client) und leitet diese entsprechend an die Unterseiten weiter. Die Main.cgi baut die Menüleiste
und alle statischen Elemente selbstständig zusammen. Der Inhalt der Unterseiten wird anhand der
ermittelten Berechtigungsinformationen aus entsprechenden Dateien für die Unterseiten geliefert und
in die statische Struktur anhand von Platzhaltern integriert.
Abbildung 75: Aufbau des Hauptmenüs und der statischen Struktur (hervorgehoben)
5.6.1.2 Berechtigungskonfiguration
Die Zuordnung von Benutzergruppen (Active Directory-Gruppen) zur Berechtigung auf die
Applikationsbereiche FzG-Suche, ÜPA, Schwellwerte und Admin können über eine entsprechende 
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 197 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Rechtedatei (<basepath>/var/www/merkur/log/rights) konfiguriert werden. Dabei ist immer
das aktuelle Berechtigungs- und Rollenkonzept (Referenzdokument [12]) zu beachten.
5.6.1.3 Bereich: FzG-Suche
Nach dem Einloggen gelangt der Benutzer direkt auf die FzG-Suche. FzG können per EHK-Id gesucht
werden.
Abbildung 76: Einstieg FzG-Suche
Für den Menüpunkt „FzG-Suche“ existieren spezielle Maskenkonfigurationsdateien, um die
berechtigungsabhängigen Abfragen/Darstellungen/Ausgaben zu spezifizieren, ohne dabei den
Quellcode anfassen zu müssen. Sie befinden sich unter <basepath>/var/www/merkur/mask. Im
Bereich „Admin“  „Einstellungen“ können berechtigte Benutzer diese Konfiguration auch per GUI
anpassen.
Nach einer erfolgreichen Suche eines FzG werden die Statusinformationen des FzG in Abhängigkeit
der Maskenkonfigurationen angezeigt:
Abbildung 77: FzG-Suche - FzG Statusanzeige
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 198 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Statusüberblick
Im oberen Bereich werden FzG-Informationen angezeigt, untergliedert in folgende Tabellen:
 Basisdaten
 Letzte Meldungen
 Dienstzustand
Sie dienen dem schnellen Überblick über die wichtigsten Statusinformationen des FzG.
Popup-Links (Buttons)
Darunter wird eine Button-Gruppe angezeigt, die Links zu Detailinformationen zum Zustandswechsel
(FzG-Statusmeldung Historie) und zu Monitoringnachrichten des FzG zur Verfügung stellt.
Weitere Informationen
Darunter folgen diverse Tabellen zu speziellen Themengebiete, die ebenso wie die zuvor
beschriebenen Darstellungen über die Maskenkonfiguration angepasst werden können.
5.6.1.4 Bereich: ÜPA
Der Bereich „ÜPA“ ist für zukünftige Releases vorgesehen und ist aktuell noch nicht mit Funktionen
hinterlegt.
5.6.1.5 Bereich: Schwellwerte
Der Bereich „Schwellwerte“ ist für zukünftige Releases vorgesehen und ist aktuell noch nicht mit
Funktionen hinterlegt.
5.6.1.6 Bereich: Admin
Der Bereich „Admin“ widmet sich der administrativen Verwaltung der Gesundheitsakte. Die
Verwaltung gliedert sich in folgende Unterbereiche:
 Debugmeldungen:
Sofern im Unterbereich „Konfiguration“ (s.u.) die Option „Debug“ ausgewählt wurde, können
hier die Applikationslogs ab dem Level „debug“ eingesehen werden.
 Konfiguration:
In diesem Unterbereich können technische Konfigurationen vorgenommen werden:
o Debug (BOOLEAN): true = Die Applikationslogs sollen im (=ab) Level „debug“
geschrieben werden. Default = false.
o ldap_ou_dc: Verbindungsparameter zum LDAP-Server
o ldap_server: LDAP-Server Adresse
o qsav_keyspace: QS-AV Keyspace in der Cassandra-DB
o qsav_port: Port zur Adressierung der Cassandra DB
o qsav_server: IP zur Adressierung der Cassandra DB
o qsav_user_r: Name des technischen Users, der nur lesen darf
o qsav_user_r_pwd: Passwort des technischen Users, der nur lesen darf
o qsav_user_rw: Name des technischen Users, der lesen und schreiben darf
o qsav_user_rw_pwd: Passwort des technischen Users, der lesen und schreiben darf
 Schwellwerttabellen:
In diesem Unterbereich werden alle Tabellen, die im Bereich „Schwellwerte“ editierbar sein
sollen, konfiguriert.
Komponentensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 199 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
5.6.1.7 Hinweis: IE Kompatibilitätsmodus
Bei Nutzung des Internet Explorers muss die Kompatibilitätsansicht für Intranetseiten deaktiviert
werden.
Abbildung 78: IE - Einstellungen der Kompatibilitätsansicht
Abbildung 79: IE - Kompatibilitätsansicht für Intranetseiten deaktivieren
Schnittstellensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 200 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
6 Schnittstellensicht
6.1 Systemschnittstellen
In diesem Kapitel werden die Systemschnittstellen aufgeführt und kurz beschrieben. Die detaillierten
Schnittstellenspezifikationen finden Sie jeweils in den referenzierten Dokumenten.
6.1.1 Überblick: Schnittstellen
Abbildung 80: Lösungsskizze Detailansicht QS-AV(Datenflussrichtung)
Nr. Schnittstelle Protokoll Bemerkungen
814 GDBS  QS-AV TCP/IP Webservice-Schnittstelle
819 QS-AV  MonP TCP/IP Webservice-Schnittstelle (REST)
830 DM  QS-AV TCP/IP DB-Schnittstelle: CQL Inserts via Cassandra-Treiber
830 (II) DM  QS-AV TCP/IP Umgekehrte Zugriffsrichtung für FzG-Full-Importe: Von
DM bereitgestellte Oracle View via JDBC-Treiber.
833 ES  QS-AV TCP/IP DB-Schnittstelle: CQL Inserts via Cassandra-Treiber
836 EDM  QS-AV TCP/IP DB-Schnittstelle: CQL Inserts via Cassandra-Treiber
838 QS-AV  LogArchiv Server TCP/IP Technische Schnittstelle (via Splunk)
884 QS-AV  Cognos TCP/IP Webservice-Schnittstelle (REST)
Schnittstellensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 201 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Tabelle 102: Überblick: Schnittstellen in MSD26
Rahmenbedingung bei der Realisierung aller Schnittstellen: Die QS-AV-spezifischen
Überwachungsprozesse sind nicht Bestandteil des Leistungsdatenstroms. Schnittstellen aus dem
Überwachungsprozess zurück in den Leistungsdatenstrom sind nicht vorgesehen. Es muss
sichergestellt werden, dass die Leistungsdatenverarbeitung unabhängig und rückkopplungsfrei von
den QS-AV-Überwachungsprozessen erfolgt.
6.1.2 SST 814 Grunddaten Bereitstellungs-Server (GDBS)  QS-AV
Beschreibung Der Bereitstellungsserver Grunddaten liefert wichtige Modellinformationen für
EOBA und MOBA:
 Abschnittssequenzen
 Tarifabschnittsliste
 Erkennungsmodell
 Informationen über aktuelle Baustellen
 Release Notes (zusammen mit jeder neuen BD-Version)
 Release Historie
QS-AV liefert die Ergebnisse der Modellbewertung an GDBS:
 Auffälligkeiten
 Fahrleistungsklassen
 [Lückenschlussvorschläge] – dieses Produdukt wurde zwar in der SST
vorgesehen, wird von QS-AV jedoch nicht geliefert.
Schnittstellenspezifikation
(Referenzdokument)
[4] [S_814_SST_00] SST 814 SST Spezifikation GDBS – QS-AV
Tabelle 103: SST 814 GDBS  QS-AV
Abbildung 81: Überblick SST 814 GDBS  QS-AV
Abbildung 82: Überblick SST 814 QS-AV  GDBS (Lückenschlussvorschläge nicht realisiert)
SST 814
GDBS  QS-AV
Release
Historie Release Notes Abschnittssequenzen
Tarifabschnittsliste Baustelleninfo Erkennungsmodell
SST 814
QS AV  GDBS
Auffälligkeiten Fahrleistungsklassen
Lückenschlussvorschläge
Schnittstellensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 202 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
6.1.3 SST 830 Device Management (DM) - QS-AV
Beschreibung Folgende für die FzG-Fehlerdetektion erforderlichen Informationen werden aus
dem Device Management an die Qualitätssicherung AV übermittelt:
 FzG Statusmeldungen (FSM, zeitnah zur Entstehung)
o FzG Stammdaten
o FULL (täglich)
 FzG Einbau-Events (nach Registrierung)
 Monitoring-Nachrichten
 Fehler Events
o MAC-Fehler
o Loop-Fehler
Schnittstellenspezifikation
(Referenzdokument)
[5] [S_830_SST_00] SST 830 Schnittstellenspezifikation Device Management
– QS AV
Tabelle 104: SST 830 DM - QS-AV
Abbildung 83: Überblick SST 830 DM  QS-AV
SST 830
DM  QS AV
FzG
Statusmeldungen
(FSM)
FzG Stammdaten
FULL (1x täglich) FzG Einbau-Events
(zeitnah)
Monitoring
Nachrichten Fehler-Events
MAC-Fehler LOOP-Fehler
Schnittstellensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 203 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
6.1.4 SST 833 Erkennungs-Service (ES) - QS-AV
Beschreibung Folgende für die Erhebungsfehlerdetektion erforderlichen Informationen werden
aus dem Erkennungsservice an QS-AV übermittelt:
 Befahrungsparameter
 Events zu RsE-Kontakten
 FSP-Bewertungs-Events
 Rangier-Events
 Events bei Rücksetzen des Erkennungszustands
 Events bei Änderung der Aufzeichnung
 Events bei Rotschaltung aus technischem Grund
 Deklarations-Events
 Events bei Grünschaltung
Schnittstellenspezifikation
(Referenzdokument)
[6] [S_833_SST_00] SST 833 Schnittstellenspezifikation Erkennungs-Service
(ES) – Qualitätssicherung Erhebung (QS-Erh)
Tabelle 105: SST 833 ES - QS-AV
Abbildung 84: Überblick SST 833 ES  QS-AV
SST 833
ES  Q-AV
Befahrungsparameter
Events zu RsE-Kontakten
FSP-Bewertungs-Events
Rangier-Events
Event bei Rücksetzen des
Erkennungszustands
Event bei Änderung der
Aufzeichnung
Event bei Rotschaltung aus
technischem Grund
Deklarations-Event
Event bei Grünschaltung
Schnittstellensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 204 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
6.1.5 SST 836 Erhebungs-Daten-Management (EDM) - QS-AV
Beschreibung Über die Schnittstelle EDM-QS-AV werden Events übertragen, die zur
Qualitätssicherung und Prüfung der bestehenden Modelle ausgewertet werden.
Die Events von EDM können nach Fehler-Events und Metrik-Events
kategorisiert werden. Fehler-Events enthalten Informationen zu
Verarbeitungsfehlern innerhalb von EDM, die bei der Tarifierung und
Fahrtenbildung auftreten können. Metrik-Events enthalten eine oder mehrere
Kennzahlen, die Aussagen über die Verarbeitungsmengen und Produkte von
EDM in einem bestimmten Zeitraum ermöglichen.
 EDM-Fehlerevents
 EDM-Metrikevents
Schnittstellenspezifikation
(Referenzdokument)
[7] [S_836_SST_00] SST 836 Schnittstellenspezifikation EDM –
Tabelle 106: SST 836 EDM - QS-AV
Abbildung 85: Überblick SST 836 ES  QS-AV
6.1.6 SST 819 QS-AV  MonP
Beschreibung Die Schnittstelle dient dazu, im QS-AV anfallende Informationen (=Events)
einheitlich an die Monitoring Plattform zu übermitteln. Dabei wird die
regelmäßige/aggregierte Übermittlung von Events (Metrik-Events, z.B.
„Nichterkennungsquote mit grüner LED Gesamtnetz“) sowie die
ereignisgetriebene Übermittlung von Events (z.B. Information-Events wie
„Import neuer Betriebsdaten in QS-Erh ist fehlgeschlagen“) unterschieden. Ein
periodisch übermittelter Healthcheck (spezielles Information-Events) gibt
Auskunft über den Funktionsstatus der QS-AV Applikation.
Event-Daten werden separat in jeder Lastinstanz des Systems QS-AV
generiert, ggf. gesammelt und über die Schnittstelle an MonP übertragen.
Die zu übertragenden Events sind in der SST-Spezifikation 819 aufgelistet und
beschrieben.
Schnittstellenspezifikation
(Referenzdokument)
[8] [S_819_SST_00] SST 819 SST-Spezifikation QS-AV – MonP
Tabelle 107: SST 819 QS-AV – MonP
SST 836
EDM  QS-AV
Metrik-Events
(Kumuliert nach
Zeitscheibe)
Summe Maut OK Summe Externe
Kosten OK
Anzahl tarifierter
Abschnitte
Anzahl nicht
tarifierter
Abschnitte
Fehler-Events
Schnittstellensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 205 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
6.1.6.1 Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation
Abbildung 86: SST 819 - Technische Realisierung Überblick (Information- und Metrik-Events)
Aus der QS-AV Applikation heraus werden Information- und Metrik-Events an MonP übermittelt.
Information-Events, mit Ausnahme des Healthchecks, werden ereignisbasiert durch Bookkeepergesteuerte Jobs erzeugt und in die Warteschlange zum Versand eingereiht (DB-Tabelle
tech_monp_pending_information_event):
Schnittstellensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 206 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
 [DSJ-QSErh-0010] BetriebsdatenRestImporter (siehe Kapitel 5.2.5.1.1):
Neues BD-Release wurde erfolgreich von GDBS heruntergeladen
 [DSJ-QSErh-0210] MonpNeueSollzuordnungenRse (siehe Kapitel 5.2.5.1.24):
Neue RsE-EO/MO-Sollzuordnungen wurden erstellt
Der Healthcheck ist ein spezielles Information-Event. Für den Versand dieser speziellen InformationEvents wird unabhängig vom Bookkeeper beim Start der QS-AV Applikation der HealthCheck in
einem eigenen Thread als PeriodicTask gestartet. Der Periodic Task greift auf das Event-Log des
Bookkeepers zu, um den Applikationsstatus zu ermitteln.
Metrik-Events werden periodisch durch Bookkeeper-gesteuerte Jobs erzeugt und in die
Warteschlange zum Versand eingereiht (DB-Tabelle tech_monp_pending_metric_event):
 [DSJ-QSFzG-0100] MonpFzgEventEmitter (siehe Kapitel 5.2.4.1.10):
Alle FzG-spezifischen Metrik-Events
 [DSJ-QSErh-0200] MonpErhEventEmitter (siehe Kapitel 5.2.5.1.23):
Alle erhebungsspezifischen Metrik-Events
Information- und Metrik-Events (Healthcheck ausgenommen) werden durch den
ScheduledMonpSender an MonP versendet. Der ScheduledMonpSender wird unabhängig vom
Bookkeeper beim Start der QS-AV Applikation in einem eigenen Thread als PeriodicTask gestartet.
Mit Hilfe der Tabelle tech_monp_instanzbezeichner_counts wird je Instanz und Event eine
eindeutige Event-Id geführt und beim Versand übergeben.
Der eigentliche Übertragungszeitpunkt an MonP hängt von der Bereitstellung der Event-Daten
durch die EventEmitter bzw. durch die Jobs für Metrik- und Information-Events im Zusammenspiel
mit dem Ausführungsintervall des ScheduledMonpSender ab. Werden keine Daten in der
Warteschlange bereitgestellt, werden auch keine Events an MonP gesendet (Warteschlange leer). -
Der ScheduledMonpSender entfernt die Elemente der Warteschlange im Zuge seiner Verarbeitung.
Beispiel: Metrik-Eventdaten werden durch den Job MonpFzgEventEmitter alle 15 min (konfigurierbar)
z.B. um 00:00 Uhr und 00:15 Uhr bereitgestellt. Das für den ScheduledMonpSender konfigurierte
Sendeintervall beträgt 5 min, z.B. 00:00 Uhr, 00:05 Uhr, 00:10 Uhr und 00:15 Uhr. Die Übertragung
findet somit nicht alle 5 sondern alle 15 min statt, in diesem Fall um 00:00 Uhr und um 00:15 Uhr.
Grund: Das relativ klein gehaltene Sendeintervall soll verhindern, dass Daten an MonP deutlich zu
spät übertragen werden, was ggf. zur Auslösung eines Alarms durch MonP führt.
Konfiguration
Die Konfiguration der Verbindungseinstellungen erfolgt mittels Umgebungsvariablen.
Umgebungsvariable Beschreibung
MONP_SERVICE_URL Muss gesetzt werden (kein Default). In der Testumgebung z.B.
"https://monp.dmz.e2e.ux.tc.corp:443/v1", in der Produktion
"https://monp.dmz.prod.ux.tc.corp:443/v1". Die Implementierung hängt
automatisch "/events" an.
MONP_IGNORE_CERTIFICATE Default: "false". Falls zu Testzwecken Zertifikate ignoriert werden sollen,
kann dies auf "true" gesetzt werden.
Tabelle 108: SST 819 - Verbindungseinstellungen (Umgebungsvariablen)
Die allgemeine Konfiguration erfolgt in der Cassandra-Tabelle tech_config mittels folgender Keys.
Key Default Beschreibung
erh.monp.information.teilsystemkuerzel QSAV Kurzbezeichnung des Teilsystems, in dem das MonPEvent generiert und verschickt wird.
erh.monp.information.konfigurationskuerzel PROD Kurzbezeichnung der Betriebsdateninstanz.
Schnittstellensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 207 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Key Default Beschreibung
erh.monp.http_connection_timeout 5 Dauer in Sekunden, wie lange beim Verbinden zu
MonP gewartet werden soll bevor ein Timeout erfolgt.
erh.monp.http_request_timeout 5 Dauer in Sekunden, wie lange beim Versenden von
Events auf eine Antwort von MonP gewartet werden
soll, bevor ein Timeout erfolgt.
erh.monp.healthcheck_period 300 Intervall in Sekunden, in dem das HealthCheck
Information-Event durch den Periodic Task
HealthCheck an MonP versendet wird.
erh.monp.event_sender_period 300 Intervall in Sekunden, in dem anstehende Informationund Metrik-Events durch den Periodic Task
ScheduledMonpSender an MonP versendet werden.
Wichtiger Hinweis: Der eigentliche
Übertragungszeitpunkt hängt von der Bereitstellung der
Event-Daten ab, die in einem anderen Zyklus erfolgen
kann. Werden keine Daten bereitgestellt, werden auch
keine Events an MonP gesendet.
Beispiel: Metrik-Eventdaten werden durch den Job
MonpFzgEventEmitter alle 15 min (konfigurierbar) z.B.
um 00:00 Uhr und 00:15 Uhr bereitgestellt. Das hier
konfigurierte Sendeintervall beträgt 5 min, z.B.
00:00Uhr, 00:05 Uhr, 00:10 Uhr und 00:15 Uhr. Die
Übertragung findet somit nicht alle 5 sondern alle 15
min statt, in diesem Fall um 00:00 Uhr und um 00:15
Uhr.
Grund: Das relativ klein gehaltene Sendeintervall soll
verhindern, dass Daten an MonP deutlich zu spät
übertragen werden, was ggf. zur Auslösung eines
Alarms durch MonP führt.
Tabelle 109: SST 819 – Allgemeine Konfiguration in Tabelle tech_config
Schnittstellensicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 208 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
6.1.7 SST 884 QS-AV  Cognos
Beschreibung Die Schnittstelle wird von QS-AV als REST-Webservice zur Verfügung gestellt.
Der Service bietet Cognos Dienste für den lesenden Zugriff auf verdichtete und
bewertete Daten der QS-AV-Applikation gemäß der Schnittstellen-Spezifikation
884. Cognos benötigt diese Daten, um automatisiert und nach internen Regeln
Berichte und Reports zu generieren.
Schnittstellenspezifikation
(Referenzdokument)
[9] [S_884_SST_00] SST 884 SST-Spezifikation QS-AV – Cognos
Tabelle 110: SST 884 QS-AV – Cognos
6.1.7.1 Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation
Für die Schnittstelle 884 werden durch QS-AV Daten bereitgestellt, die mittels REST-Webservice von
Cognos zur Erstellung von Berichten und Reports abgerufen werden können. Der Webservice bietet
mehrere Endpunkte (Services). Für jeden Endpunkt existiert eine eigene Quelltabelle (in der
Abbildung blau eingefärbt), aus der die angefragten Daten gelesen werden. Diese Quelltabelle wird
jeweils durch einen durch den Bookkeeper gesteuerten Job (in der Abbildung grün dargestellt) befüllt,
der die relevanten Daten aus seinen Input-Tabellen des QS-AV Datenbestands ermittelt.
Abbildung 87: SST 884 - Technische Realisierung Überblick Datenbereitstellung für Cognos
Die einzelnen Jobs sind in der Komponentensicht je nach Zugehörigkeit in den Kapiteln 5.2.4 qsfzg: Qualitätssicherung der Fahrzeuggeräte oder 5.2.5 qs-erh: Qualitätssicherung der Erhebung
beschrieben.
6.1.8 SST 838 QS-AV  LogArchiv Server
Bei der Schnittstelle 838 handelt es sich um eine rein technische Schnittstelle zur automatischen
Übertragung der Applikationslog-Dateien an den zentralen LogArchiv Server (Splunk) mittels
Forwarder.
Die globalen Vorgaben zum Logging müssen eingehalten werden. Diese sind im Anhang [10] White
Paper – AppAgile Logging-Format spezifiziert.
Ablaufsicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 209 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
7 Ablaufsicht
Der Programmablauf wird bestimmt durch die beiden Komponenten Bookkeeper (QS-AV Applikation)
und Timebucket-Switcher, die je als Applikationen innerhalb eigenständiger Pods laufen. Die
Datenbereitstellung und Taktgebung des Timebucket-Switchers ist die Voraussetzung für die
Durchführung und Konzertierung der Qualitätssicherungsalgorithmen auf Seiten des Bookkeepers.
Detaillierte Beschreibungen der beiden Applikationen können Sie den Kapiteln
5.1 Schwerpunktkapitel: Bookkeeper (qs-common) und 5.3 Timebucket-Switcher Applikation
entnehmen.
7.1 Timebucket-Switcher-Ablauf
qs-timebucketswitcher
qs-common
qs-boot spark submit MMaaii
nn
LLoonnggRRuunnnneerr
runBlocking() RRuull
eeCCoonnffii
ggTTii
mmeebbuucckkeettss
Cassandra DB TTii
mmeebbuucckkeettSSwwii
ttcchheerreexxtteennddss LLoonnggRRuunnnnaabbllee step() switch TB
use en_q_timbuckets bkpr_eventlog write
TTii
mmeebbuucckkeeSScchheemmaa
TTii
mmeebbuucckkeett
get instance
based on
Abbildung 88: Programmablauf auf Seiten des Timebucket-Switchers (vereinfacht)
Hinweis: In diesem Kapitel werden Grundkenntnisse des Timebucket-Switchers vorausgesetzt.
Weitere Informationen dazu finden Sie im Kapitel 5.3 Timebucket-Switcher Applikation.
Die Timebucket-Switcher Applikation wird lokal auf dem Timbucket-Switcher-Pod gestartet.
Die Klasse Main startet über den LongRunner den TimebucketSwitcher, der zyklisch die
Timebuckets in der Tabelle en_q_timebuckets aktualisiert („switcht“) und die Verarbeitung in der
Tabelle bkpr_eventlog protokolliert.
Der Timebucket-Switcher wird solange ausgeführt, bis die Applikation beendet wird. 
Ablaufsicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 210 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
7.2 Bookkeeper-Ablauf
Die folgende Grafik stellt vereinfacht den Programmablauf dar.
Abbildung 89: Programmablauf auf Seiten des Bookkeepers (vereinfacht)
Hinweis: In diesem Kapitel werden Grundkenntnisse des Bookkeepers vorausgesetzt. Weitere
Informationen dazu finden Sie im Kapitel 5.1 Schwerpunktkapitel: Bookkeeper (qs-common).
Die QS-AV Applikation wird durch spark-submit des qs-boot.jar an den Spark-Kontext
übermittelt und somit zur Ausführung im Cluster gestartet.
Die Klasse Start startet über den LongRunner den JobExecutor, der zyklisch anhand des
Zustands-Logs (bkpr_eventlog) mittels PreconditionChecker gemäß der Konfiguration
(RuleConfig) RuleJobs ausführt, die über das CassandraRuleJobRepository verwaltet
werden. Im Zustands-Log protokolliert der Timebucket-Switcher die Aktualisierung der Timebuckets.
Der Bookkeeper läuft solange keine Abbruchbedingung von außen gesetzt wird.
Die Ablaufsicht aller Jobs inklusive ihrer Namen, Ausführungsintervalle und Abhängigkeiten wird
formatbedingt als digitaler Anhang zur Verfügung gestellt. Siehe Anhang [BK-Graph].
Abbildung 90: Miniaturansicht Bookkeeper Ablaufsicht (Grafik im Anhang [BK-Graph] )
Ablaufsicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 211 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
Beispiel für das Lesen der Grafik: Der MdnParser bezieht seine Daten aus der Tabelle
lds_dm_en_q_mdn (15 Min Partitionen), zerlegt den Base64-kodierten String mit den MonitoringDaten (Payload) und schreibt die Ergebnisse in der Zieltabelle dm_mdn. Die Tabelle dm_mdn fungiert
wiederum als Quelltabelle für die MDN-Normalisierung (MdnNormalizer). Die Ergebnisse der
Normalisierung werden viertelstündlich in die Zieltabelle lds_dm_data_primary_mdn_ergebnis
geschrieben. Für die 24-stündliche FzG-Verdichtung (FzGVerdichtung) werden Daten aus den
Quelltabellen fzg_mdn_normalisierung_ergebnis, lds_dm_em_q_fehler und
fzg_fsm_normalisierung_ergebnis gelesen (jeweils 96 Partitionen à 15 Minuten), korreliert
und verdichtet.
Datenmodell
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 212 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
8 Datenmodell
Das Datenmodell (siehe Referenzdokument [13]) wird zusammen mit den Software-Lieferungen
zur Verfügung gestellt.
Verteilungssicht
A_QS-AV_DS_00_Detailspezifikation_v2.0.docx Seite 213 von 220
Version: 2.0 <vertraulich> Stand: 20.07.2017
9 Verteilungssicht
Aufbau und Verteilung der Systemkomponenten sind im Kapitel 4.2 Technischer Kontext dargestellt
und beschrieben.
Weitere Deployment-spezifische Details werden im Betriebshandbuch (siehe Referenzdokument [17]

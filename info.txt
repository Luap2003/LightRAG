Nachtr√§gliche Anpassungen aus Implementierung f√ºr MSD25:
Grafik √úberblick Software-Komponenten QS-AV Applikation aktualisiert
Jobs dokumentiert:MobaVerdichter. ProcessDSRC15, ProcessDSRC24, FspProcessor, BetriebsdatenRestImporter (ersetzt manuellen Betriebsdatenimport), MoVerdichtungDefaultValueSetter, GenericMetrikVerdichtung
Aktualisierung der Dokumentation s√§mtlicher Jobs (u.a. FsmNormierung) bez√ºglich Aktualit√§t, Format, etc. 
Neustrukturierung des Kapitels Komponentenansicht
Redaktionelle √úberarbeitungen des Kapitels Software-Komponenten (QS-AV-Applikation und Bookkeeper Schwerpunktkapitel)
Redaktionelle √úberarbeitung des Kapitels Ablaufsicht
√úberarbeitung der Bookkeeper-Grafiken im Anhang
Vereinheitlichung von Namen und Bezeichnungen
Verweise auf Confluence bez√ºglich Pseudocodes und Regeln entfernt
Komplette √úberarbeitung der einleitenden Kapitel 1-4 (Anpassungen an ge√§nderte Terminologien, Infrastruktur und Lieferstruktur)
Kapitel 3 Achitekturentscheidungen hinzugef√ºgt, Grobarchitektur aus Technischem Kontext verschoben und integriert
Dokumentation f√ºr die Gesundheitsakte hinzugef√ºgt
1.9
12.07.2017
Alina Kupgisch,
Leyla Sotoudeh
Anpassungen f√ºr MSD26
Neue Jobs:
[DSJ-QSFzG-0080] FzgBewerter
[DSJ-QSFzG-0090] DeklarationEventNormierung
[DSJ-QSFzG-0100] MonpFzgEventEmitter
[DSJ-QSFzG-0110] CognosFzgMiscUpdater 
[DSJ-QSFzG-0120] CognosFzgMapUpdater 
[DSJ-QSFzG-0130] CognosFzgStatusFehlerUpdater 
[DSJ-QSErh-0012] GdbsUploadPump
[DSJ-QSErh-0011] GdbsProductsCreator
[DSJ-QSErh-0110] EobaEventCreator
[DSJ-QSErh-0120] EobaModellBewerter
[DSJ-QSErh-0130] EoVerdichtungDefaultValueSetter
[DSJ-QSErh-0140] EobaVerdichtungJob
[DSJ-QSErh-0150] ModellVerdichterWeekly
[DSJ-QSErh-0160] BetriebsdatenValidierung
[DSJ-QSErh-0170] EobaKreisClusterBuilder
[DSJ-QSErh-0180] EobaTorClusterBuilder
[DSJ-QSErh-0190] EobaLueckenschlussClusterBuilder
[DSJ-QSErh-0200] MonpErhEventEmitter
[DSJ-QSErh-0210] MonpNeueSollzuordnungenRse
[DSJ-QSErh-0220] CognosErhebungsquotenUpdater 
[DSJ-QSErh-0230] CognosDsrcUpdater
[DSJ-QSErh-0240] CognosOnqUpdater 
[DSJ-QSErh-0250] ModellBewertungenRuleJobExecutor
 [DSJ-QSErh-0021] ProcessOnqSollzuordnungJob 
Angepasste Jobs:
[DSJ-QSErh-0010] BetriebsdatenRestImporter
(Grafik angepasst, Produkt Erkennungsobjekte hinzugef√ºgt, Versand von MonP-Info-Event hinzugef√ºgt)
[DSJ-QSErh-0070] MobaVerdichtungJob (Name und Beschreibung angepasst, Verdichtung erh_moba_fzg_verdichtung_stunde wurde entfernt)
[DSJ-QSErh-0090] GenericMetrikVerdichtung
(Hinweis: Bei der Verdichtung werden ausschlie√ülich Datens√§tze mit g√ºltigen BD-Releases und -Instanzen ber√ºcksichtigt. Die von EDM √ºbermittelte Aggregation ‚Äûrestliche‚Äú wird ignoriert.)
[DSJ-QSErh-0020] ProcessONQ15Job (Name und Beschreibung angepasst)
[DSJ-QSErh-0030] ProcessONQ24Job (Name und Beschreibung angepasst)
[DSJ-QSErh-0040] ProcessDSRC15Job (Name und Beschreibung angepasst)
[DSJ-QSErh-0050] ProcessDSRC24Job (Name und Beschreibung angepasst)
Schwellwerte (Kapitel 5.4.1 Soll- und Schwellwerte) erweitert. 
Schnittstellen zu MonP und Cognos hinzugef√ºgt.
1.9
12.07.2017
Leyla Sotoudeh
Freigabe zum Review

Tabelle 2: Versionshistorie

Inhaltsverzeichnis
Dokument - QS Status	2
Versionshistorie	2
Abbildungsverzeichnis	9
Tabellenverzeichnis	12
1	Einleitung	15
1.1	Hintergrund	15
1.2	Wichtige Hinweise	16
1.3	Gegenstand dieses Dokuments	17
1.4	Zielsetzung	17
2	Rahmenbedingungen	18
2.1	Technischen Rahmenbedingungen	18
2.1.1	Hardware	18
2.1.2	Software QS-AV Persistenz: DataStax Enterprise Enterprise	19
2.1.3	Software QS-AV Applikation inklusive Bookkeeper	19
2.1.4	Software Timebucket-Switcher Applikation	24
2.1.5	Software Fahrzeuggesundheitsakte	25
2.1.6	Software Zeppelin	26
2.1.7	Software OpsCenter	27
2.2	Organisatorische Rahmenbedingungen	28
2.2.1	Richtlinien	28
3	Architekturentscheidungen	31
3.1	Ziele	31
3.2	Festlegung	31
3.3	Entscheidungsprozess	32
3.4	Grobarchitektur	36
3.4.1	Datenhaltungsschicht (DSE-Ring)	37
3.4.2	Applikationsschicht (QS-AV Applikation und Timebucket-Switcher)	37
3.4.3	Pr√§sentationsschicht: Fahrzeuggesundheitsakte	38
3.4.4	Pr√§sentationsschicht / Data-Science (Zeppelin)	38
3.4.5	Betriebliche Verwaltung und √úberwachung des DSE-Rings (OpsCenter)	39
3.4.6	Anbindung an Active Directory	39
3.4.7	√úberblick und weiterf√ºhrende Informationen	39
4	Systemarchitektur	41
4.1	Fachlicher Kontext	41
4.1.1	Allgemein	41
4.1.2	Eingabe	42
4.1.3	Verarbeitung	43
4.1.4	Ausgabe	44
4.1.5	Kontext AV	45
4.2	Technischer Kontext	47
4.2.1	√úbersicht	48
4.2.2	DSE-Ring (Datenhaltung)	49
4.2.3	QS-AV Applikation inklusive Bookkeeper (Applikation)	52
4.2.4	Timebucket-Switcher (Applikation)	53
4.2.5	Fahrzeuggesundheitsakte (Pr√§sentation)	54
4.2.6	Zeppelin (Pr√§sentation)	55
4.2.7	OpsCenter (Betriebliche Verwaltung und √úberwachung)	56
4.2.8	Hei√üer DSE-Ring ‚Äì Kalter DSE-Ring	57
5	Komponentensicht	58
5.1	Schwerpunktkapitel: Bookkeeper (qs-common)	58
5.1.1	Hauptmerkmale des Bookkeepers	58
5.1.2	Bookkeeper Glossar	59
5.1.3	√úberblick und allgemeine Funktionsweise	63
5.1.4	Wiederaufsatz und Job-Ausf√ºhrung	66
5.1.5	Konfiguration	68
5.1.6	Zustands-Log	72
5.1.7	Admin-Interface (REST-API)	75
5.1.8	Fehlerbehandlung	76
5.2	QS-AV Applikation	77
5.2.1	qs-parent: Parent-POM (technisch)	78
5.2.2	qs-boot: Hauptapplikation (run)	78
5.2.3	qs-common: Basiskomponenten und Utilities	78
5.2.4	qs-fzg: Qualit√§tssicherung der Fahrzeugger√§te	79
5.2.5	qs-erh: Qualit√§tssicherung der Erhebung	106
5.2.6	qs-common-test: Testspezifische Basiskomponenten und Utilities	168
5.3	Timebucket-Switcher Applikation	169
5.3.1	qs-timebuchetswitcher	169
5.4	DSE-Ring (Datenhaltung)	171
5.4.1	Soll- und Schwellwerte	171
5.5	Fahrzeuggesundheitsakte	182
5.5.1	Aufbau und Funktionsweise	182
6	Schnittstellensicht	187
6.1	Systemschnittstellen	187
6.1.1	√úberblick: Schnittstellen	187
6.1.2	SST 814 Grunddaten Bereitstellungs-Server (GDBS) ü°®ü°™  QS-AV	188
6.1.3	SST 830 Device Management (DM) - QS-AV	189
6.1.4	SST 833 Erkennungs-Service (ES) - QS-AV	190
6.1.5	SST 836 Erhebungs-Daten-Management (EDM) - QS-AV	191
6.1.6	SST 819 QS-AV ü°™ MonP	191
6.1.7	SST 884 QS-AV ü°™ Cognos	194
6.1.8	SST 838 QS-AV ü°™ LogArchiv Server	194
7	Ablaufsicht	195
7.1	Timebucket-Switcher-Ablauf	195
7.2	Bookkeeper-Ablauf	196
8	Datenmodell	198
9	Verteilungssicht	199
10	Konzepte	200
10.1	Parallelisierung und Threading	200
10.2	Logging	200
10.3	Fehlercode- und Fehlerbehandlung	200
10.4	Ablaufsteuerung	200
10.5	Transaktionsbehandlung	200
10.6	Sessionbehandlung	200
10.7	Clustering & Replikation	200
10.8	Benutzeroberfl√§chen	200
10.9	Benutzer- und Rollenkonzept	200
10.10	Datenschutz/Datensicherheit	201
10.11	Hochverf√ºgbarkeit und Skalierbarkeit	201
10.12	Allgemeine Festlegungen	201
10.12.1	G√ºltigkeitsbereiche	201
10.12.2	Format der Betriebsdatenversion	201
11	Referenzdokumente	203
12	Verweise	204
13	Glossar und Abk√ºrzungsverzeichnis	205
14	Anhang	206

Abbildungsverzeichnis

Abbildung 1: System√ºberblick (schematisch)	17
Abbildung 2: Software-Einsatz f√ºr der QS-AV Applikationsschicht	19
Abbildung 3: QS-AV Architekturkomponenten	36
Abbildung 4: System√ºberblick im fachlichen Kontext	41
Abbildung 5: Eingehende Schnittstellen	42
Abbildung 6: Verarbeitungsprinzip	43
Abbildung 7: Ausgehende Schnittstellen	44
Abbildung 8: L√∂sungsskizze Projekt AV	45
Abbildung 9: L√∂sungsskizze Detailansicht QS-AV	46
Abbildung 10: Vereinfachte AV Architektur- und Datenfluss√ºbersicht	47
Abbildung 11: QS-AV Architekturkomponenten	48
Abbildung 12: Aufbau DSE-Pod	49
Abbildung 13: Verteilung im Cluster, schematische Darstellung (DataStax Enterprise - Spark Integration)	51
Abbildung 14: Aufbau Bookkeeper-Pod	52
Abbildung 15: Aufbau TB-Switcher-Pod	53
Abbildung 16: Aufbau Gesundheitsakte-Pod	54
Abbildung 17: Einordnung und Aufbau Zeppelin-Pod	55
Abbildung 18: OpsCenter Server - Schnittstellen und Kommunikationsprotokolle	57
Abbildung 19: √úberblick Bookkeeper (Kontext)	63
Abbildung 20: Bookkeeper √úberblick Detail (Kontext)	65
Abbildung 21: Bookkeeper ‚Äì Klassen und Objekte (Ablauf)	66
Abbildung 22: Detail - JobExecutor Ablauf (stark vereinfacht)	67
Abbildung 23: Bookkeeper ‚Äì Klassen und Objekte (Konfiguration)	68
Abbildung 24: Zustands-Log (Bookkeeper)	72
Abbildung 25: √úberblick Software-Komponenten QS-AV Applikation inkl. Timebucket-Switcher	77
Abbildung 26: Job-Ablauf (schematisch): BatchImportFzgStammdaten	79
Abbildung 27: Job-Ablauf (schematisch): FsmNormierung	81
Abbildung 28: Job-Ablauf (schematisch): MdnParser	83
Abbildung 29: Job-Ablauf (schematisch): MdnNormaliser	84
Abbildung 30: Job-Ablauf (schematisch): FzgVerdichtung	89
Abbildung 31: Job-Ablauf (schematisch): FzGgVerdichtungPopulation	90
Abbildung 32: Job-Ablauf (schematisch): FspProcessor	91
Abbildung 33: Job-Ablauf (schematisch):  FzgBewerter	92
Abbildung 34: Job-Ablauf (schematisch): DeklarationEventsNormierung	95
Abbildung 35: Job-Ablauf (schematisch): MonpFzgEventEmitter	98
Abbildung 36: Job-Ablauf (schematisch): CognosFzgMiscUpdater	100
Abbildung 37: Job-Ablauf (schematisch): CognosFzgMapUpdater	102
Abbildung 38: Job-Ablauf (schematisch): CognosFzgStatusFehlerUpdater	104
Abbildung 39: Job-Ablauf (schematisch): BetriebsdatenRestImporter	107
Abbildung 40: Job-Ablauf (schematisch): GdbsProductsCreator	110
Abbildung 41: Job-Ablauf (schematisch): GdbsUploadPump	113
Abbildung 42: √úberblick ONQ	115
Abbildung 43: Job-Ablauf (schematisch): ProcessONQ15Job	116
Abbildung 44: Job-Ablauf (schematisch): ProcessOnqSollzuordnungJob	118
Abbildung 45: Job-Ablauf (schematisch): ProcessONQ24Job	120
Abbildung 46: √úberblick DSRC-Quotenberechnung	122
Abbildung 47: Job-Ablauf (schematisch): ProcessDSRC15Job	123
Abbildung 48: Job-Ablauf (schematisch): ProcessDSRC24Job	125
Abbildung 49: Job-Ablauf (schematisch): MobaEventCreator	127
Abbildung 50: Job-Ablauf (schematisch): MobaVerdichtungJob	130
Abbildung 51: Job-Ablauf (schematisch): MoVerdichtungDefaultValueSetter	132
Abbildung 52: Job-Ablauf (schematisch): GenericMetrikVerdichtung	134
Abbildung 53: Job-Ablauf (schematisch): ProcessRSEIdKorrektur	135
Abbildung 54: Job-Ablauf (schematisch): EobaEventCreator	137
Abbildung 55: Job-Ablauf (schematisch): EobaModellBewerter	139
Abbildung 56: Job-Ablauf (schematisch): EoVerdichtungDefaultValuesSetter	142
Abbildung 57: Job-Ablauf (schematisch): EobaVerdichtungJob	143
Abbildung 58: Job-Ablauf (schematisch): ModellVerdichterWeekly	145
Abbildung 59: Job-Ablauf (schematisch): BetriebsdatenValidierung	146
Abbildung 60: Job-Ablauf (schematisch): EobaKreisClusterBuilder	149
Abbildung 61: Job-Ablauf (schematisch): EobaTorClusterBuilder	152
Abbildung 62: Job-Ablauf (schematisch): EobaLueckenschlussClusterBuilder	154
Abbildung 63: Job-Ablauf (schematisch): MonpErhEventEmitter	156
Abbildung 64: Job-Ablauf (schematisch): MonpNeueSollzuordnungenRse	158
Abbildung 65: Job-Ablauf (schematisch): CognosErebungsquotenUpdater	160
Abbildung 66: Job-Ablauf (schematisch): CognosOnqUpdater	162
Abbildung 67: Job-Ablauf (schematisch): CognosDsrcUpdater	164
Abbildung 68: Job-Ablauf (schematisch): ModellBewertungenRuleJobExecutor	166
Abbildung 69: Timebucket-Switcher ‚Äì Klassen und Objekte (Ablauf)	169
Abbildung 70: Timebucket-Switcher ‚Äì Klassen und Objekte (Konfiguration)	170
Abbildung 71: Aufbau des Hauptmen√ºs und der statischen Struktur (hervorgehoben)	183
Abbildung 72: Einstieg FzG-Suche	183
Abbildung 73: FzG-Suche - FzG Statusanzeige	184
Abbildung 74: IE - Einstellungen der Kompatibilit√§tsansicht	185
Abbildung 75: IE - Kompatibilit√§tsansicht f√ºr Intranetseiten deaktivieren	186
Abbildung 76: L√∂sungsskizze Detailansicht QS-AV(Datenflussrichtung)	187
Abbildung 77: √úberblick SST 814 GDBS ü°™ QS-AV	188
Abbildung 78: √úberblick SST 814 QS-AV ü°™  GDBS (L√ºckenschlussvorschl√§ge nicht realisiert)	188
Abbildung 79: √úberblick SST 830 DM ü°™ QS-AV	189
Abbildung 80: √úberblick SST 833 ES ü°™ QS-AV	190
Abbildung 81: √úberblick SST 836 ES ü°™ QS-AV	191
Abbildung 82: SST 819 - Technische Realisierung √úberblick (Information- und Metrik-Events)	192
Abbildung 83: SST 884 - Technische Realisierung √úberblick Datenbereitstellung f√ºr Cognos	194
Abbildung 84: Programmablauf auf Seiten des Timebucket-Switchers (vereinfacht)	195
Abbildung 85: Programmablauf auf Seiten des Bookkeepers (vereinfacht)	196
Abbildung 86: Miniaturansicht Bookkeeper Ablaufsicht (Grafik im Anhang [BK-Graph] )	196

Tabellenverzeichnis
Tabelle 1: Dokument- QS Status	2
Tabelle 2: Versionshistorie	5
Tabelle 3: Einsatz von DataStax Enterprise	19
Tabelle 4: Einsatz von DataStax Enterprise	20
Tabelle 5: Einsatz von snappy-java	20
Tabelle 6: Einsatz von Scala	20
Tabelle 7: Einsatz von nscala-time	21
Tabelle 8: Einsatz von play-json	21
Tabelle 9: Einsatz von Jetty	21
Tabelle 10: Einsatz von Java	21
Tabelle 11: Einsatz von Guava	22
Tabelle 12: Einsatz von slf4j-api	22
Tabelle 13: Einsatz von JUnit	22
Tabelle 14: Einsatz von ScalaTest	22
Tabelle 15: Einsatz von spark-testing-base	22
Tabelle 16: Einsatz der H2 Database	23
Tabelle 17: Einsatz von snappy-java	24
Tabelle 18: Einsatz von Scala	24
Tabelle 19: Einsatz von nscala-time	24
Tabelle 20: JRE-Version	24
Tabelle 21: HTTP Server	25
Tabelle 22: Python	25
Tabelle 23: Einsatz von Zeppelin	26
Tabelle 24: Einsatz von DataStax Enterprise	26
Tabelle 25: JRE-Version	26
Tabelle 26: Einsatz von Guava	26
Tabelle 27: Einsatz von slf4j-api	26
Tabelle 28: Einsatz von OpsCenter Server	27
Tabelle 29: JRE-Version	27
Tabelle 30: Teilkomponenten (Pods)	39
Tabelle 31: DSE Analytics Node Aspekte	50
Tabelle 32: Einordnung und Aufbau OpsCenter-Pod	56
Tabelle 33: Bookkeeper RuleSchema-Konfiguration ‚Äì RuleConfig	69
Tabelle 34: Bookkeeper TimebucketSchema-Konfiguration - RuleConfigTimebuckets	71
Tabelle 35: Zustands-Log-Tabelle bkpr_eventlog ‚Äì Typen von Protokolleintr√§gen	73
Tabelle 36: Administrations-API (Bookkeeper) - Service f√ºr die Bedienungsanleitung	75
Tabelle 37: Administrations-API (Bookkeeper) - Service zur Auflistung abgebrochener Jobs	75
Tabelle 38: Administrations-API (Bookkeeper) - Service zum Neustart eines Jobs	75
Tabelle 39: Administrations-API (Bookkeeper) - Service zum Neustart aller Jobs	76
Tabelle 40: BatchImportFzgStammdaten ‚Äì Umgebungsvariablen	80
Tabelle 41: FSMNormierung ‚Äì Konfiguration in Tabelle tech_config	82
Tabelle 42: MdnNormalizer  - Konfiguration in Tabelle tech_rule_schwellwert	88
Tabelle 43: FzgBewerter - Konfiguration in Tabelle tech_rule_schwellwert	94
Tabelle 44: DeklarationEventsNormierung ‚Äì Konfiguration in Tabelle tech_rule_schwellwert	96
Tabelle 45: Input-Tabellen der FzG-spezifischen Metrik-Events	98
Tabelle 46: Output-Tabellen der FzG-spezifischen der Metrik-Events	98
Tabelle 47: MonpErhEventEmitter ‚Äì Konfiguration in Tabelle tech_config	99
Tabelle 48: CognosFzgMiscUpdater ‚Äì Konfiguration in Tabelle tech_config	101
Tabelle 49: CognosFzgMiscUpdater ‚Äì Konfiguration in Tabelle tech_config	103
Tabelle 50: CognosFzgStatusFehlerUpdater ‚Äì Konfiguration in Tabelle tech_config	104
Tabelle 51: BetriebsdatenRestImporter ‚Äì Konfiguration der Umgebungsvariablen	108
Tabelle 52: GdbsProductsCreator ‚Äì Konfiguration in Tabelle tech_config	111
Tabelle 53: GdbsUploadPump ‚Äì Konfiguration in Tabelle tech_config	114
Tabelle 54: ProcessONQ15  - Konfiguration in Tabelle tech_rule_schwellwert	117
Tabelle 55: ProcessOnqSollzuordnungJob  - Konfiguration in Tabelle tech_rule_schwellwert	119
Tabelle 56: ProcessONQ24Job  - Konfiguration in Tabelle tech_rule_schwellwert	121
Tabelle 57: ProcessDSRC15Job - Konfiguration in Tabelle tech_rule_schwellwert	124
Tabelle 58: ProcessONQ24  - Konfiguration in Tabelle tech_rule_schwellwert	126
Tabelle 59: MobaEventCreator - Konfiguration in Tabelle tech_rule_schwellwert	128
Tabelle 60: MobaEventCreator - Konfiguration in Tabelle tech_config	128
Tabelle 61: MobaVerdichtungJob - Konfiguration in Tabelle tech_rule_schwellwert	131
Tabelle 62: MobaVerdichtungJob ‚Äì Konfiguration in Tabelle tech_config	131
Tabelle 63: ProcessRSEIdKorrektur - Konfiguration in Tabelle tech_rule_schwellwert	136
Tabelle 64: EobaEventCreator - Konfiguration in Tabelle tech_rule_schwellwert	138
Tabelle 65: EobaModellBewerter - Konfiguration in Tabelle tech_rule_schwellwert	140
Tabelle 66: EobaVerdichtungJob - Konfiguration in Tabelle tech_rule_schwellwert	144
Tabelle 67: BetriebsdatenValidierung - Konfiguration in Tabelle tech_rule_schwellwert	148
Tabelle 68: BetriebsdatenValidierung ‚Äì Konfiguration in Tabelle tech_config	148
Tabelle 69: EobaKreisClusterBuilder ‚Äì Konfiguration in Tabelle erh_eoba_clustering	151
Tabelle 70: Input-Tabellen der erhebungsspezifischen Metrik-Events	156
Tabelle 71: Output-Tabellen der der erhebungsspezifischen Metrik-Events	156
Tabelle 72: MonpErhEventEmitter ‚Äì Konfiguration in Tabelle tech_config	157
Tabelle 73: CognosErhebungsquotenUpdater ‚Äì Konfiguration in Tabelle tech_config	161
Tabelle 74: CognosOnqUpdater ‚Äì Konfiguration in Tabelle tech_config	163
Tabelle 75: CognosDsrcUpdater ‚Äì Konfiguration in Tabelle tech_config	165
Tabelle 76: ModellBewertungenRuleJobExecutor ‚Äì Konfiguration in Tabelle tech_rule_Schwellwert	167
Tabelle 77: Soll- und Schwellwerte f√ºr FzG (MDN-Normierung)	175
Tabelle 78: Soll- und Schwellwerte f√ºr FzG-Bewertung	176
Tabelle 79: Soll- und Schwellwerte f√ºr Eventnormierung	176
Tabelle 80: Soll- und Schwellwerte f√ºr ONQ/DSRC	177
Tabelle 81: Soll- und Schwellwerte f√ºr MOBA	178
Tabelle 82: Soll- und Schwellwerte f√ºr EOBA	178
Tabelle 83: Soll- und Schwellwerte f√ºr Modellbewertung	179
Tabelle 84: Soll- und Schwellwerte f√ºr Betriebsdatenvalidierung	180
Tabelle 85: Soll- und Schwellwerte f√ºr EOBAClustering	181
Tabelle 86: √úberblick: Schnittstellen in MSD26	187
Tabelle 87: SST 814 GDBS ü°™ QS-AV	188
Tabelle 88: SST 830 DM - QS-AV	189
Tabelle 89: SST 833 ES - QS-AV	190
Tabelle 90: SST 836 EDM - QS-AV	191
Tabelle 91: SST 819 QS-AV ‚Äì MonP	191
Tabelle 92: SST 819 - Verbindungseinstellungen (Umgebungsvariablen)	193
Tabelle 93: SST 819 ‚Äì Allgemeine Konfiguration in Tabelle tech_config	193
Tabelle 94: SST 884 QS-AV ‚Äì Cognos	194
Einleitung
Hintergrund
Die deutliche Ausweitung des mautpflichtigen Streckennetzes auf Bundesstra√üen ist mit der vorhandenen Fahrzeugger√§teflotte (insbesondere der Altger√§te) und der bislang dezentralen Erkennungs- und Tarifierungslogik aufgrund von Speicherrestriktionen nicht umsetzbar. Die Verlagerung der Erkennungs- und Tarifierungslogik und damit auch der umfangreicheren Betriebsdaten in zentrale Systeme wird damit notwendig. 
Aufgrund des gesteigerten Umfangs des Erkennungsmodells ist es zudem nicht mehr m√∂glich, die bisherige Praxis der reinen manuellen Modellierung aufrecht zu erhalten. Fehler im teilautomatisch generierten Modell m√ºssen erkannt werden und es muss durch die nachfolgende Anwendung statistischer Methoden kontinuierlich optimiert werden. 
Im Zuge der Zentralisierung der Erkennung soll die Qualit√§tssicherung des Automatischen Verfahrens ebenfalls zentralisiert werden.
Im Projekt QS-AV wurden die Komponenten zur automatischen Qualit√§tssicherung realisiert. Gegenstand dieses Dokuments ist die Detailspezifikation dieser Komponenten, die im Folgenden nach Schichten kategorisiert aufgef√ºhrt werden: 
Applikation 
Die QS-AV Applikation mit den Teilkomponenten:
QS-FzG (Qualit√§tssicherung der Fahrzeugger√§te) 
QS-Erh (Qualit√§tssicherung der Erhebung)
Persistenz
Verteilte Datenhaltung in einem DataStax Enterprise Cluster basierend auf der Cassandra Datenbank.
Pr√§sentation
Komponenten zur Darstellung und Interaktion:
Fahrzeuggesundheitsakte (Subkomponente von QS-FzG) zum Abruf von Fahrzeugger√§teinformationen
Zeppelin zum Zugriff auf die QS-AV Persistenz via Web-Notebooks zu Analysezwecken (Apache Zeppelin)
Betriebliche Verwaltung und √úberwachung 
OpsCenter zum betrieblichen Monitoring und der Verwaltung des DataStax Enterprise Clusters
Wichtige Hinweise
Die fachlichen Anforderungen, sowie die fachlichen Klassendiagramme und fundierten Hintergr√ºnde wurden und werden konsistent √ºber den gesamten Entwicklungszyklus im [3] Lastenheft QS-FzG und Lastenheft QS-Erhebung dokumentiert und aktualisiert.
Sofern f√ºr das Verst√§ndnis der Detailspezifikation notwendig, werden komplette Passagen aus dem Lastenheft [3] zitiert, in der Regel jedoch nur referenziert. Anforderungen aus dem Lastenheft werden mittels der darin festgelegten Nomenklatur (LH-QSAV-xxxx) referenziert. 
Die Kl√§rungshistorie zur konkreten Umsetzung der Anforderungen inklusive der beteiligten Ansprechpartner aus Fachbereich und Entwicklung k√∂nnen den entsprechenden User Stories in JIRA entnommen werden.
Hinweis: Im Dokumentenverlauf werden wichtige Hinweise gerahmt und mit einem Ausrufezeichen markiert hervorgehoben.

Weiterf√ºhrende Informationen und wichtige Dokumentenverweise werden gerahmt und mit einer B√ºroklammer markiert hervorgehoben.

 Wichtige Informationen und Definitionen werden gerahmt und mit einem Informationssymbol markiert hervorgehoben.

 Trigger: Informationen zu zeitgesteuerten Jobs und Trigger (Bookkeeper) werden gerahmt und mit einer Uhr markiert hervorgehoben.

Gegenstand dieses Dokuments
Die Detailspezifikation zur Qualit√§tssicherung Automatisches Verfahren (QS-AV) vermittelt dem Leser die technischen Details zur √úberwachung und Qualit√§tssicherung der Mauterhebung. 

Abbildung 1: System√ºberblick (schematisch)
Zielsetzung
Ziele und Anforderungen entnehmen Sie bitte dem Referenzdokument [3] Lastenheft QS-FzG und Lastenheft QS-Erhebung.
Rahmenbedingungen
Technischen Rahmenbedingungen
Hardware
Deployment und Betrieb erfolgen in einer AppAgile Umgebung, basierend auf der Cloud Computing Plattform OpenShift (Red Hat Linux). Alle Software-Komponenten werden in Docker-Images integriert und zusammen mit der jeweils zugeh√∂rigen Deployment-Konfiguration ausgeliefert. Die entsprechenden Docker-Container werden als Pods auf AppAgile-Knoten deployed und gestartet.
Alle Details zur Hardware-Umgebung inklusive der technischen Voraussetzungen finden Sie in den Referenzdokumenten [17] [A_QSAV_HBB_00] QS-AV Betriebshandbuch und [16] [A_QSAV_HBI_00] QS-AV Installationshandbuch.
Software QS-AV Persistenz: DataStax Enterprise Enterprise
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen f√ºr die QS-AV Persistenzschicht aufgelistet.
DataStax Enterprise
Bezeichnung
DataStax Analytics Node / DataStax Agent
Version
5.0.4 / 6.0.8
Einsatz
Zentrale Datenhaltung f√ºr QS-AV
DataStax Analytics Node unter Einsatz von Cassandra 3.0.9 
Agent
Datastax Agent in der Version 6.0.8 zur Kommunikation mit dem OpsCenter
Detailinformationen zu DataStax Enterprise 5.0.4 entnehmen Sie der Produktdokumentation.

Tabelle 3: Einsatz von DataStax Enterprise
Software QS-AV Applikation inklusive Bookkeeper
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen f√ºr die QS-AV Applikation gemeinsam f√ºr die Teilkomponenten QS-FzG und QS-Erh aufgelistet.

Abbildung 2: Software-Einsatz f√ºr der QS-AV Applikationsschicht
DataStax Enterprise
Bezeichnung
DataStax Enterprise (DSE)
Teilkomponente/n
QS-FzG / QS-Erh 
Version
5.0.4
Einsatz
Wichtig: Ruhende Installation zum Zugriff auf den DSE-Ring (QS-AV Datenhaltung). Der Bookkeeper-Pod ist nicht als Teil des DSE-Rings konfiguriert.
Zugriff auf DSE Cassandra DB (lesend / schreibend)
Einsatz der Analytics-Komponente: 
Spark 1.6.2 (spark-core, spark-sql, spark-hive, spark-streaming, spark-test-tags, spark-network-common) unter Nutzung von DataFrames und RDDs
Spark Cassandra Connector 1.6.2 (f√ºr Scala und Java), Cassandra Treiber Version 3.0.3
Detailinformationen zu DataStax Enterprise 5.0.4 entnehmen Sie der Produktdokumentation.

Tabelle 4: Einsatz von DataStax Enterprise
snappy-java
Bezeichnung
snappy-java
Teilkomponente/n
QS-FzG / QS-Erh
Version
1.1.2.6
Einsatz
qs-common (Bibliothek)
Die Bibliothek wird zur optionalen Kompression von Requests und Responses auf Transport-Ebene f√ºr den Cassandra Treiber eingesetzt.

Tabelle 5: Einsatz von snappy-java
Scala
Bezeichnung
Scala (scala-library, scala-compiler)
Teilkomponente/n
QS-FzG / QS-Erh
Version
2.10.5 
Einsatz
Programmierung der Anwendungslogik 
Einsatz u.a. aufgrund der Kompatibilit√§t und Performance-Vorteile mit dem Spark Framework Layer

Tabelle 6: Einsatz von Scala
nscala-time
Bezeichnung
nscala-time
Teilkomponente/n
QS-FzG / QS-Erh
Version
2.12.0
Einsatz
qs-common (Bibliothek)
Scala Wrapper f√ºr Joda Time, Nutzung als Bibliothek f√ºr Datums- und Zeitberechnungen

Tabelle 7: Einsatz von nscala-time
play-json
Bezeichnung
play-json
Teilkomponente/n
QS-FzG / QS-Erh
Version
2.4.8
Einsatz
qs-common 
Scala JSON Bibliothek

Tabelle 8: Einsatz von play-json
Jetty
Bezeichnung
jetty-all-server
Teilkomponente/n
QS-Commons 
Version
8.1.14.v20131031
Einsatz
Webserver
Zur Exposition von REST-Services

Tabelle 9: Einsatz von Jetty
Java
Bezeichnung
Java (JDK und JRE)
Teilkomponente/n
QS-FzG / QS-Erh
Version
1.8.0_u102
Einsatz
Laufzeitumgebung (JRE)
Ben√∂tigt f√ºr die Laufzeitumgebung (JVM)
Entwicklung (JDK)
Zum Einsatz in der Entwicklung (im Zusammenspiel mit Scala)
Unit-Testing (JUnit)
F√ºr in Java geschriebene Unit-Tests

Tabelle 10: Einsatz von Java
Google Guava
Bezeichnung
guava
Teilkomponente/n
QS-FzG / QS-Erh
Version
19.0
Einsatz
Allgemein
Erweiternde Java-Bibliothek

Tabelle 11: Einsatz von Guava
slf4j-api
Bezeichnung
slf4j-api
Teilkomponente/n
QS-FzG / QS-Erh 
Version
1.7.12
Einsatz
Logging
Logging API

Tabelle 12: Einsatz von slf4j-api
JUnit 
Bezeichnung
JUnit
Teilkomponente/n
QS-FzG / QS-Erh 
Version
4.12
Einsatz
Unit-Testing
F√ºr in Java geschriebene Unit-Tests.

Tabelle 13: Einsatz von JUnit
ScalaTest 
Bezeichnung
ScalaTest
Teilkomponente/n
QS-FzG / QS-Erh 
Version
2.2.5
Einsatz
Testing
F√ºr in Scala geschriebene Tests.

Tabelle 14: Einsatz von ScalaTest
spark-testing-base
Bezeichnung
spark-testing-base
Teilkomponente/n
QS-FzG / QS-Erh
Version
1.6.2_0.4.5
Einsatz
Testing
Basisklassen f√ºr Spark-Tests

Tabelle 15: Einsatz von spark-testing-base
H2 Database
Bezeichnung
h2
Teilkomponente/n
QS-FzG / QS-Erh 
Version
1.4.192
Einsatz
Testing
Java SQL-Datenbank zur Testunterst√ºtzung

Tabelle 16: Einsatz der H2 Database
Software Timebucket-Switcher Applikation
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen f√ºr den Timebucket-Switcher aufgelistet.
snappy-java
Bezeichnung
snappy-java
Version
1.1.2.6
Einsatz
qs-common (Bibliothek)
Die Bibliothek wird zur optionalen Kompression von Requests und Responses auf Transport-Ebene f√ºr den Cassandra Treiber eingesetzt.

Tabelle 17: Einsatz von snappy-java
Scala
Bezeichnung
Scala (scala-library, scala-compiler)
Version
2.10.5 
Einsatz
Programmierung der Anwendungslogik 

Tabelle 18: Einsatz von Scala
nscala-time
Bezeichnung
nscala-time
Version
2.12.0
Einsatz
qs-common (Bibliothek)
Scala Wrapper f√ºr Joda Time, Nutzung als Bibliothek f√ºr Datums- und Zeitberechnungen

Tabelle 19: Einsatz von nscala-time
Java (JRE)
Bezeichnung
Java (JRE)
Version
1.8.0_u102
Einsatz
F√ºr den TB-Switcher ben√∂tigte Java Version (JRE)

Tabelle 20: JRE-Version
Software Fahrzeuggesundheitsakte
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen f√ºr die Fahrzeuggesundheitsakte aufgelistet.
HTTP Server
Bezeichnung
Apache HTTP Server
Version
2.4.25
Einsatz
HTTP Server f√ºr die Web-Anwendung

Tabelle 21: HTTP Server
Python
Bezeichnung
Python
Version
2.7.6
Einsatz
Programmierung der Anwendungslogik, inklusive:
Python CGI (aktuellste Version)
Python_Sqlite 3.2.6
PySpark 1.6.8
Python_Cassandra 3.7.0
Python_CQL 1.4.0
Python_Base64 (aktuellste Version)
Python_Numpy 1.8.2
Python_Matplotlib 1.3.1

Tabelle 22: Python
Software Zeppelin
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen f√ºr die Zeppelin-Installation aufgelistet.
Zeppelin
Bezeichnung
Zeppelin
Version
0.7.0
Einsatz
Apache Zeppelin Installation
Benutzter Built-In Interpreter: Spark-Interpreter und Cassandra-Interpreter

Tabelle 23: Einsatz von Zeppelin
DataStax Enterprise
Bezeichnung
DataStax Enterprise (DSE)
Version
5.0.4
Einsatz
Wichtig: Ruhende Installation zum Zugriff auf den DSE-Ring (QS-AV Datenhaltung). Der Zeppelin-Pod ist nicht als Teil des DSE-Rings konfiguriert.
Detailinformationen zu DataStax Enterprise 5.0.4 entnehmen Sie der Produktdokumentation.

Tabelle 24: Einsatz von DataStax Enterprise
Java (JRE)
Bezeichnung
Java (JRE)
Version
1.8.0_u102
Einsatz
F√ºr Apache Zeppelin ben√∂tigte Java Version (JRE)

Tabelle 25: JRE-Version
Google Guava
Bezeichnung
guava
Version
19.0
Einsatz
Allgemein erweiternde Java-Bibliothek

Tabelle 26: Einsatz von Guava
slf4j-api
Bezeichnung
slf4j-api
Version
1.7.12
Einsatz
Logging API

Tabelle 27: Einsatz von slf4j-api
Software OpsCenter
In diesem Abschnitt wird die genutzte Software inklusive Versionen und Einsatzbereichen f√ºr die OpsCenter-Installation aufgelistet.
OpsCenter Server
Bezeichnung
OpsCenter Server
Version
6.0.8
Einsatz
OpsCenter Server zur Kommunikation mit dem DSE-Ring (Monitoring und Verwaltung)

Tabelle 28: Einsatz von OpsCenter Server
Java (JRE)
Bezeichnung
Java (JRE)
Version
1.8.0_u102
Einsatz
F√ºr OpsCenter ben√∂tigte Java Version (JRE)

Tabelle 29: JRE-Version

Organisatorische Rahmenbedingungen
Richtlinien
Vorgaben zum Logging 
Siehe [10] White Paper ‚Äì AppAgile Logging-Format.
Entwicklerrichtlinien
Entwicklungsrichtlinien AppAgile: [11] Entwicklungsrichtlinien AppAgile ‚Äì Technische Dokumentation
Scala Style Guide: https://github.com/databricks/scala-style-guide
Spark Guide: http://www.cloudera.com/documentation/enterprise/5-6-x/PDF/cloudera-spark.pdf 
Benennung der Cassandra-Tabellen
Zur Vereinheitlichung der Tabellennamen wird folgendes Schema f√ºr die Tabellennamen angewandt:
Schema
Systempr√§fix ‚Äì Tabellennamen optional inkl. fachlichem Pr√§fix ‚Äì optional: Suffix f√ºr Art der Daten
Aufbau des Tabellennamens
[ fzg | erh | bkpr | tech | lds ] _tabellenname_ ( [ mvw | tmp | prv ] )
Man beachte die korrespondierenden Farben zwischen dem Pattern und zum Aufbau des Tabellennamens selbst. In blau ist hier der Systempr√§fix, in violett der Tabellenname selbst und in gr√ºn der optionale Suffix abgebildet.
Der Tabellenname wird je Wortbedeutung in Lowercase-Camelcase geschrieben (wortF√ºrWortBeginneKlein) und diverse Wortbedeutungen werden jeweils durch einen Unterstrich getrennt (z.B: fzg_fachlicheBedeutung_by_ehkid). Wie im Code sollte auch hier die Grundsprache Englisch sein mit jeweils eingedeutschten Fachbezeichnungen.
In LDS-Tabellen, beginnend mit dem "lds_"-Pr√§fix sind die Prim√§rdaten aus den Eingangsschnittstellen zu finden. Alle anderen Sekund√§rdaten, welche erzeugt werden, sind in anderen Tabellen zu finden. Diese unterteilen sich in fachliche sowie technische Daten.
Systempr√§fix
Fachliche Daten sind zu finden in ‚Äì nach Systempr√§fix plus deren vorrangiger, hierarchischer Trennung:
fzg ‚Äì f√ºr alle Daten, welche zu QS-FzG geh√∂ren
erh ‚Äì f√ºr alle Daten, welche zu QS-Erh geh√∂ren
lds ‚Äì f√ºr alle Prim√§rdaten aus den LDS-Systemen (Schnittstellentabellen)
Technische Daten sind zu finden in ‚Äì nach Systempr√§fix plus deren vorrangiger, hierarchischer Trennung:
bkpr ‚Äì f√ºr alle Daten, welche dem Bookkeeper geh√∂ren
tech ‚Äì f√ºr alle anderen technischen Daten, wie tempor√§re und Staging-Tabellen
Tabellenname in der Mitte
Der Tabellenname in der Mitte kann ein fachliches oder technisches inneres Pr√§fix enthalten ‚Äì je nach Tabellenart. Die Beispiele unten zeigen die Verwendung interner Pr√§fixe.
optionaler Suffix
Das Suffix ist jeweils optional. Die beiden vorgesehenen Suffixe sind:
mvw ‚Äì ein Suffix f√ºr eine abgeleitete Materialized View aus einer anderen Tabelle
tmp ‚Äì ein Suffix f√ºr eine Tabelle, welche lediglich zur Laufzeit konsistent und bef√ºllt sein muss
prv ‚Äì Tabellen mit Zwischenergebnissen, welche aus technischen Gr√ºnden vorgehalten werden m√ºssen
Beispiele
Beispiele QS-FzG
fzg_someItems
fzg_verdichtung_by_ehkid
fzg_someOtherItems
Beispiele QS-Erh
erh_gdbs_grunddaten
erh_mo_listOfMos 
erh_mo_listOfMos_by_state
Beispiele Mischung
fzg_verdichtung_by_ehkid
erh_gdbs_grunddaten
erh_mo_listOfMos 
erh_mo_listOfMos_by_state
bkpr_persistedStates
bkpr_otherData
tech_stagingTable_ruleAbc
tech_otherData
lds_dm_useCaseA
lds_es_useCaseA
lds_es_useCaseC
lds_edm_useCaseB

Pattern f√ºr Schwellwert- & Konfigurationstabelle
Die Tabellen f√ºr fachliche Schwellwerte und technische Konfigurationen sind:
tech_rule_schwellwert
tech_config
Die in beiden Tabellen befindlichen Keys sind mit einem Pr√§fix zu versehen, wie die Tabellennamen der Cassandra-Tabellen selbst auch. Auf diese Art sollen √§hnliche und zusammengeh√∂rige Werte schneller gefunden werden k√∂nnen. Das Pr√§fix soll vom fachlichen Modul oder aus Fachbezeichnern abgeleitet werden. Die Keys der Tabellen sollen in beiden Tabellen:
komplett in Kleinbuchstaben geschrieben sein,
mit einem Punkt getrennt werden,
ein f√ºhrendes Systempr√§fix enthalten und
danach die inneren Pr√§fixe vom Allgemeinen ins Spezielle aufweisen.
Schwellwertschl√ºssel
Zugelassene Systempr√§fixe f√ºr Schl√ºssel in Schwellwerttabellen:
qserh
qsfzg 
Beispiele f√ºr Keys der fachlichen Schwellwerte in tech_rule_schwellwert:
qserh.moba.betrachtungszeitraum.enddate
qserh.onq.nummernkreis.oben
qserh.moba.anothervalue
qserh.eo.somevalue
qsfzg.fsm.somevalue
Konfigurationsschl√ºssel
Zugelassene Systempr√§fixe f√ºr Schl√ºssel in Konfigurationstabellen:
qserh
qsfzg
bkpr
tech
Beispiele f√ºr technische Konfigurationen in tech_config:
qsfzg.sw.aktuelleversion
bkpr.timebucket.fsm
bkpr.timebucket.mdn
bkpr.timebucket.befahrungsparameter
tech.configurationvalue
Architekturentscheidungen
In diesem Kapitel sind die Architekturentscheidungen unter Angabe der Ziele und externen Festlegungen f√ºr QS-AV beschrieben. Im Anschluss wird die daraus resultierende Grobarchitektur dargestellt und begr√ºndet.
Hintergr√ºnde zu den Architekturentscheidungen aus der Projektplanungsphase finden Sie im Referenzdokument [15] QS-AV ‚Äì Entscheidungsempfehlung zur technischen Architektur (Capgemini). Die folgenden Inhalte innerhalb dieses Kapitels basieren auf der Entscheidungsempfehlung und f√ºhren diese weiter.
Ziele
In diesem Abschnitt werden die wichtigsten Ziele aufgelistet, die bei der Architekturbewertung ber√ºcksichtigt wurden.
Kompatibilit√§t der Betriebsinfrastruktur
Die Zielarchitektur soll zur bestehenden Betriebsinfrastruktur (HSR2 oder AppAgile) kompatibel sein.
Einfache Betreibbarkeit 
Die Zielarchitektur soll m√∂glichst einfach betreibbar sein. Das bedeutet, dass Aufrechterhaltung und √Ñnderung der L√∂sung im Betrieb mit m√∂glichst geringem Aufwand durchgef√ºhrt werden k√∂nnen.
Herstellersupport 3 Jahre+
F√ºr eingesetzte Standardkomponenten soll ein Enterprise- und Long-Term-Support von mindestens drei Jahren angeboten werden.
Geeignet f√ºr Mengenger√ºst
Bei der Auswahl der Zielarchitektur muss das ‚Äì durch die Aggregation von Daten aus allen Systemen des Leistungsdatenstroms  vergleichsweise gro√üe ‚Äì Mengenger√ºst ber√ºcksichtigt werden.
Ausreichende Performance
Bei der Auswahl der Zielarchitektur muss die Komplexit√§t der Korrelationen innerhalb der Qualit√§tssicherungsalgorithmen hinsichtlich der Performance ber√ºcksichtigt werden.
Nutzung als Data-Science-Platform
Die L√∂sung muss eine Data-Science-Platform f√ºr explorative Datenanalysen (Fachbereich) bieten.
Geringer Entwicklungsaufwand
Der Entwicklungsaufwand muss m√∂glichst gering gehalten werden.
Einhaltung der Datenschutzvorgaben
Das hohe Datenschutzniveau muss bei der Auswahl der Architekturkomponenten ber√ºcksichtigt werden.
Festlegung
Folgende Festlegung wurde im Projektverlauf von au√üen an das Projekt getragen:
Betriebsinfrastruktur: AppAgile 
Die QS-AV-Systeme sollen in der AppAgile-Umgebung betrieben werden.
Entscheidungsprozess
Schritt 1: Identifizierung von L√∂sungskandidaten 
Folgende L√∂sungskandidaten f√ºr die Datenhaltung wurden anhand der obigen Ziele und Festlegungen identifiziert und evaluiert:
Kurzname                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 
L√∂sungskomponente
Evaluierte Version
Splunk
Splunk inklusive Prediction App
6.3.3 / 1.0
Oracle
Oracle
12c
DSE
DataStax Enterprise
4.8
Hadoop
Cloudera Hadoop Distribution (CDH)
5.5.2

Hinweis: Splunk wurde u.a. aus folgenden Gr√ºnden bereits vor der Detailbewertung aus der Auswahl f√ºr QS-AV entfernt: F√ºr die Prediction App wird kein Enterprise-Support angeboten, die Verdichtungen und Korrelationen sind nicht trivial durchzuf√ºhren und zu persistieren, der Prediction App fehlten zudem notwendige Algorithmen.
Schritt 2: Bewertung nach Betriebskriterien
ü°´ Kriterium / L√∂sung ü°™
Oracle
DSE
Hadoop
Kompatibilit√§t zur bestehenden Infrastruktur
++
+
+
Einfacher Betrieb
+
++
-
Long-Term-Support f√ºr mind. 3 Jahre
++
0
--
Third-Level-Support vorhanden
++
++
++
Belastbarkeit des Third-Level-Supports
++
+
+
Verf√ºgbarkeit von Sicherheitsupdates / Patches
++
++
++
Zukunftssicherheit nach Marktanalysten
++
+
-
Punkte
13
9
2

Skala: ++=2 Punkte, +=1 Punkt, 0=0 Punkte, -=-1Punkt, --=-2 Punkte
Schritt 3: Bewertung nach Data-Science-Kriterien
ü°´ Kriterium / L√∂sung ü°™
Oracle
DSE
Hadoop
Eignung f√ºr QS-AV Aufgaben im Allgemeinen
-
++
++
Wertsch√∂pfungsh√∂he f√ºr die Verarbeitung von Geodaten
++
++
-
Eignung f√ºr Zeitreihen
-
++
--
M√§chtigkeit des Analysestacks und Verf√ºgbarkeit von SQL und Flexibilit√§t
0
++
++
Punkte
0
8
1

Skala: ++=2 Punkte, +=1 Punkt, 0=0 Punkte, -=-1Punkt, --=-2 Punkte
Schritt 4: Bewertung nach Entwicklungskriterien
ü°´ Kriterium / L√∂sung ü°™
Oracle
DSE
Hadoop
Entwicklungsaufwand
++
++
+
Hohes Datenschutzniveau
++
++
0
Last & Performance
--
++
+
Wissensbasis, Erfahrung und Referenzprojekte
++
0
0
Flexibilit√§t bzgl. zuk√ºnftiger Anforderungen
+
++
++
Punkte
5
8
4

Skala: ++=2 Punkte, +=1 Punkt, 0=0 Punkte, -=-1Punkt, --=-2 Punkte
Schritt 5: Bewertung nach weiteren technischen Kriterien
ü°´ Kriterium / L√∂sung ü°™
Oracle
DSE
Hadoop
Durchgehende Verf√ºgbarkeit (HA)
0
++
++
Transaktionssicherheit
++
+
-
Verf√ºgbarkeit OR-Mapper
++
-
-
Verf√ºgbarkeit Queues
++
0
++
Vollindizierung auf Dokumentenbasis
--
++
++
Punkte
4
4
4

Skala: ++=2 Punkte, +=1 Punkt, 0=0 Punkte, -=-1Punkt, --=-2 Punkte
Schritt 4: Summieren der Punkte
ü°´ Bereich / L√∂sung ü°™
Oracle
DSE
Hadoop
Betriebskriterien
13
9
2
Data-Science-Kriterien
0
8
1
Entwicklungskriterien
5
8
4
Weitere technische Kriterien
4
4
4
Gesamtpunkte
22
29
11

Schritt 5: Ber√ºcksichtigung des Gartner-Quadranten (ODBMS 2015)

Allgemein: Oracle und DataStax werden als Leader kategorisiert, Oracle im Marktf√ºhrungsbereich.
Analyse des 3-Jahrestrends: Durch immer st√§rkere Marktdurchdringung gibt es ausschlie√ülich bei DataStax eine steigende Tendenz.
Zusammenfassung der Bewertung
Oracle scheidet durch Datenmenge und die Performance bei deren Verarbeitung aus. 
Oracle kann schnell flie√üende Daten (Velocity) voraussichtlich nicht bew√§ltigen.
Oracle kann die Datenmengen (Volume) voraussichtlich nicht bew√§ltigen.
QS-AV kann hinsichtlich Datenmenge und Performance sowohl mit DSE als auch Hadoop realisiert werden.
DSE ist im Vergleich die dominante L√∂sung in allen untersuchten Bereichen.
DSE ist im Vergleich zu Hadoop einfacher zu betreiben und verf√ºgt √ºber einen schlankeren Technologiestack.
Fazit: Realisierung mit DataStax Enterprise auf der AppAgile-Plattform. Als Programmiersprache wurde Scala aufgrund der Performance-Benchmarks gew√§hlt. Das Spark-Framework bietet performanten Zugriff auf die Datenhaltung und erm√∂glicht die Nutzung sowohl von DataFrames (API) als auch SQL-Statements.
Zus√§tzlich wurde Apache Zeppelin als Frontend zum Zugriff auf die Datenhaltung gew√§hlt. Gr√ºnde daf√ºr sind die Verf√ºgbarkeit zahlreicher Interpreter (z.B. Spark, Cassandra, R) zum Zugriff auf die Datenhaltung sowie die M√∂glichkeit unterschiedlicher Visualisierungen und einer LDAP-Anbindung.
Im folgenden Kapitel wird die daraus resultierende Grobarchitektur dargestellt und die Aufteilung der Teilkomponenten begr√ºndet.
Grobarchitektur
Dieser Abschnitt gibt einen √úberblick √ºber die Komponenten von QS-AV. Weiterf√ºhrende Informationen zu den einzelnen Teilkomponenten finden Sie im Kapitel 4.2 Technischer Kontext.

Abbildung 3: QS-AV Architekturkomponenten
Allgemein: Alle oben dargestellten QS-AV-Teilkomponenten werden in AppAgile als Pods (Docker Container) betrieben.
Anmerkungen zur Plattformentscheidung AppAgile und Auswirkungen
Der Fokus des Platform-as-a-Service-Angebots von AppAgile liegt auf dem Betrieb von stateless Cloud-Diensten mit persistenten Features. Um die vom Hersteller genannten Vorteile der Plattform, wie Skalierbarkeit, Schnelligkeit und Flexibilit√§t optimal nutzen zu k√∂nnen, empfiehlt sich der Einsatz von weitestgehend statuslosen Microservice-Architekturen. Die persistenten Features bestehen kurz zusammengefasst aus transparent nutzbarerem NFS-Storage f√ºr die Persistenz, sowie dem Support ausgew√§hlter Datenbankprodukte (u.a. Hadoop, mongoDB, PostgreSQL und standalone Cassandra).
Hinweis: Das f√ºr die Realisierung von QS-AV ausgew√§hlte Produkt DataStax Enterprise basiert vereinfacht auf dem Prinzip, dass Datenbankabfragen durch einen optimierten DataStax-eigenen Cassandra-Treiber im Zusammenspiel mit dem Spark-Framework verteilt im Datenbank-Cluster ausgef√ºhrt werden. Die Abfragen werden dabei automatisch so zerlegt und auf die einzelnen Knoten im Cluster verteilt, dass Teilergebnisdaten parallel und gezielt auf denjenigen Knoten, auf denen die betreffenden Partitionen tats√§chlich gespeichert sind, ermittelt werden. Die Teilergebnisse werden weitestgehend vorgefiltert und transformiert, bevor sie im letzten Schritt zusammengef√ºhrt werden und das Gesamtergebnis der Abfrage ermittelt wird.
Im Zusammenspiel von DataStax Enterprise mit AppAgile als Plattform ergeben sich folgende Spannungsfelder:
Die beiden Kernkomponenten der DSE Analytics Nodes  - Cassandra DB und der Spark-Context - k√∂nnen nicht auf verschiedene Pods verteilt werden (Mischung von stateless und stateful Services = Microservices Anti-Pattern). 
DataStax als Hersteller empfiehlt allgemein, einen Baremetal-Betrieb virtualisierten L√∂sungen vorzuziehen.
Um die Performance-Vorteile der verteilten Verarbeitung effizient sch√∂pfen zu k√∂nnen, m√ºssen die den einzelnen Knoten im Cluster zugeordneten Datenpartitionen f√ºr diese lokal zugreifbar sein. Anstatt der standardm√§√üig in der AppAgile-Plattform eingesetzten NFS-Storages wird die Anbindung lokaler SSDs von DataStax dringend empfohlen.
Bei einem Review der QS-AV Architektur im Januar 2017 wurde unter Einbeziehung des Teilprojekts Transition und BzS aus Gr√ºnden der Vereinheitlichung im Projekt MaB40K AppAgile als Plattform f√ºr den Betrieb von QS-AV festgelegt. Die f√ºr QS-AV gew√§hlte Architektur wurde auf Basis dieser Entscheidung adaptiert und umgesetzt.   
Datenhaltungsschicht (DSE-Ring)
Die QS-AV Datenhaltung und die Verarbeitung der Datenbankoperationen erfolgt verteilt in einem DataStax Analytics Cluster (DSE-Ring), basierend auf DataStax Enterprise. Der DSE-Ring besteht softwareseitig aus n DataStax Analytics Nodes, die via Konfiguration verbunden werden. Bestandteile dieser sind u.a. Cassandra als verteiltes Datenbanksystem, sowie das Apache Spark Framework f√ºr performante Datenbankoperationen im Cluster. 
Die DataStax Agent Software wird eingesetzt, um die Kommunikation mit dem DataStax OpsCenter (s.u.) zur betrieblichen Verwaltung und √úberwachung zu erm√∂glichen. 
Die DataStax Analytics Nodes werden zusammen mit dem DataStax Agent in einem Docker Container ausgeliefert und als Pods betrieben (‚ÄûDSE-Pods‚Äú).
Vorteile der Architektur
Infrastrukturelle Trennung der Datenhaltungsschicht von Applikations- und Pr√§sentationsschicht
Einfach erweiterbares Datenbankcluster mit verteiltem Analyse-Framework mit folgenden Vorteilen: Einfach skalierbar, hoch verf√ºgbar, performant auch bei gro√üem Datenvolumen
Datenbank-Upgrades und ‚ÄìPatches sind im laufenden Betrieb m√∂glich (Rolling Upgrade)
Applikationsschicht (QS-AV Applikation und Timebucket-Switcher)
Die propriet√§re QS-AV Applikation enth√§lt die spezifische Gesch√§ftslogik zur Qualit√§tssicherung des Automatischen Verfahrens. Sie wird auf einem eigenen Pod ausgeliefert und gestartet (‚ÄûBookkeeper-Pod‚Äú) und an den Spark Context des DSE-Rings gebunden. Die spezifischen Datenbankanfragen k√∂nnen so durch das Spark Framework performanceoptimiert im Cluster verteilt abgearbeitet werden. Die QS-AV Applikation ist in zwei Teilkomponenten aufgeteilt: QS-FzG (Qualit√§tssicherung der Fahrzeugger√§te) und QS-Erh (Qualit√§tssicherung der Erhebung).
Beim Timebucket-Switcher handelt es sich um eine propriet√§re  Applikation, die folgende Aufgaben erf√ºllt: Der Timebucket-Switcher generiert 1. in konfigurierbaren Abst√§nden neue zeitbasierte Partition Keys (Timebuckets) f√ºr Cassandra Tabellen und fungiert 2. als Taktgeber f√ºr die Abarbeitung der Jobs innerhalb der QS-AV Applikation, die durch das √Ñndern der Timebuckets getriggert werden. 
Die Schnittstelle zwischen Timebucket-Switcher und QS-AV Applikation ist die gemeinsame Datenhaltung (DSE-Ring). Der Timebucket-Switcher schreibt direkt per CQL in die Cassandra-Datenbank, die QS-AV Applikation greift via Spark zu (Schnittstellentabellen). 
Vorteile der Architektur
Infrastrukturelle Trennung der Applikationsschicht von Datenhaltungs- und Pr√§sentationsschicht
Die Datenbankanfragen der QS-AV Applikation werden im Cluster verteilt durch das Spark Framework optimiert ausgef√ºhrt. Dies erm√∂glicht hohe Performanzwerte bei gleichzeitig niedrigem Netzwerk-Traffic. 
Applikations-Updates erfordern keine Downtime der Persistenz (im Regelfall).
Die Trennung der schlanken Timebucket-Switcher Applikation von der QS-AV Applikation soll gew√§hrleisten, dass Partitionen innerhalb der Cassandra DB auch bei einem Ausfall oder einer St√∂rung der QS-AV Applikation nicht √ºberlaufen k√∂nnen und die Taktung aufrechterhalten bleibt.
Ein Upgrade der QS-AV Applikation beschr√§nkt sich auf das Deployment des Bookkeeper-Pods (insbesondere f√ºr Regel√§nderungen und ggf. verk√ºrzte Release-Prozesse relevant). 
Pr√§sentationsschicht: Fahrzeuggesundheitsakte
Mit der Fahrzeuggesundheitsakte wird eine Web-Applikation zum einfachen und schnellen Abruf von Informationen zu einem Fahrzeugger√§t zur Verf√ºgung gestellt.  ist die Fahrzeuggesundheitsakte als Subkomponente (UI) von QS-FzG eingeordnet. Die Authentifizierung und Autorisierung erfolgen gegen das TC-Active Directory anhand der darin verwalteten Benutzer und Gruppen. Die Fahrzeuggesundheitsakte wird in einem Docker Container ausgeliefert und im ‚ÄûFahrzeuggesundheitsakte-Pod‚Äú betrieben.
Vorteile der Architektur
Infrastrukturelle Trennung der Pr√§sentationsschicht von Applikations- und Datenhaltungsschicht
Zentrale Berechtigungsverwaltung
Ein Upgrade der Fahrzeuggesundheitsakte-Applikation beschr√§nkt sich auf das Deployment des Gesundheitsakte-Pods.
Pr√§sentationsschicht / Data-Science (Zeppelin)
Mit Apache-Zeppelin werden Web-Notebooks mit Zugriff auf die Daten im DSE-Ring zu Analyse- und Reporting-Zwecken f√ºr die Fachbereiche zur Verf√ºgung gestellt. Mitarbeiter des Regelbetriebs erhalten zus√§tzlich die Berechtigung, technische Konfigurationen und Schwellwerte anzupassen. Die Authentifizierung und Autorisierung erfolgen unter Verwendung des Shiro-Frameworks gegen das TC-Active Directory anhand der darin verwalteten Benutzer und Gruppen. Zeppelin wird in einem Docker Container inklusive vorkonfigurierter Spark- und Cassandra-Interpreter ausgeliefert und im ‚ÄûZeppelin-Pod‚Äú betrieben.
Vorteile der Architektur
Infrastrukturelle Trennung der Pr√§sentationsschicht von Applikations- und Datenhaltungsschicht
Zentrale Berechtigungsverwaltung
Die Datenbankanfragen, die via Spark-Interpreter abgesetzt werden, werden im Cluster verteilt und optimiert ausgef√ºhrt. Dies erm√∂glicht hohe Performanz bei gleichzeitig niedrigem Netzwerk-Traffic. 
Ein Upgrade der Zeppelin-Version beschr√§nkt sich auf das Deployment des Zeppelin-Pods. 
Betriebliche Verwaltung und √úberwachung des DSE-Rings (OpsCenter)
Mit dem DataStax OpsCenter wird eine Web-UI mit Funktionalit√§ten zur Verwaltung und √úberwachung  des DSE-Rings im Betrieb zur Verf√ºgung gestellt. Das DataStax OpsCenter kommuniziert mit Hilfe der DataStax Agents mit den DSE Analytics Nodes. Dabei werden u.a. Informationen zur Ermittlung des Cluster-Gesamtstatus √ºbertragen.  Bei Bedarf k√∂nnen Konfigurationsparameter im laufenden Betrieb angepasst und im Cluster propagiert werden. Des Weiteren stehen u.a. Cluster-Repair-Funktionalit√§ten sowie √úbersichten zu Auslastungen und Hot-Spots zur Verf√ºgung. Der DataStax OpsCenter Server wird in einem eigenen Docker Container ausgeliefert und im ‚ÄûOpsCenter-Pod‚Äú betrieben.
Vorteile der Architektur
Betriebliches Tool vom Hersteller zur √úberwachung des Cluster-Status inklusive Repair-Funktionalit√§ten
Cluster-Optimierung im laufenden Betrieb m√∂glich
Schlankes Kommunikationsprotokoll
Anbindung an Active Directory 
Zur Authentifizierung und Autorisierung von Benutzern und technischen Benutzern werden die QS-AV Komponenten an das TC-Active Directory angebunden.
Vorteile 
Nutzung der zentralen IAM IT-Infrastruktur
Nutzung der zentralen Benutzerverwaltungs- und Berechtigungsprozesse
Berechtigungen k√∂nnen zentral auditiert werden
√úberblick und weiterf√ºhrende Informationen
Die einzelnen Teilkomponenten/Pods werden in der folgenden Tabelle aufgelistet. Weiterf√ºhrende Informationen finden Sie in den referenzierten Kapiteln.
Komponente
Kurzbeschreibung
Anzahl
Kapitel
DSE-Pod
n DSE-Pods bilden den DSE-Ring
n
4.2.2
OpsCenter-Pod
Web-UI zur √úberwachung und Verwaltung des DSE-Rings
1
4.2.7
Bookkeeper-Pod
Enth√§lt die Software-Komponenten mit der  QS-AV-spezifischen Applikationslogik (QS-AV Applikationsschicht)
1
4.2.3
Timebucket-Switcher-Pod
Enth√§lt mit dem Timebucket-Switcher den Generator f√ºr Partition Keys (Timebuckets) und Taktgeber f√ºr die Abarbeitung der QS-AV Applikationslogik
1
4.2.4
Gesundheitsakte-Pod
Stellt eine Web-UI zum berechtigungsgesteuerten Abruf von FzG-Informationen zur Verf√ºgung
1
4.2.5
Zeppelin-Pod
Stellt berechtigungsgesteuerten Spark-Zugriff auf den DSE-Ring f√ºr Web-Notebooks zur Verf√ºgung.
1
4.2.6

Tabelle 30: Teilkomponenten (Pods)
DataStax Enterprise bietet mit Cassandra allgemeine DB-Clusterfunktionalit√§ten, wie z.B. automatische Replikation (gem√§√ü Replikationsfaktor), Lastverteilung, Datenverteilung, Einhaltung der Konsistenzlevel. Die Konfiguration dieser Features erfolgt im Rahmen der Konfiguration der einzelnen Knoten. In einigen F√§llen (z.B. Consistency Level) k√∂nnen die konfigurierten Werte f√ºr einzelne Requests von der Applikation √ºberschrieben werden.
Spark-Requests, die √ºber die QS-AV Applikation an das DB-Cluster gestellt werden, werden ebenfalls √ºber das Cluster verteilt.

Weiterf√ºhrende Informationen zu DataStax Enterprise sowie AppAgile entnehmen Sie bitte der jeweiligen Produktdokumentation. 
Details zu Einrichtung und Konfiguration werden in den Installations- und Betriebshandb√ºchern dokumentiert.
Systemarchitektur
In diesem Kapitel wird die QS AV-Systemarchitektur untergliedert nach fachlichem und technischem Kontext beschrieben.
Fachlicher Kontext
Der an dieser Stelle gebotene √úberblick stellt die Top-Level-Beschreibung der realisierten Systeme im fachlichen Kontext dar. Er dient als Grundlage f√ºr das weitere Verst√§ndnis der Detailspezifikation. Die detaillierte fachliche Spezifikation entnehmen Sie dem  [3] Lastenheft QS-FzG und Lastenheft QS-Erhebung.
Allgemein

Die Qualit√§tssicherung des Automatischen Verfahrens basiert kurz zusammengefasst auf der Verarbeitung und regelbasierten Verarbeitung und Bewertung von eingehenden Betriebs- und Leistungsdaten parallel zum Leistungsdatenstrom. 

Abbildung 4: System√ºberblick im fachlichen Kontext 
Eingabe
Die Eingabedaten (Betriebs- und Leistungsdaten) werden via Schnittstellen von folgenden Systemen zur Verf√ºgung gestellt und in der QS-AV Datenhaltung gespeichert:
GDBS     Betriebsdaten (Grunddaten)
DM
ES           Leistungsdaten (Daten aus dem Leistungsdatenstrom)
EDM


Abbildung 5: Eingehende Schnittstellen
Weiterf√ºhrende Informationen zu den Schnittstellen finden Sie im Kapitel 6 Schnittstellensicht und den entsprechenden Schnittstellenspezifikationen (siehe Referenzdokumente).
Verarbeitung
Auf Basis der Eingabedaten werden die Qualit√§tssicherungsalgorithmen in Form von Regeln angewendet, getrennt nach:
QS-FzG: Fahrzeugger√§tspezifische Qualit√§tssicherung
QS-Erh: Erhebungsspezifische Qualit√§tssicherung
Die Verarbeitung innerhalb der QS-FzG und der QS-Erh verl√§uft dabei nach folgendem Schema:



Abbildung 6: Verarbeitungsprinzip
LH-QSAV-2335
Die vom Leistungsdatenstrom eintreffenden Informationen werden in dem Prozessschritt "Normierung" insoweit vorbereitet, dass die nachfolgenden Schritte auf m√∂glichst einheitlichen Strukturen erfolgen k√∂nnen. Sowohl QS-Erh  als auch QS-FzG  generieren in der Normierung Events, die erste Indizien f√ºr m√∂gliche Erkennungsmodell- oder FzG-Fehler darstellen und noch keine konkreten Fehler aufzeigen. 
In der "Korrelation" werden Ereignisse (z.B. Befahrungen und RsE-Kontakte), die zun√§chst voneinander unabh√§ngig erscheinen, miteinander in Beziehung gesetzt. Durchfahrungen in MOBA und EOBA werden zu unterschiedlichen Zeitpunkten im Leistungsstrom bearbeitet, an QS-Erh  √ºbermittelt. Eine Identifikation von Befahrungsauff√§lligkeiten und die Korrelation der zu unterschiedlichen Zeitpunkten in der QS AV eintreffenden Events erfolgt an dieser Stelle. 
Die in dem vorangegangenen Schritt korrelierten Ereignisse werden in den Verdichtungsprozessen nach Zeitscheiben angeordnet. Diese Indizien werden erst in einem weiteren Schritt zu Events verdichtet, die Auff√§lligkeiten beschreiben. Dieser Schritt ist in der "Verdichtung" beinhaltet. Dort werden aktuelle Befahrungen und statistische Ergebnisse mit historisierten Daten und/oder (System-)Statusinformationen der FzG-Plattform korreliert, zu konkreten Events im Erkennungsmodell oder der FzG verdichtet und bei entsprechender Bewertung an die Monitoring-Plattform (MonP) oder das Incident Management (IM) eskaliert.
Auf Basis der Ergebnisse der Verdichtung erfolgt die "Bewertung". 
Ausgabe
Die Ergebnisse der Qualit√§tssicherung, Monitoring- und Reporting-Informationen werden f√ºr folgende Systeme zur Verf√ºgung gestellt:
GDBS 
MonP 
Cognos 
Die System-Logs werden zentral archiviert:
LogArchiv Server

Abbildung 7: Ausgehende Schnittstellen
Weiterf√ºhrende Informationen zu den Schnittstellen finden Sie im Kapitel 6 Schnittstellensicht und den entsprechenden Schnittstellenspezifikationen (siehe Referenzdokumente).
Kontext AV
Die folgende L√∂sungsskizze des Architekturmanagements zeigt die Einordnung der QS AV-Komponenten im √ºbergeordneten Kontext des AV-Projekts.

Abbildung 8: L√∂sungsskizze Projekt AV
In der folgenden Abbildung ist der QS-AV-relevante Ausschnitt im L√∂sungskontext vergr√∂√üert dargestellt. 

Abbildung 9: L√∂sungsskizze Detailansicht QS-AV
Weiterf√ºhrende Informationen zu den Schnittstellen finden Sie im Kapitel 6 Schnittstellensicht und den entsprechenden Schnittstellenspezifikationen (siehe Referenzdokumente).


Technischer Kontext
Die folgende Abbildung stellt die vereinfachte AV Architektur- und Datenfluss√ºbersicht dar.

Abbildung 10: Vereinfachte AV Architektur- und Datenfluss√ºbersicht
In der Abbildung 10: Vereinfachte AV Architektur- und Datenfluss√ºbersicht sind im oberen Bereich die QS-AV-spezifischen Komponenten vereinfacht dargestellt. 
Darunter sind von links nach rechts in Datenflussrichtung die Komponenten des Leistungsdatenstroms dargestellt, bestehend aus DM, ES und EDM. Die Darstellung der Schnittstellen erfolgt durch Pfeile, wobei der Leistungsdatenstrom durch rote Pfeile und Hilfsdatenstr√∂me durch blaue Pfeile visualisiert werden (nach Datenflussrichtung).
√úbersicht

Abbildung 11: QS-AV Architekturkomponenten
In den folgenden Unterkapiteln wird die Architektur der einzelnen Komponenten im Detail dargestellt.
DSE-Ring (Datenhaltung)
Der DSE-Ring besteht aus 1-n durch Konfiguration verbundene DSE-Pods.

Abbildung 12: Aufbau DSE-Pod
AppAgile Node
Alle DSE-Pods laufen auf AppAgile Worker-Nodes als Docker Container. Der AppAgile Worker-Node stellt u.a. die Docker Engine und ein gemountetes Dateisystem (NFS oder SSD) f√ºr die Persistenz zur Verf√ºgung. 
Docker Container
Die DSE-Pods enthalten die DataStax Komponenten DSE Analytics Node (inkl. Cassandra DB) und DataStax Agent. Im Rahmen der Installation werden die DSE-Pods so konfiguriert, dass sie einen DSE-Ring bilden und via DataStax Agent mit dem OpsCenter-Server kommunizieren k√∂nnen. 
Details zur Konfiguration entnehmen Sie bitte dem Referenzdokument [16] [A_QSAV_HBI_00] QS-AV Installationshandbuch.
DSE Analytics Node
Die DSE Analytics Nodes bestehen aus folgenden Komponenten:
Spark Worker
Spark Master 
(hier startet der Spark-Driver-Prozess)
Cassandra File System (CFS)
DataStax Enterprise File System (DSEFS)
Cassandra DB
Die DSE Analytics Nodes k√∂nnen in unterschiedlicher Basiskonfiguration laufen:
Aspekt
Modus
Initiale Verteilung
Beschreibung
Cluster-Topologie
Seed oder Non-Seed
Seed: zus√§tzlich in einem DSE-Pod
Non-Seed: in allen DSE- Pods
Seed: DSE Analytics Node, der f√ºr das Bootstrapping neuer Nodes im Cluster verantwortlich ist. 
Wichtig: Der Ausfall des Seeds hat keinen Einfluss auf die Funktionsf√§higkeit des Clusters an sich, sondern lediglich auf die Erweiterbarkeit des Clusters im Ausfallzeitraum (Bootstrapping).
Spark
Master oder Worker
Master: zus√§tzlich in einem DSE-Pod
Worker: in allen DSE-Pods
Ein Spark Master 
(Failover-sicher durch Session Replication)
Master = Der DSE Analytics Node, auf dem der Spark Master l√§uft.  Der Spark Master verteilt die Spark-Applikationsanfragen auf die Spark Worker im Cluster. Der Spark-Master kann im Cluster wandern.

Tabelle 31: DSE Analytics Node Aspekte
Hintergrund: DSE Analytics Nodes k√∂nnen als reine Spark Worker und einer im Cluster zus√§tzlich als Spark Master laufen. Der Spark Master verteilt die Spark-Applikationsanfragen auf die Spark Worker im Cluster. Die Spark Worker sind f√ºr die eigentliche Verarbeitung bzw. die Transaktionen mit der Cassandra-Datenbank verantwortlich. Die folgende Abbildung stellt dies schematisch dar. 

Abbildung 13: Verteilung im Cluster, schematische Darstellung (DataStax Enterprise - Spark Integration)
DataStax Agent
Der DataStax Agent kommuniziert mit dem DSE Analytics Node innerhalb des gleichen Pods via CQL (native Zugriffe auf die Cassandra DB) sowie via JMX (JVM Monitoring). Nach au√üen kommuniziert der DataStax Agent via STOMP, CQL und JMX mit dem OpsCenter-Server. Der Agent √ºbermittelt nicht nur selbstst√§ndig Statusinformationen, sondern reagiert auch auf konkrete Anfragen und Befehle, die vom OpsCenter-Server gesendet werden.
QS-AV Applikation inklusive Bookkeeper (Applikation)

Abbildung 14: Aufbau Bookkeeper-Pod
AppAgile Node
Der Bookkeeper-Pod l√§uft auf einem AppAgile Worker-Node als Docker Container. 
Docker Container
Die im Bookkeeper-Pod enthaltenen Komponenten werden im Folgenden beschrieben. 
QS-AV Schema
Enth√§lt die DDLs f√ºr die aktuell eingesetzte Version QS-AV Applikation (zum Aufsetzen/Patchen der Datenbank).
QS-AV Applikation
Die eigentliche Gesch√§ftslogik von QS-FzG und QS-Erh ist in einer propriet√§ren Spark-Applikation, der ‚ÄûQS-AV Applikation‚Äú, implementiert. Sie wird mit allen Dependencies zur Verf√ºgung gestellt. Gestartet wird die Applikation durch √úbermitteln der Applikation an den Spark-Kontext des DSE-Clusters mittels dse spark-submit. 
Mit dem √úbermitteln wird die QS-AV Applikation in die Laufzeitumgebungen von Spark-Master und Spark-Workern zur Ausf√ºhrung im Cluster √ºbergeben. 
Weiterf√ºhrende Informationen zur QS-AV Applikation und dem Bookkeeper finden Sie in den Kapiteln 5.2 QS-AV Applikation und 5.1 Schwerpunktkapitel: Bookkeeper (qs-common).  
DSE Analytics Node (Library)
Der Bookkeeper-Pod-spezifische Docker Container enth√§lt neben der propriet√§ren Applikationskomponenten auch eine Distribution des DataStax Enterprise Analytics Node zur Nutzung als Bibliothek. Wichtig: Der DataStax Analytics Node wird auf dem Bookkeeper-Pod nicht als Teil des DSE-Rings konfiguriert und auch nicht gestartet.
Timebucket-Switcher (Applikation)

Abbildung 15: Aufbau TB-Switcher-Pod
AppAgile Node
Der TB-Switcher-Pod l√§uft auf einem AppAgile Worker-Node als Docker Container. 
Docker Container: Timebucket-Switcher Applikation
Die standalone Scala-Applikation Timebucket-Switcher enth√§lt die Logik zur automatischen Generierung zeitbasierter Partition Keys (Timebuckets) f√ºr die Cassandra Tabellen innerhalb des DSE-Rings und fungiert als Taktgeber f√ºr die Abarbeitung der Jobs innerhalb der QS-AV Applikation, die durch das √Ñndern der Timebuckets getriggert werden. 
Die Schnittstelle zwischen Timebucket-Switcher und QS-AV Applikation ist die gemeinsame Datenhaltung (DSE-Ring). Der Timebucket-Switcher schreibt direkt per CQL in die Cassandra-Datenbank des DSE-Rings. 
Weiterf√ºhrende Informationen zum Timebucket-Switcher finden Sie im Kapitel 5.3 Timebucket-Switcher Applikation.
Fahrzeuggesundheitsakte (Pr√§sentation)

Abbildung 16: Aufbau Gesundheitsakte-Pod
AppAgile Node
Der Gesundheitsakte-Pod l√§uft auf einem AppAgile Worker-Node als Docker Container. 
Docker Container: Web-Applikation und HTTP-Server
Die auf Python/CGI basierende Web-Applikation ‚ÄûFahrzeuggesundheitsakte‚Äú wird inklusive ben√∂tigter Drittanbieter-Bibliotheken auf dem Gesundheitsakte-Pod ausgeliefert. Die Web-Applikation wird mit Hilfe eines vorkonfigurierten Apache HTTP-Servers zur Verf√ºgung gestellt. Authentifizierung und Autorisierung  erfolgen konfigurierbar per LDAP gegen Active Directory.
Weiterf√ºhrende Informationen zur Fahrzeuggesundheitsakte finden Sie im Kapitel 5.5 Fahrzeuggesundheitsakte.

Zeppelin (Pr√§sentation)
Apache Zeppelin bietet autorisierten Benutzern aus den Fachbereichen Web-Notebooks mit Zugriff auf den DSE-Ring zu Analyse- und Auswertungszwecken.

Abbildung 17: Einordnung und Aufbau Zeppelin-Pod
AppAgile Node
Der Zeppelin-Pod l√§uft auf einem AppAgile Node als Docker. Der AppAgile Node stellt u.a. die Docker Engine und ein gemountetes Dateisystem f√ºr allgemeine Konfigurationsdaten und die Web-Notebooks, die au√üerhalb des Containers gesichert werden sollen, zur Verf√ºgung. 
Authentifizierung und Autorisierung  erfolgen zweistufig:
Zwischen Benutzer und Zeppelin: Konfigurierbar per LDAP gegen Active Directory gem√§√ü Rollen- und Rechtekonzept
Zwischen Zeppelin und dem DSE-Ring (Cassandra): Konfigurierbar durch Zeppelin-Administratoren innerhalb der Interpreter anhand technischer Cassandra-Benutzer gem√§√ü Rollen- und Rechtekonzept
Docker Container: Apache Spark, Zeppelin
Der Zeppelin-spezifische Docker Container enth√§lt folgende Komponenten:
Distribution des DataStax Enterprise Analytic Nodes zur Nutzung als Bibliothek. 
Wichtig: Der DataStax Analytics Node wird auf dem Bookkeeper-Pod nicht als Teil des DSE-Rings konfiguriert und auch nicht gestartet.
Apache Zeppelin-Installation, u.a. inklusive:
Jetty (Webserver)
Spark- und Cassandra-Interpreter
Gemountetes Dateisystem zur persistenten Speicherung von Konfigurationseinstellungen und Web-Notebooks
OpsCenter (Betriebliche Verwaltung und √úberwachung)
Das DataStax OpsCenter bietet autorisierten Benutzern ein Web-UI zur Wartung und zum Monitoring des DSE-Rings im Betrieb.
Hinweis: Benutzer- und Zugriffsrechte werden direkt im OpsCenter konfiguriert. 

Tabelle 32: Einordnung und Aufbau OpsCenter-Pod
AppAgile Node
Der OpsCenter-Pod l√§uft auf einem AppAgile Node als Docker Container. 
Docker Container: DataStax OpsCenter
Der OpsCenter-spezifische Docker Container enth√§lt folgende Komponenten:
DataStax OpsCenter Server (konfiguriert zum Zugriff auf den DSE-Ring)
OpsCenter Daemon (Interaktion mit dem Cassandra Cluster √ºber die auf den DSE Analytics Nodes installierten DataStax Agents, exponiert Management- und Monitoring- Funktionalit√§ten per API f√ºr das Web-UI)
HTTP-Server (Bereitstellung des Web-UI)
Folgende Protokolle werden zur Kommunikation mit dem DSE-Ring eingesetzt: STOMP, JMX, CQL 
Die folgende Grafik visualisiert die Schnittstellen und Kommunikationsprotokolle des OpsCenter Servers:

Abbildung 18: OpsCenter Server - Schnittstellen und Kommunikationsprotokolle
Hei√üer DSE-Ring ‚Äì Kalter DSE-Ring
Hinweis: Konzepte, die neben dem ersten DSE-Ring f√ºr OLTP-Workloads (‚Äûhei√üer Ring‚Äú), wie oben beschrieben, den Aufbau und Betrieb eines zweiten DSE-Rings f√ºr OLAP-artige Queries (‚Äûkalter Ring‚Äú) beinhalten, wurden bislang nicht verabschiedet / umgesetzt. 
Hintergrund: Ansatz eines Kalten-Ring-Konzepts ist es, den hei√üen Ring f√ºr alle schreibenden Operationen im Regelbetrieb (OLTP) sowie als Basis f√ºr die QS-AV Applikation und ihre umfangreichen Qualit√§tssicherungsalgorithmen vorzubehalten. 
F√ºr alle OLAP-artigen Workloads auf den Daten in der Persistenzschicht (wie z.B. FB-Analysen via Zeppelin, Einzelfallrecherchen, Problem- und Incident-Management, Erzeugung von Reports) k√∂nnte ein zweiter ‚Äûkalter‚Äú Ring mit den replizierten Daten des hei√üen Rings aufgebaut werden. Die Replikation der Daten darf dabei nur unidirektional vom hei√üen Ring in Richtung des kalten Rings erfolgen.
Komponentensicht
In diesem Kapitel werden die Softwarekomponenten und Funktionen der propriet√§ren Komponenten
QS-AV Applikation inklusive Bookkeeper
Fahrzeuggesundheitsakte
und
Timebucket-Switcher
beschrieben.
Hinweis: Einen √úberblick √ºber die Komponenten finden Sie in den Kapiteln 3.4 Grobarchitektur und 4 Systemarchitektur (Unterkapitel 4.2 Technischer Kontext). 
Schwerpunktkapitel: Bookkeeper (qs-common)
Der Bookkeeper ist eine Subkomponente der QS-AV Applikation (Maven-Projekt: qs-common). Aufgrund seiner zentralen Aufgabe als Steuerkomponente f√ºr den Programmablauf und zum besseren Verst√§ndnis der Bez√ºge in den Folgekapiteln wird dieses Schwerpunktkapitel vorangestellt.
 Die Qualit√§tssicherungsalgorithmen der QS-AV Applikation basieren auf der mehrstufigen Transformation von Eingangsdaten aus dem Leistungsdatenstrom: Die Daten werden in der QS-AV Applikation normiert, korreliert, verdichtet und bewertet. W√§hrend der Korrelierungs-, Verdichtungs- und Bewertungsoperationen m√ºssen Daten aus unterschiedlichen Datenquellen miteinander in Beziehung gebracht und ausgewertet werden. Die Ergebnisse der Verarbeitungsoperationen werden in Zieltabellen und √ºber ausgehende Schnittstellen (Ausgangsdatenstr√∂men) zur Verf√ºgung gestellt. 
Die Bookkeeper-Komponente ist innerhalb der QS-AV Applikation f√ºr die Ablaufsteuerung zust√§ndig. Die Verarbeitung erfolgt innerhalb sogenannter RuleJobs (im weiteren Verlauf auch kurz als Job bezeichnet), f√ºr die sowohl inhaltliche als auch zeitliche Abh√§ngigkeiten konfiguriert werden k√∂nnen. 
Hauptmerkmale des Bookkeepers
Implementierung in Scala (100%)
Verwendung von UTC 
Single-Threaded (sequenzielle Ausf√ºhrung der Jobs)
Trennung der Steuerungslogik von der Verarbeitungslogik
Konfigurative Ablaufsteuerung
Intervallverarbeitung von nach Zeitscheiben partitionierten Daten aus Cassandra-Tabellen (=Timebuckets): Konfiguration der Abh√§ngigkeiten von Quelltabelle/n ü°™ Verarbeitung (RuleJobs) ü°™ Zieltabelle/n
Signalgesteuerte Verarbeitung: Konfiguration von Signal-Events als Vor- oder Nachbedingungen
Zeitlich gesteuerte Verarbeitung: Konfiguration von (fr√ºhesten) Startzeiten
Informationen zur Konfiguration entnehmen Sie dem Kapitel 5.1.5 Konfiguration
RuleJobRepository mit Wiederaufsatzfunktion nach Applikationsneustart
Bookkeeper Glossar
In diesem Abschnitt werden die wichtigsten Begriffe und Konzepte des Bookkeepers erl√§utert. 
Lesehinweis: Das alphabetisch sortierte Glossar kann im Lesefluss zun√§chst √ºberbl√§ttert werden und zum besseren Verst√§ndnis der folgenden Kapitel immer wieder herangezogen werden.
Anchor-Timebucket
Das Anchor-Timebucket ist das verankernde ü°™ Timebucket, f√ºr die Beschreibung jedes konkreten RuleJobs, der vom Bookkeeper generiert wird. 
F√ºr jeden RuleJob gibt es genau ein Anchor-Timebucket, das einen eindeutigen Ausgangspunkt bestimmt, wie aus dem zugrundeliegenden ü°™ Job-Schema alle konkreten Timebuckets der Vor- und Nachbedingungen f√ºr den zu generierenden RuleJob errechnet werden k√∂nnen. 
Im einfachsten Fall entspricht das Anchor-Timebucket dem Timebucket der Vorbedingung. Dies ist genau dann der Fall, wenn es genau ein ü°™ Timebucket-Schema als Vorbedingung gibt, der Offset dieses ü°™ Timebucket-Schemas 0 ist und die TruncationDimensions von ü°™ Job-Schema und ü°™ Vorbedingung gleich sind. 
Gelten mehrere ü°™ Vorbedingungen, so wird das jeweils fr√ºheste relevante ü°™ Timebucket als Anchor-Timebucket gesetzt. Aus diesem errechnet sich auch die ü°™ Referenzuhrzeit.


Job (RuleJobExecutor)
Ein RuleJobExecutor oder auch kurz Job im Sinne des Bookkeepers ist eine durch den Bookkeeper ausf√ºhrbare Scala-Klasse, in der Fachlogik implementiert ist, und die das Trait RuleJobExecutorHeavy implementiert. Die konkreten Klassen sind je nach Zugeh√∂rigkeit in den entsprechenden Teilkomponenten QS-Fzg oder QS-Erh implementiert. 


Job-Daten (RuleJob)
Ein RuleJob beschreibt die Datengrundlage, die als Parameter dem dazugeh√∂rigen RuleJobExecutor (ü°™ Job ) √ºbergeben wird. Die Erzeugung obliegt dem Bookkeeper unter Verwendung eines ü°™ Anchor-Timebuckets und eines RuleSchemas (ü°™ Job-Schema) aus der RuleConfig (ü°™ Job-Konfiguration).


Job-Konfiguration (RuleConfig)
Die Job-Konfiguration enth√§lt alle ü°™ Job-Schemata, ü°™  Timebucket-Schemata  sowie die ü°™ Timebucket-Konfiguration. 


Job-Schema (RuleSchema)
Jeder ü°™ Job muss, damit er ausgef√ºhrt werden kann, innerhalb der ü°™ Job-Konfiguration (RuleConfig) anhand eines RuleSchemas konfiguriert werden. 
Im RuleSchema k√∂nnen neben dem Ausf√ºhrungsintervall (‚ÄûTruncationDimansion‚Äú) bzw. -zeitpunkt auch ü°™ Vor- und ü°™ Nachbedingungen f√ºr die Ausf√ºhrung eines konkreten ü°™ Jobs aus QS-FzG oder QS-Erh festgelegt werden.  
ü°™ Vor- und ü°™ Nachbedingungen werden durch ü°™ Timebucket-Schemata (ü°™ Timebucket-Schema) definiert.
Weitere Informationen finden Sie in den Kapiteln 5.1.5.1 RuleConfig ‚Äì Job-Konfiguration und 5.1.5.2 RuleConfigTimebuckets ‚Äì Timebucket-Konfiguration. 


Jobkette
Eine Jobkette wird durch die Verkn√ºpfung mehrerer Jobs anhand ihrer im jeweiligen ü°™ Job-Schema definierten Vor- und Nachbedingungen gebildet. 
Beispiel: Eine Jobkette, in der die Ausf√ºhrung eines Jobs J2 von einem Job J1 abh√§ngt, wird sinngem√§√ü folgenderma√üen konfiguriert: Nachbedingung f√ºr Job1 = Vorbedingung f√ºr Job2


Nachbedingung (postCondition)
Im Bookkeeper-Kontext kann ein ü°™ Timebucket-Schema als Nachbedingung innerhalb eines ü°™ Job-Schemas festgelegt werden. Nachbedingungen dienen in erster Linie dazu ü°™ Jobketten aufzubauen.


Referenzuhrzeit
Die Referenzuhrzeit ist ein Zeitpunkt, der ausgehend vom ü°™ Anchor-Timebucket berechnet wird. Dabei wird zum Anchor-Timebucket-Timestamp das Ausf√ºhrungsintervall laut ü°™ Job-Konfiguration (TruncationDimension des RuleSchemas) addiert. 
Beispiel: 
Zeitstempel Anchor-Timebucket = 010617_1500
Ausf√ºhrungsintervall des ü°™ Jobs = Hours(1)
Die Referenzuhrzeit ist 16:00 Uhr am 1. Juni 2017. 
Die Referenzuhrzeit wird √ºber das RuleJob-Objekt (konkreter ü°™ Job) zur Verf√ºgung gestellt. 
Hintergrund: ü°™ Jobs wie z.B. ProcessDSRC15 ben√∂tigen die Referenzuhrzeit zur Ermittlung von Daten abh√§ngig vom aktuellen Timebucket (Endzeitpunkt des Ausf√ºhrungsintervalls).


Timebucket (Timebucket)
Die Verarbeitung innerhalb der QS-AV Applikation basiert auf der Partitionierung von Daten nach Zeitscheiben, die in Bl√∂cken (‚ÄûMini-Batches‚Äú) abgearbeitet werden. Ein Timebucket besteht aus einem Zeitstempel im Format ‚ÄûDDMMYY_hhmm‚Äú und geh√∂rt immer zu einem spezifischen Datenstrom. Der Datenstrom wird durch einen String bestimmt, der im Falle einer Cassandra-Tabelle deren Namen enth√§lt.
Der ü°™ Timebucket-Katalog aller Timebuckets mit ihren jeweils aktuellen Zeitstempeln wird in der Cassandra DB gehalten. 
Siehe auch ü°™ Timebucket-Schema.


Timebucket-Katalog (Tabelle en_q_timebuckets)
Der Timebucket-Katalog enth√§lt f√ºr alle konfigurierten ü°™ Timebuckets die jeweils aktuellen Timebucket-Zeitstempel. Er ist in der Tabelle en_q_timebuckets in der Cassandra-DB gespeichert.
Werden Datens√§tze durch den LDS-Client (au√üerhalb von QS-AV) in nach Timebuckets partitionierten Tabellen geschrieben, so muss zun√§chst der jeweils aktuelle Wert f√ºr den Timebucket-Zeitstempel dem Timebucket-Katalog entnommen werden. Der Timebucket-Zeitstempel wird dann als (Teil des) Partition Key(s) gespeichert. 
Der Timebucket-Katalog wird durch den ü°™ Timebucket-Switcher aktualisiert.
Hinweis: Werden Datens√§tze durch einen ü°™ Job (innerhalb von QS-AV) geschrieben, so ist die Job-Implementierung selbst daf√ºr verantwortlich, die gew√ºnschten Ziel-Timebuckets f√ºr die zu beschreibenden Cassandra-Tabellen zu bestimmen. Daf√ºr stehen die Attribute aus den ü°™ Job-Daten zur Verf√ºgung. Dabei sind insbesondere das ü°™ Anchor-Timebucket, aber auch ü°™ Timebuckets der ü°™ Vor- und Nachbedingungen relevant.


Timebucket-Konfiguration (RuleConfigTimebuckets)
Die Timebucket-Konfiguration enth√§lt alle ü°™ Timebucket-Schemata. Sie wird f√ºr die ü°™ Job-Konfiguration (Vor- und Nachbedingungen im ü°™ Job-Schema) sowie zur Konfiguration des Timebucket-Switchers ben√∂tigt.


Timebucket-Schema (TimebucketSchema)
Ein Timebucket-Schema ist eine abstrakte Beschreibung eines ü°™ Timebuckets, innerhalb der ü°™ Timebucket-Konfiguration. F√ºr ein vollst√§ndige Beschreibung sind drei Angaben n√∂tig:
Die TruncationDimension, die bestimmt, in welchem zeitlichen Abstand ü°™ Timebuckets aus diesem Timebucket-Schema erzeugt werden k√∂nnen (z.B. alle 5 Minuten).
Ein Offset, das den Abstand zum ü°™ Anchor-Timebucket innerhalb eines ü°™ Job-Schemas bestimmt.
Ein String, der den Datenstrom des erzeugten Timebuckets referenziert. Bei Timebuckets f√ºr Cassandra-Tabellen ist hier der Cassandra-Tabellen-Name verzeichnet.
Formal k√∂nnen folgende drei Timebucket-Typen konfiguriert werden:
Tabellen-Timebuckets: Timebucket, das auf eine konkrete Tabelle zeigt, z.B. LDS-Tabellen. Die Daten der Tabellen werden via ü°™ Timebucket-Switcher Zeitscheiben zugeordnet und als Timebuckets im ü°™ Timebucket-Katalog  persistiert. 
Timer-Timebuckets: Besitzen keinen Bezug auf eine Tabelle. Timer-Timebuckets werden zur reinen Zeitsteuerung von ü°™ Jobs, die keinerlei inhaltliche Abh√§ngigkeiten zu anderen Jobs oder Datenstr√∂men als Vorbedingung haben, verwendet. Stattdessen wird ein ü°™ Timebucket-Schema √ºbergeben, welches ein Timer-Timebucket beschreibt. Der ü°™ Timebucket-Switcher switcht die Timebuckets, die im ü°™ Timebucket-Katalog persistiert werden. 
Signal-Timebuckets: Signal-Timebuckets sind Timebuckets, die als Vorbedingung f√ºr einen zeitgesteuerten ü°™ Job  fungieren, bzw. als Ergebnis (=Nachbedingung) eines ü°™ Jobs erzeugt werden. Sie werden nicht vom Timebucket-Switcher betrachtet, da sie in der Kette der Jobausf√ºhrungen direkt ins Zustands-Log geschrieben und abgearbeitet werden.


Timebucket-Switcher
Die Timebucket-Switcher Applikation ist f√ºr die Aktualisierung (‚Äûswitch‚Äú) der Timebucket-Zeitstempel im ü°™ Timebucket-Katalog zust√§ndig. Die Zeitscheibengr√∂√üe kann dabei je ü°™ Timebucket anhand des ü°™ Timebucket-Schemas konfiguriert werden. Nach jeder Aktualisierung schreibt der Timebucket-Switcher einen entsprechenden Eintrag ins ü°™ Zustands-Log. Da sowohl die Datenpartitionierung als auch die Ablaufsteuerung von den ü°™ Timebuckets abh√§ngt, werden hohe Verf√ºgbarkeitsanforderungen an die Timebucket-Switcher Applikation gestellt. Weitere Details zum Timebucket-Switcher finden Sie im Kapitel 5.3 Timebucket-Switcher Applikation.


Vorbedingung (preCondition)
Im Bookkeeper-Kontext kann ein ü°™ Timebucket-Schema als Vorbedingung innerhalb eines ü°™ Job-Schemas festgelegt werden. Vorbedingungen definieren die Abh√§ngigkeiten eines Jobs von einem oder mehreren ü°™ Timebuckets.


Zustands-Log
Der Bookkeeper verwaltet ein Zustands-Log bestehend aus den drei Cassandra-Tabellen bkpr_eventlog, bkpr_waiting_timebuckets und bkpr_rule_jobs. 
Im Zustands-Log erfolgt die Speicherung und Protokollierung (Historie) von Events (Status√ºberg√§ngen), Job-Statusinformationen und ‚ÄìDaten, sowie Timebucket- Zuordnungen zu Jobs (Vorbedingungen) und deren Status.
Das Zustands-Log fungiert als zentrale Informationsstelle und Schnittstelle zwischen Bookkeeper und ü°™ Timebucket-Switcher Applikation.
Weitere Informationen finden Sie im Kapitel 5.1.6 Zustands-Log.

√úberblick und allgemeine Funktionsweise

Abbildung 19: √úberblick Bookkeeper (Kontext)
In der obigen Abbildung sind mittig der Bookkeeper (Teil der QS-AV Applikation) und die umgebenden Teil-/komponenten mit Fokus auf die Ablaufsteuerung dargestellt.
Der Bookkeeper enth√§lt die Ablaufsteuerung (‚ÄûWiederaufsatz und Job-Ausf√ºhrung‚Äú) f√ºr die in den Teilkomponenten QS-FzG und QS-Erh implementierten Jobs. Zur Steuerung durch den Bookkeeper k√∂nnen sowohl inhaltliche Abh√§ngigkeiten als auch zeitliche Abh√§ngigkeiten konfiguriert werden. N√§here Informationen zu den unterschiedlichen Abh√§ngigkeiten finden Sie im Kontext der Kapitel 5.1.4 Wiederaufsatz und Job-Ausf√ºhrung und 5.1.5 Konfiguration.
Die Erf√ºllung der zeitlichen Abh√§ngigkeiten (Ausf√ºhrungsintervalle) wird anhand der Systemzeit im Zusammenspiel mit der aktuellen Zeitscheibe (Anchor-Timebucket) gepr√ºft. Die inhaltlichen Abh√§ngigkeiten werden anhand des Zustands-Logs gepr√ºft. Die Job-Status(-√ºberg√§nge) werden vom Bookkeeper ebenfalls ins Zustands-Log geschrieben. Weitere Informationen zum Zustands-Log finden Sie im Kapitel 5.1.6 Zustands-Log.
 Die Verarbeitung innerhalb der QS-AV Applikation basiert auf der Partitionierung von Daten nach Zeitscheiben (‚ÄûTimebuckets‚Äú), die in Bl√∂cken abgearbeitet werden. Ein Timebucket besteht aus einem Zeitstempel im Format ‚ÄûDDMMYY_hhmm‚Äú und geh√∂rt immer zu einem spezifischen Datenstrom (i.d.R. einer Cassandra-Tabelle). Der Timebucket-Katalog aller Timebuckets mit ihren jeweils aktuellen Zeitstempeln wird in der Cassandra DB gehalten. 
Werden Datens√§tze durch einen an das QS-AV liefernden LDS-Client in nach Timebuckets partitionierten Tabellen geschrieben, so muss zun√§chst der jeweils aktuelle Wert f√ºr den Timebucket-Zeitstempel dem Timebucket-Katalog entnommen werden. Dieser wird dann als (Teil des) Partition Key(s) gespeichert. 
Die Verwendung der Timebuckets erm√∂glicht es, asynchron eintreffende oder verarbeitete Daten unterschiedlicher Datenquellen performant aus der Cassandra DB selektieren zu k√∂nnen. Diese durch die Timebuckets synchronisierten Daten k√∂nnen dann innerhalb von Batch-Operationen verarbeitet werden. 
Weitere Informationen zu Konfiguration und Arten von Timebuckets entnehmen Sie den Kapiteln 5.1.5 Konfiguration und 5.1.2 Bookkeeper Glossar (ü°™Timebucket-Schema).
Die Timebucket-Switcher Applikation ist f√ºr die Aktualisierung (‚Äûswitch‚Äú) der Timebucket-Timestamps im Timebucket-Katalog zust√§ndig. Die Zeitscheibengr√∂√üe kann dabei je Timebucket-Schema konfiguriert werden. Nach jeder Aktualisierung schreibt der Timebucket-Switcher einen entsprechenden Eintrag ins Zustands-Log. Da sowohl die Datenpartitionierung als auch die Ablaufsteuerung von den Timebuckets abh√§ngt, werden hohe Verf√ºgbarkeitsanforderungen an die Timebucket-Switcher Applikation gestellt. Weitere Details zum Timebucket-Switcher finden Sie im Kapitel 5.3 Timebucket-Switcher Applikation.
√úber eine REST-Schnittstelle (‚ÄûAdmin-Interface‚Äú) k√∂nnen Statusinformationen zum Bookkeeper aus dem Zustands-Log abgerufen und abgebrochene Jobs neu gestartet werden. Weitere Informationen zum Admin-Interface finden Sie im Kapitel 5.1.7 Admin-Interface (REST-API).
Detailabbildung
Die folgende Abbildung 20: Bookkeeper √úberblick Detail (Kontext) gibt einen detaillierteren √úberblick √ºber die Zusammenh√§nge. In den darauffolgenden Kapiteln werden die einzelnen Bestandteile und Konzepte beschrieben.

Abbildung 20: Bookkeeper √úberblick Detail (Kontext)
Wiederaufsatz und Job-Ausf√ºhrung
In der folgenden Abbildung sind die Beziehungen der zentralen Klassen und Objekte der Bookkeeper-Ablaufsteuerung dargestellt.



Abbildung 21: Bookkeeper ‚Äì Klassen und Objekte (Ablauf)
Die Klasse LongRunner wird bei Programmstart instanziiert und gestartet (siehe auch Kapitel 5.2.2 qs-boot: Hauptapplikation (run)). Sie kontrolliert die Jobausf√ºhrung √ºber die Komponente JobExecutor und behandelt alle auftretenden nicht-fatalen Fehler. 
Der JobExecutor f√ºhrt folgende Schritte aus:

Abbildung 22: Detail - JobExecutor Ablauf (stark vereinfacht)
Bei der Initialisierung des JobExecutors wird gepr√ºft, ob Recovery-Ma√ünahmen notwendig sind. Bei Bedarf wird ein Wiederaufsatz mit Hilfe der Informationen im Zustands-Log durchgef√ºhrt. 
Nach der Initialisierung des JobExectors startet der LongRunner die Verarbeitung der regul√§r konfigurierten und der per Admin-Interface neugestarteten Jobs. Dazu ruft der LongRunner in einer Schleife die step()-Methode des JobExecutors auf. Je Durchlauf werden dabei folgende Schritte ausgef√ºhrt: 
Im Zustands-Log wird nach Jobs gesucht, die per Admin-Interface zum Neustart vorgemerkt wurden. Diese Jobs werden nach dem FIFO-Prinzip direkt ausgef√ºhrt.
Die regul√§r konfigurierten Jobs (gem√§√ü RuleConfig) werden ausgef√ºhrt:
Pr√ºfung der zeitlichen Abh√§ngigkeiten: Anhand der aktuellen Zeitscheibe wird ermittelt, welche Jobs gem√§√ü ihres Ausf√ºhrungsintervalls zum Start bereit sind. Die betreffenden Jobs werden zur Ausf√ºhrung vorgemerkt, indem entsprechende Eintr√§ge ins Zustands-Log aufgenommen werden (siehe auch Kapitel 5.1.6 Zustands-Log).
Pr√ºfung der inhaltlichen Abh√§ngigkeiten: Mit Hilfe des RulePreconditionCheckers wird ermittelt, ob die in der RuleConfig (Job-Konfiguration) spezifizierten Vorbedingungen (preConditions) f√ºr den Job-Start erf√ºllt sind. Dazu z√§hlt u.a. ob alle Vorg√§nger-Jobs ausgef√ºhrt wurden, vorausgesetzte Tabellen-Timebuckets gef√ºllt sind und/oder vorausgesetzte Signal-Events registriert wurden (Signal-Timebuckets). 
Sind alle Abh√§ngigkeiten bzw. Vorbedingungen f√ºr einen Job erf√ºllt, wird er ausgef√ºhrt: Anhand des zugeh√∂rigen RuleSchemas (Job-Schema) wird √ºber das CassandraRuleJobRepository eine konkrete Instanz des RuleJobs erzeugt und die Ausf√ºhrung durch den konfigurierten RuleJobExecutorHeavy angetriggert. Die Implementierungen der RuleJobs an sich befinden sich in den Teilkomponenten qs-fzg und qs-erh. 
Innerhalb der gleichen Jobkette ggf. abh√§ngige Jobs werden direkt im Anschluss ausgef√ºhrt.
Informationen zum Status bzw. zu Status√ºberg√§ngen werden im Zustands-Log festgehalten.
Nach jedem Schleifendurchlauf wird 2 Sekunden pausiert.
Konfiguration
Die Job- und Timebucket-Konfiguration erfolgt √ºber die Klassen RuleConfig und RuleConfigTimebuckets, wie in folgender Abbildung dargestellt. 


Abbildung 23: Bookkeeper ‚Äì Klassen und Objekte (Konfiguration)
In der Klasse RuleConfig erfolgt die Job-Konfiguration f√ºr die einzelnen Jobs jeweils anhand eines RuleSchemas (Job-Schema) mit ihren Vor- und Nachbedingungen. 
In der Klasse RuleConfigTimebuckets (Timebucket-Konfiguration) werden die innerhalb der Job-Konfiguration als Vor- und Nachbedingungen genutzten Timebuckets anhand von TimebucketSchemas spezifiziert. 
Allgemein m√ºssen alle genutzten Timebuckets an dieser Stelle konfiguriert werden. Die Timebucket-Konfiguration wird vom Bookkeeper und dem Timebucket-Switcher ben√∂tigt.
RuleConfig ‚Äì Job-Konfiguration
Die Konfiguration der Jobs innerhalb der RuleConfig verkn√ºpft folgende Aspekte:
Job-Klasse: Verkn√ºpfung der konkreten Job-Implementierung aus den Teilkomponenten qs-fzg und qs-erh (z.B. Job MobaEventCreator im Package de.tollcollect.zme.qs.erh.rule). Hinweis: Die Jobs m√ºssen das Trait RuleJobExecutorHeavy direkt oder abgeleitete Traits (z.B. RuleJobExecutorLight)  implementieren.
Ausf√ºhrungsintervall: Ausf√ºhrungsintervall f√ºr den Job (truncationDimension)
Vorbedingung/en (preConditions): Vorbedingungen, die als Voraussetzung f√ºr die Ausf√ºhrung eines Jobs angesehen werden. Es handelt sich dabei um ein oder mehrere Timebuckets, die per TimebucketSchema in der RuleConfigTimebuckets konfiguriert werden.
Nachbedingung/en (postConditions): Nachbedingungen, die als Ergebnisse der Job-Ausf√ºhrung erf√ºllt sind und die als Vorbedingungen f√ºr Folge-Jobs in der Verarbeitungskette (Jobkette) fungieren. Es handelt sich dabei um ein oder mehrere Timebuckets, die per TimebucketSchema in der RuleConfigTimebuckets konfiguriert werden.
Wichtige Hinweise: Eine Tabelle (= ein TimebucketSchema) darf jeweils immer nur f√ºr einen RuleJob als Ausgangstabelle fungieren. 
Weitere Informationen zu Arten von Timebuckets bzw. Timebucket-Schemata finden Sie im Bookkeeper Glossar (Kapitel 5.1.2).
Konfiguration eines RuleSchema (via Klasse RuleConfig)
Parameter
Typ
Beschreibung
name
String
Eindeutige Bezeichnung f√ºr den Job, der auch f√ºr das Zustands-Log verwendet wird.
truncationDimension
TruncationDimension
Ausf√ºhrungsintervall/-zeitpunkt des RuleJobs. Kann wie folgt angegeben werden:
Ein Mal t√§glich zur vollen Stunde:
FixedHourInDay(<Stunden, z.B. 23>)‚Äú 
Intervall ‚Äì alle x Stunden:
Hours(<x Stunden, z.B. 1>)‚Äú
Intervall ‚Äì alle y Minuten:
Minutes(<y Minuten, z.B. 15>)‚Äú
preConditions
Seq[TimebucketSchema]
Vorbedingung/en als Liste von TimebucketSchemata
Es handelt sich dabei um ein oder mehrere Timebuckets, die per TimebucketSchema in der RuleConfigTimebuckets konfiguriert werden.
Die Konfiguration von preConditions f√ºhrt dazu, dass der Job erst gestartet wird, wenn alle Vorbedingungen erf√ºllt sind.
postConditions
Seq[TimebucketSchema]
Optionale Nachbedingung/en als Liste von TimebucketSchemata
Es handelt sich dabei um ein oder mehrere Timebuckets, die per TimebucketSchema in der RuleConfigTimebuckets konfiguriert werden.
Die Angabe von postConditions dient dazu, dass  Folge-Jobs innerhalb der  gleichen Jobkette direkt im Anschluss an  die erfolgreiche Ausf√ºhrung des dazugeh√∂rigen RuleJobExecutors gestartet werden  k√∂nnen, sofern damit alle Voraussetzungen (preConditions) f√ºr die betreffenden Folge-Jobs erf√ºllt sind. 
ruleExec
RuleJobExecutorHeavy
Klasse des konkret zu startenden Jobs (RuleJob) aus qs-fzg oder qs-erh, der das Trait RuleJobExecutorHeavy oder eine Ableitung implementiert und die Ausf√ºhrungslogik enth√§lt
forceRun
Boolean
Flag, das angibt, ob der Job ausgef√ºhrt werden soll, auch wenn nicht alle Voraussetzungen erf√ºllt sind. Default ist false

Tabelle 33: Bookkeeper RuleSchema-Konfiguration ‚Äì RuleConfig
Zus√§tzliche Funktionen
Das TimebucketSchema stellt die Funktion explode(INT) bereit. Sie erwartet eine Zahl vom Typ INT, die angibt, wie viele Timebuckets des TimebucketSchemas (gem√§√ü der truncationDimension des TimebucketSchemas) vom Anchor-Timebucket aus  in die Zukunft blickend  betrachtet werden sollen. Wird z.B. ein RuleSchema definiert, welches t√§glich um 0 Uhr l√§uft und die letzten 24 Stunden auswerten soll, dann muss bei einem TimebucketSchema mit einer Partitionierung von 15 Minuten ein explode von 96 angegeben werden (<TimebucketSchema>.explode(96).toList()).
Um sicherzustellen, dass alle Daten des Eingangsdatenstroms eines Jobs erfasst und einmalig verarbeitet werden k√∂nnen, m√ºssen die als preConditions genutzten Timebuckets mit dem Ausf√ºhrungsintervall des Jobs √ºbereinstimmen bzw. durch die Nutzung von explode() angeglichen werden. 
Beispiele  
RuleSchema("fzg/dm/mdnParser/15m", Minutes(15), List(timebucketSchemaMdn), List(timebucketSchemaDmMdn), MdnParser),
RuleSchema("erh/edm/metricVerdichtung/24h", FixedHourInDay(0),  timebucketSchemaEdmMetrik.explode(96).toList, List(timebucketMetrikOut24h), new GenericMetricVerdichtung(), forceRun = true)

Die aktuelle Konfiguration f√ºr die QS-AV Applikation ist im Kapitel 7 Ablaufsicht grafisch dargestellt.
RuleConfigTimebuckets ‚Äì Timebucket-Konfiguration
Die grafische Darstellung finden Sie in der Abbildung 23: Bookkeeper ‚Äì Klassen und Objekte (Konfiguration).
Konfiguration eines TimebucketSchema (innerhalb der Klasse RuleConfigTimebuckets)
Das Timebucket-Schema beschreibt ein Timebucket, das sich aus dem Namen des Eingangs-/Ausgangsdatenstroms, dem Intervall f√ºr die Partitionierung und optional einem konkreten Timebucket (vom Anchor-Zeitpunkt aus gesehen) zusammensetzt. 
Die unterschiedlichen Arten von Timebuckets sind im Glossar-Eintrag zum Timebucket-Schema beschrieben (siehe Kapitel 5.1.2 Bookkeeper Glossar). Die Unterscheidung ist nur f√ºr die Konfiguration relevant und nicht f√ºr den Bookkeeper oder den Timebucket-Switcher, die jedes zur Verf√ºgung stehende Timebucket gleich behandeln.
Das TimebucketSchema spezifiziert ein Timebucket √ºber folgende Parameter:
Parameter
Typ
Beschreibung
tableName
String
Der tableName kann:
Name einer realen Eingangs-/Ausgangs-Tabelle sein, die mit diesem Timebucket verkn√ºpft ist
Eine frei gew√§hlte Bezeichnung f√ºr ein Timing-Timebucket sein, das √ºber das Zustands-Log hinweg keine weitere Relevanz f√ºr den Ablauf hat
Name eines Signal-Ereignisses sein
Hinweise: Aus Gr√ºnden der √úbersichtlichkeit sollte der Tabellenname bei Timer-Timebuckets immer mit dem Pr√§fix ‚Äûtimingtb-‚Äú und der von Signal-Timebuckets mit ‚Äûsignal-‚Äú beginnen.
truncationDimension
TruncationDimension
Intervall f√ºr die Segmentierung der Daten eines Datenstroms (Partitionierung). Kann wie folgt angegeben werden:
Ein Mal t√§glich zur vollen Stunde:
FixedHourInDay(<Stunden, z.B. 23>)‚Äú 
Intervall ‚Äì alle x Stunden:
Hours(<x Stunden, z.B. 1>)‚Äú
Intervall ‚Äì alle y Minuten:
Minutes(<y Minuten, z.B. 15>)
anchorDistanceInTruncationSizeMultiple
Int
Wert, der angibt, auf das wievielte Timebucket in der Vergangenheit sich vom Anchor des RuleSchemas bezogen werden soll. 
Beispiel f√ºr einen Job, der jede Stunden ausgef√ºhrt werden soll und ein TimebucketSchema mit einer Partitionierung von 15 Minuten verwendet: Wird als anchorDistanceInTruncationSizeMultiple ein Wert von 3 angegeben, w√ºrde nur das jeweils dritte Timebucket jeder Stunde, also √§quivalent 00:45, 01:45, 02:45  usw. verarbeitet werden.
Soll jedes Timebucket ausgewertet werden, ist ein Wert von 0 anzugeben.

Tabelle 34: Bookkeeper TimebucketSchema-Konfiguration - RuleConfigTimebuckets
Wichtiger Konfigurationshinweis: F√ºr alle Timebuckets, die durch den Timebucket-Switcher geswitcht werden sollen (Timer- und Tabellen-Timebuckets), m√ºssen zus√§tzlich folgende Konfigurationst√§tigkeiten durchgef√ºhrt werden:
1) TimeBucketSchema zur Liste ldsEnQtimebuckets in der Klasse RuleConfigTimebuckets hinzuf√ºgen.
2) Timebucket im Timebucket-Katalog (Tabelle en_q_timebuckets) per CQL-insert registrieren: INSERT INTO en_q_timebuckets ( table_name, curr_timebucket ) VALUES ('<tableName>', '19700101_0000');
Beispiele
TimebucketSchema(enQmdn, Minutes(15), 0)
TimebucketSchema(‚ÄûtimingTb-RseKontaktDeletion‚Äú, FixedHourInDay(4), 0)

Die aktuelle Konfiguration f√ºr die QS-AV Applikation ist im Kapitel 7 Ablaufsicht grafisch dargestellt.
Zustands-Log
Der Bookkeeper verwaltet ein Zustands-Log bestehend aus den drei Cassandra-Tabellen bkpr_eventlog, bkpr_waiting_timebuckets und bkpr_rule_jobs.

Abbildung 24: Zustands-Log (Bookkeeper)
Speicherung und Protokollierung
Im Zustands-Log erfolgt die Speicherung und Protokollierung (Historie) von:
Protokolleintr√§gen (Events und Status√ºberg√§nge)
Job-Statusinformationen und -Daten 
Timebucket-Zuordnungen zu Jobs (Vorbedingungen) und deren Status
Einsatz
Das Zustands-Log wird ben√∂tigt f√ºr:
Die Funktionsf√§higkeit des Bookkeepers, der die Job-Ausf√ºhrung in Abh√§ngigkeit des Zustands-Logs steuert.
Den Wiederaufsatz nach geordnetem Shutdown oder Applikationscrash.
Den Neustart von Jobs, die nicht erfolgreich ausgef√ºhrt werden konnten, per Admin-Interface.
Die Nachvollziehbarkeit der Ablaufsteuerung (Protokollierung der Job-Status).
Nutzung
Das Zustands-Log wird von folgenden Teil-/Sub-/Komponenten genutzt:
Bookkeeper
Timebucket-Switcher
Admin-Interface
Datenbanktabellen
Im Folgenden sind die Zustands-Log-Tabellen im Detail beschrieben.
Datenmodell: Die detaillierte Dokumentation der Attribute/Spalten finden Sie im Referenzdokument [13] Datenmodell. 
bkpr_eventlog: Protokolleintr√§ge (Events und Status√ºberg√§nge) 
Bei der Tabelle bkpr_eventlog handelt es sich um die zentrale Tabelle f√ºr die Bookkeeper-Events und Status√ºberg√§nge. Jede Zeile entspricht einem atomaren und konsistenten Protokolleintrag des Bookkeeper-Ablaufes. Jeder Eintrag ist durch eine zum Insert-Zeitpunkt generierte UUID (event_id) eindeutig identifizierbar.
Zu jedem Protokolleintrag k√∂nnen zus√§tzlich ben√∂tigte Daten in den Sekund√§rtabellen bkpr_waiting_timebuckets und bkpr_rule_jobs gespeichert und via UUIDs verkn√ºpft werden. Die Schreibvorg√§nge erfolgen immer  in Form von Cassandra-Atomic-Batches. Dadurch soll sichergestellt werden, dass Eintr√§ge im Zustands-Log immer komplett √ºber alle betreffenden Tabellen persistiert werden.
Jeder Protokolleintrag ist eindeutig typisiert. Die folgende Tabelle beschreibt zu jedem verf√ºgbaren Typen (event_type:INT) die typabh√§ngig beschriebenen Datenfelder innerhalb der Tabelle sowie die per Atomic-Batch beschriebenen Sekund√§rtabellen:
Typ
Name (Info)
Event-Typ spezifische Spalten in bkpr_eventlog
Beschriebene Sekund√§rtabellen
1
LdsEnqTimebucketSwitched 
new_timebuckets = set<TEXT> 
Menge von Timebuckets kodiert als String im Format: <tabellenname>;<timebucket> 
Beispiel ‚Äúlds_dm_en_q_xy;201601170915‚Äú
 -
2
RuleJobExecutedSuccess 
new_timebuckets = set<TEXT> 
Menge von Timebuckets kodiert als String im Format: <tabellenname>;<timebucket> 
Beispiel ‚Äúlds_dm_en_q_xy;201601170915‚Äú
 -
3
RuleJobExecutedFailed 
new_timebuckets = set<TEXT> 
Menge von Timebuckets kodiert als String im Format: <tabellenname>;<timebucket> 
Beispiel ‚Äúlds_dm_en_q_xy;201601170915‚Äú
 -
4
NewRuleJobsCreated 
rule_jobs_created = set<TIMEUUID> Menge mit Referenzen auf Tabelle bkpr_rule_Jobs per TIMEUUID
bkpr_waiting_timebuckets
bkpr_rule_jobs 
5
RuleJobReadyToExecute 
rule_job_to_exec = TIMEUUID mit Referenz auf Tabelle bkpr_rule_Jobs per TIMEUUID
bkpr_waiting_timebuckets
bkpr_rule_jobs 
6
NoJobReadyToExecute 
 -
bkpr_waiting_timebuckets
bkpr_rule_jobs 

Tabelle 35: Zustands-Log-Tabelle bkpr_eventlog ‚Äì Typen von Protokolleintr√§gen
Jeder Protokolleintrag weist einen der vier Zust√§nde (state) von 0 bis 3 auf: 
0=‚Äûcreated‚Äú
1=‚Äûoutdated‚Äú
2=‚ÄûaPrioriOutdated‚Äú
3=‚Äûrestarted‚Äú 
Der Zustand spielt eine zentrale Rolle f√ºr den konsistenten Wiederaufsatz-Prozess. Bei Erstellung wird der Zustand initial auf den Wert 0 (created) gesetzt. Ausnahme ist der Event-Typ 6 (NoJobReadyToExecute), der initial auf den Wert 2 (aPrioriOutdated) gesetzt wird, weil dieses Event nicht zum Wiederaufsatz n√∂tig ist und nur zur besseren √úberpr√ºfbarkeit der Korrektheit aller Bookkeeper-Entscheidungen geschrieben wird. 
Bis auf den Event-Typ LdsEnqTimebucketSwitched (1) hat jeder Protokolleintrag einen  eindeutigen Vorg√§ngereintrag im Log, der in der Spalte event_id_outdated referenziert wird. Der Zustand dieses referenzierten Events wird beim INSERT des Nachfolge-Events auf den Zustand 1 (outdated) gesetzt und damit obsolet f√ºr den Wiederaufsatz, weil ein neuer konsistenter Zustand f√ºr dieses Blatt des baumartigen Job-Ausf√ºhrungs-Graphen erreicht ist.
bkpr_waiting_timebuckets: Timebucket-Zuordnungen zu Jobs 
Beim Einstellen von neuen RuleJobs wird f√ºr alle TimebucketSchemata, die als Vorbedingung f√ºr das Ausf√ºhren dieses RuleJobs konfiguriert wurden, in dieser Tabelle ein neuer Key angelegt (tablename_timebucket) bzw. ein schon bestehender Key um die UUID des wartenden RuleJobs erweitert. Auf diese Weise ist bei "Eintreffen" neuer fertiger Timebuckets (Abschluss eines Timebuckets durch switch) eine effiziente Entscheidung dar√ºber m√∂glich, welche RuleJobs auf das Timebucket warten. Die eingetroffenen Timebuckets werden aus dieser Tabelle wieder gel√∂scht, nachdem die Bookkeeper-Entscheidung konsistent als Protokolleintrag in der Tabelle bkpr_eventlog  geloggt worden ist.
bkpr_rule_jobs: Job-Statusinformationen und ‚ÄìDaten
In dieser Tabelle werden alle wartenden RuleJobs vermerkt. Via dem Partition-Key rule_job_id  (TIMEUUID) wird von den beiden anderen Tabellen auf diese Tabelle verwiesen. Beim Eintreffen von neuen Timebuckets werden diese aus der Spalte needed_timebuckets gel√∂scht. Sobald dieses Set leer geworden ist, kann der entsprechende RuleJob ausgef√ºhrt werden.
Admin-Interface (REST-API)
F√ºr den Wiederaufsatz wurde eine Jetty-basierte Administrations-REST-API erstellt, die per Default unter http://127.0.0.1:8092 erreichbar ist.  Hinweis: Host und Port sind √ºber die Umgebungsvariablen QSAV_ADMIN_HOST und QSAV_ADMIN_PORT beim Deployment konfigurierbar.
√úber die API lassen sich abgebrochene Jobs ausgeben und neu starten. Die Jobs werden bei der Job-Ausf√ºhrung je Zyklus bevorzugt behandelt.
Die zur Verf√ºgung gestellten Services sind im Folgenden beschrieben.
Service: Usage (Bedienungsanleitung)
Aufruf
GET http://127.0.0.1:8092/                                       
Beschreibung
Gibt die Bedienungsanleitung zur√ºck

Tabelle 36: Administrations-API (Bookkeeper) - Service f√ºr die Bedienungsanleitung
Service: Abgebrochene Jobs
Aufruf
GET http://127.0.0.1:8092/admin/failed_jobs.json                              
Beschreibung
Gibt eine Liste aller abgebrochenen Jobs zur√ºck
Beispiel
curl http://127.0.0.1:8092/admin/failed_jobs.json 
[{
  "jobName" : "fzg/dm/mdnParser/15m",
  "errorMessage" : "Artificial, I don't want to know",
  "failTime" : "2017-03-23T11:32:41.526",
  "logId" : "0b3e6d60-0fb4-11e7-86e3-7178fe8bf3d4",
  "jobId" : "92abf363-0fb1-11e7-8ac7-7178fe8bf3d4",
  "timebucketsProvided" : "fzg_dm_mdn;20170323_1000",
  "timebucketsNeeded" : "",
  "timebucketsNeededOriginally" : "lds_dm_en_q_mdn;20170323_1000"
}, {
  "jobName" : "fzg/dm/mdnParser/15m",
  "errorMessage" : "Artificial, I don't want to know",
  "failTime" : "2017-03-23T11:32:41.534",
  "logId" : "0b3fa5e0-0fb4-11e7-86e3-7178fe8bf3d4",
  "jobId" : "ac3904b0-0fb3-11e7-b1d6-7178fe8bf3d4",
  "timebucketsProvided" : "fzg_dm_mdn;20170323_1015",
  "timebucketsNeeded" : "",
  "timebucketsNeededOriginally" : "lds_dm_en_q_mdn;20170323_1015"
} ]

Tabelle 37: Administrations-API (Bookkeeper) - Service zur Auflistung abgebrochener Jobs
Service: Job neu starten
Aufruf
POST http://127.0.0.1:8092/admin/job_restart?id=<Log-ID>
Beschreibung
Startet einen Job mit gegebener Log-ID neu
Beispiel
curl -X POST "http://localhost:8092/admin/job_restart?id=c066c1f0-0fb0-11e7-a5af-7178fe8bf3d4"
Job Restart requested

Tabelle 38: Administrations-API (Bookkeeper) - Service zum Neustart eines Jobs
Service: Mehrere Jobs neu starten
Aufruf
POST http://127.0.0.1:8092/admin/job_restart_all?name=<Name>&maxHours=<hours>
Beschreibung
Startet alle Jobs mit dem gegebenen namen, die in den letzten maxHours fehlgeschlagen sind neu.
Beispiel
curl -X POST "http://127.0.0.1:8092/admin/job_restart_all?name=fzg/dm/mdnParser/15m&maxHours=2"
Requested restart of following jobs Vector(0b3e6d60-0fb4-11e7-86e3-7178fe8bf3d4, 0b3fa5e0-0fb4-11e7-86e3-7178fe8bf3d4, 0c604c90-0fbe-11e7-9a4c-7178fe8bf3d4)

Tabelle 39: Administrations-API (Bookkeeper) - Service zum Neustart aller Jobs
Hinweis: Beim Aufruf ist auf korrektes HTTP-Escaping zu achten.
Fehlerbehandlung
Die Fehlerbehandlung ist im Referenzdokument [17] [A_QSAV_HBB_00] QS-AV Betriebshandbuch dokumentiert.
QS-AV Applikation
Die Realisierung der Gesch√§ftslogik der Bereiche QS-AV erfolgt gekapselt innerhalb entsprechender Teilkomponenten: qs-fzg und qs-erh. Diese sind zusammen mit weiteren technischen Komponenten in den √ºbergeordneten Applikationskontext eingebettet. 
Hinweis: Technisch sind die Teilkomponenten als Maven-Projekte realisiert.

Abbildung 25: √úberblick Software-Komponenten QS-AV Applikation inkl. Timebucket-Switcher
Die Aufteilung der Maven-Projekte basiert auf folgenden Aspekten:
Rahmen-Paket, das alle funktionalen Subkomponenten enth√§lt (qs-parent)
Applikationskontext f√ºr den Programmstart (qs-boot)
Kapselung gemeinsam genutzter Basisfunktionalit√§ten (z.B. Bookkeeping) (qs-common)
Aufteilung der Gesch√§ftslogik nach QS-FzG und QS-Erh (qs-fzg und qs-erh)
Kapselung gemeinsam genutzter Basisfunktionalit√§ten und Utilities der Komponententests (qs-common-test)
Timebucket-Switcher (qs-timebucketswitcher)
Die einzelnen Projekte werden in den folgenden Abschnitten n√§her beschrieben.
qs-parent: Parent-POM (technisch)
Das Maven-Modul qs-parent aggregiert folgende Maven-Module:
<modules>
        <module>../qs-common</module>
        <module>../qs-common-test</module>
        <module>../qs-fzg</module>
        <module>../qs-erh</module>
        <module>../qs-boot</module>
  <module>../qs-timebucketswitcher</module>
</modules>
Listing 1: qs-parent Module (Auszug aus der pom.xml)
Das Parent-POM dient der Zusammenfassung der Software-Module im Build-Prozess. Dar√ºber hinaus bietet es u.a. M√∂glichkeiten zur Vererbung gemeinsamer Merkmale, wie dem Quell-Encoding (UTF-8), zur globalen Definition von Compiler-Versionen, zum zentralen Management transienter Dependencies oder zum Version-Alignment.
Das Modul qs-parent enth√§lt keine Anwendungslogik.
qs-boot: Hauptapplikation (run)
Das Maven-Modul qs-boot fasst die Subkomponenten qs-fzg und qs-erh in einem gemeinsamen Applikationskontext zusammen. 
Hinweis: Mit dem Start von qs-boot wird das Scheduling und die Ausf√ºhrung der Qualit√§tssicherungs-Jobs aus den Teilkomponenten qs-fzg und qs-erh durch den Bookkeeper gestartet. Siehe auch Kapitel 7 Ablaufsicht.
qs-common: Basiskomponenten und Utilities
In der Subkomponente qs-common sind gemeinsam genutzte Basisfunktionalit√§ten gekapselt:
Bookkeeper  inklusive abstrakten Jobs und Utility-Funktionen zur Benutzung in konkreten Jobimplementierungen innerhalb der Packages de.tollcollect.zme.qs.common.jobs und de.tollcollect.zme.qs.common.rule. 
Der Bookkeeper ist als Komponente zur Programmablaufsteuerung gesondert im Kapitel 5.1 Schwerpunktkapitel: Bookkeeper (qs-common) beschrieben.
Basiskonfiguration de.tollcollect.zme.qs.common.conf.tech: 
Die Klasse Config bietet Zugriff auf die in der Cassandra-DB gespeicherte Applikationskonfiguration (Tabelle tech_config).
Die Klasse SchwellwertProvider bietet Zugriff auf die in der Cassandra-DB gespeicherten Schwellwerte (Tabelle schwellwert).
Context: Applikationskontext u.a. mit SparkContext, CassandraSQLContext, HiveContext , Config, SchwellwertProvider, Cassandra-Host-, Cassandra-Session- und ‚ÄìCassandra-Keyspace-Konfiguration
Utils: Allgemeine Utilities finden sich im Package de.tollcollect.zme.qs.common.util
qs-fzg: Qualit√§tssicherung der Fahrzeugger√§te
Die Teilkomponente qs-fzg enth√§lt die Anwendungslogik inklusive der Qualit√§tssicherungsalgorithmen f√ºr die Qualit√§tssicherung der Fahrzeugger√§te. Die Implementierung ist komplett in Scala gehalten und erfolgt innerhalb einzelner Jobs, die durch den Bookkeeper gesteuert werden.
Jobs
In den nachfolgenden Unterkapiteln werden die Jobs f√ºr qs-fzg beschrieben. Der Ablauf der Jobsteuerung kann dem Kapitel 7 Ablaufsicht entnommen werden. 
Datenmodell: In den folgenden Unterkapiteln werden Tabellen und Attribute genannt, die im Datenmodell gelistet sind. Das Datenmodell (Tool-Export) [13] wird zusammen mit den Software-Lieferungen zur Verf√ºgung gestellt.	
Die Ablaufsicht aller Jobs inklusive ihrer Namen, Ausf√ºhrungsintervalle und Abh√§ngigkeiten ist im Bookkeeper-Graph dargestellt. Siehe Anhang [BK-Graph].
[DSJ-QSFzG-0010] BatchImportFzgStammdaten (LH-QSAV-4137, -4138 und -4593)
Paket: de.tollcollect.zme.qs.fzg.masterdata
Die Scala-Klasse BatchImportFzgStammdaten, behandelt die Persistierung von FzG-Stammdaten√§nderungen, sowie der Persistierung neuer FzG-Stammdaten. Sie enth√§lt die Logik zum FULL-Import der  FzG-Stammdaten aus der Quelltabelle der DM Oracle DB View. Der Job ist wie folgt implementiert: 

Abbildung 26: Job-Ablauf (schematisch): BatchImportFzgStammdaten
 Trigger: Der FULL-Import wird 1x t√§glich vom Bookkeeper gestartet.
Ablauf
Der Zugriff auf die FzG-Stammdaten erfolgt mittels Oracle JDBC-Treiber. Folgende Schritte werden durchgef√ºhrt:
Laden der Daten aus der DM Oracle View in einen Spark Dataframe
Mapping und Transformation der Daten der Quelltabelle FZG_STAMMDATEN_VIEW auf die Struktur der Zieltabelle lds_dm_fzg_stammdaten 
Speicherung der transformierten Stammdaten in der Cassandra-Datenbank. 
Wichtig: Je FzG existiert ein Eintrag in der Cassandra-Zieltabelle lds_dm_fzg_stammdaten. Beim FULL-Import werden die Daten aller vorhandenen FzG √ºberschrieben (PRIMARY KEY ehk_id), neue FzG werden automatisch hinzugef√ºgt. In der Zieltabelle werden alle FzG seit dem ersten Import-Zeitpunkt gef√ºhrt, also ggf. auch nicht mehr eingesetzte FzG. Der Status der FzG ist in der Spalte status_usecycle vermerkt (m√∂gliche Werte: 0 = Eingebaut, 1 = Ausgebaut, 2 = Im Einbau, 3 = Einbauabbruch).
Die Persistierung der zus√§tzlich von DM via LDS-Client √ºbertragenen Stammdaten-Events (Einbau-Events) erfolgt in derselben Cassandra-Zieltabelle, die auch f√ºr die FULL-Importe verwendet wird (lds_dm_fzg_stammdaten). Siehe dazu auch Kapitel 6.1.3 SST 830 Device Management (DM) - QS-AV.
Konfiguration
Der Zugriff √ºber den Oracle JDBC-Treiber wird in folgenden Umgebungsvariablen im Rahmen der Erstellung der Deployments konfiguriert:
Variable
Beschreibung
QSAV_EXTERNAL_LDS_DM_ORACLE_JDBC_CONNECTION_URI
URI der Oracle-DB
QSAV_EXTERNAL_LDS_DM_ORACLE_SCHEMA
Das zu verwendende Oracle Schema

Tabelle 40: BatchImportFzgStammdaten ‚Äì Umgebungsvariablen
[DSJ-QSFzG-0020] FsmNormierung (LH-QSAV-3831, -4118, -4119, -4091, -4082)
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job FsmNormierung dient der Normierung der √ºber den LDS-Client eingehenden FSM (Fahrzeugger√§te-Statusmeldungen). Die Normierung liefert vier Ergebnisse: 
Erfassung von FSM Zustands√§nderungen: Der Zustandswechsel einer FSM wird durch Vergleich der FSM mit ihrem direkten Vorg√§nger ermittelt und gespeichert.
Erfassung von Regelbits: Auswertung des FSM und Setzen der Regelbits (boolean-Werte). 
Aktualisierung der FSM in der Cassandra-Tabelle: Aktualisierung vorhandener Eintr√§ge in der Cassandra-Tabellen fzg_fsm_eingang, um sicherzustellen, dass zu jeder Zeit nur ein aktueller Eintrag pro FzG existiert. 
Anonymisierung und L√∂schung: Daten der Ausgangstabellen werden im Zuge der Normierung anonymisiert und mit TTLs versehen. Hierzu wird ein DataFrame geteilt und mit unterschiedlichen TTLs nach Cassandra geschrieben.
Wichtig: Das Sub-DataFrame muss immer den Prim√§rschl√ºssel der Zieltabelle enthalten.

Der Job ist in der Klasse FsmNormierung, wie folgt implementiert:

Abbildung 27: Job-Ablauf (schematisch): FsmNormierung
 Trigger: Die FSM-Normierung wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
Einlesen der FSM des aktuellen 15-Minuten-Timebuckets aus der Quelltabelle (lds_es_en_q_fsm).
FzG-spezifisch (in Abh√§ngigkeit der EHK-Id) pr√ºfen, ob mit den eingelesenen FSM Zustands√§nderungen gegen√ºber der letzten in QS-AV gespeicherten erfolgt sind. 
F√ºr FSM mit Zustands√§nderung: Ermittlung der Regelbits im Rahmen der FSM-Normierung  
Speicherung der Ergebnisse in den drei Ausgangstabellen:
fzg_fsm_zustandswechsel: Speicherung des Zustandswechsels f√ºr jede ge√§nderte FSM (attributweise). 
fzg_fsm_normalisierung_ergebnis: Speicherung der Ergebnisse der Normierung f√ºr jede ge√§nderte FSM.
fzg_fsm_eingang: Speicherung bzw. Aktualisierung des Eingangszeitpunkts der ersten und der letzten FSM eines Fahrzeugger√§ts. 
L√∂schung des verarbeiteten Timebuckets in der LDS-Inputtabelle (lds_es_en_q_fsm).
Allgemeine Hinweise
Im Zuge der FSM-Normierung wird eine Anonymisierung und L√∂schung der Zeitstempel-Attribute mittels TTLs durchgef√ºhrt:
fzg_fsm_zustandswechsel: 
Der Zeitstempel wird nach einer konfigurierbaren Anzahl an Sekunden (zustandswechsel_day_anonymization_interval_sec) auf den Tag  aggregiert (0 Uhr). Siehe auch Konfiguration.
Der Zeitstempel wird nach einer konfigurierbaren Anzahl an Sekunden (zustandswechsel_year_anonymization_interval_sec) auf das Jahr aggregiert (1.Januar 0 Uhr). Siehe auch Konfiguration.
fzg_fsm_normalisierung_ergebnis: 
Der Zeitstempel wird nach einer konfigurierbaren Anzahl an Sekunden (ergebnis_day_anonymization_interval_sec) auf den Tag aggregiert (0 Uhr). Siehe auch Konfiguration.
Der Zeitstempel wird nach einer konfigurierbaren Anzahl an Sekunden (ergebnis_deletion_interval_sec) gel√∂scht. Siehe auch Konfiguration.
Konfiguration
Die Konfiguration erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
fzg.sw.aktuelle_version
Unconfigured
Versionsbezeichnung der aktuellsten produktiv ausgerollten EtcApp; hier ben√∂tigt f√ºr die Ermittlung, ob die in der FSM angegebene Version der aktuellen entspricht
fzg.fsm_normierung.ergebnis_day_
anonymization_interval_sec
3628800
Sekunden nach dem der Zeitstempel in lds_dm_data_primary_fsm_ergebnis gel√∂scht wird.
fzg.fsm_normierung.ergebnis_deletion_
interval_sec
10368000
Sekunden nach dem Eintr√§ge in lds_dm_data_primary_fsm_ergebnis gel√∂scht werden.
fzg.fsm_normierung.zustandswechsel_
day_anonymization_interval_sec
3628800
Sekunden nach dem Eintr√§ge in lds_dm_data_primary_fsm_ergebnis gel√∂scht werden.
fzg.fsm_normierung.zustandswechsel_
year_anonymization_interval_sec
31536000
Sekunden nach dem der auf den Tag gerundete Zeitstempel in lds_dm_data_primary_fsm_zustandswechsel gel√∂scht wird.

Tabelle 41: FSMNormierung ‚Äì Konfiguration in Tabelle tech_config
[DSJ-QSFzG-0030] MdnParser (LH-QSAV-4140, -4220, -4144)
Paket: de.tollcollect.zme.qs.fzg.rule
Das Parsen und Dekodieren der Monitoring-Nachrichten (MDN) ist in der Klasse MdnParser implementiert. 

Abbildung 28: Job-Ablauf (schematisch): MdnParser
 Trigger: Der MdnParser wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
Einlesen der Monitoring-Nachrichten (MDN) f√ºr das jeweilige Timebucket aus der Tabelle lds_dm_en_q_mdn. 
Dekodieren der Detailinformationen  je Monitoring-Nachricht, die als base64-kodierter String im Attribut daten_base64 geliefert werden.
Speichern der MDN mit den dekodierten Informationen in der Tabelle fzg_dm_mdn. Dabei werden Formate wie Zeitangaben etc. an die QS-AV-Datenformate angeglichen.
[DSJ-QSFzG-0040] MdnNormalizer (LH-QSAV-4140, -4220, -4144)
Paket: de.tollcollect.zme.qs.fzg.rule
Die Normierung inklusive der Anwendung der Normierungsregeln ist in der Klasse MdnNormalizer, implementiert.

Abbildung 29: Job-Ablauf (schematisch): MdnNormaliser
 Trigger: Der MdnNormaliser wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
Einlesen der durch den MDNParser aufbereiteten neuen und ge√§nderten MDN-Daten der Tabelle fzg_dm_mdn.
Zwischenspeichern der MDN-Daten in der Hilfstabelle fzg_mdn_normalisieurng_intermediate_prv f√ºr die Durchf√ºhrung der Normalisierung.
Anwenden der Normalisierungsregeln auf die zwischengespeicherten Daten. Dazu werden u.a. zum Teil herstellerbezogene Schwellwertpr√ºfungen durchgef√ºhrt. F√ºr die Durchf√ºhrung der Normalisierung werden Daten aus den folgenden beiden Hilfstabellen herangezogen:
fzg_hersteller
tech_rule_schwellwert
Speichern der Ergebnisse in der Tabelle fzg_mdn_normalisierung_ergebnis
Entfernen von Eintr√§gen aus der Hilfstabelle fzg_mdn_normalisieurng_intermediate_prv.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
fzg.mdn.1_batt
100
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 1 (Grundig).
fzg.mdn.2_batt
101
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 2 (Siemens).
fzg.mdn.3_batt
102
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_batt
103
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_batt
104
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_batt
105
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_batt
106
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_batt
107
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_batt
108
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_batt
110
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_batt
111
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_batt
112
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_batt
113
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_batt
114
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_capa_flash
200
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 1 (Grundig).
fzg.mdn.2_capa_flash
201
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 2 (Siemens).
fzg.mdn.3_capa_flash
202
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_capa_flash
203
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_capa_flash
204
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_capa_flash
205
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_capa_flash
206
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_capa_flash
207
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_capa_flash
208
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_capa_flash
210
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_capa_flash
211
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_capa_flash
212
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_capa_flash
213
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_capa_flash
214
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 1 (Grundig).
fzg.mdn.2_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 2 (Siemens).
fzg.mdn.3_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 1 (Grundig).
fzg.mdn.2_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 2 (Siemens).
fzg.mdn.3_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 1 (Grundig).
fzg.mdn.2_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 2 (Siemens).
fzg.mdn.3_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 14 (Bosch 2G Pilot).

Tabelle 42: MdnNormalizer  - Konfiguration in Tabelle tech_rule_schwellwert
[DSJ-QSFzG-0050] FzgVerdichtung (LH-QSAV-3520, -4239, -3184, -4366)
Paket: de.tollcollect.zme.qs.fzg.rule	
Der Job FzgVerdichtung hat die Aufgabe FzG-bezogene Daten aus der FSM-Normierung, MDN-Normierung und DM-Fehlerevents auf Basis aller Timebuckets eines Tages zu verdichten. Die Korrelation wird im gleichen Zug durchgef√ºhrt, jedoch nicht als Zwischenergebnis persistiert.
Die folgende Abbildung zeigt schematisch die Implementierung des Jobs.

Abbildung 30: Job-Ablauf (schematisch): FzgVerdichtung
 Trigger: Die FzG-bezogene Verdichtung (FzgVerdichtung) wird 1x pro Tag durch den Bookkeeper gestartet.
Ablauf
Laden der Daten aus den Quelltabellen lds_dm_en_q_fehler, fzg_fsm_normalisierung_ergebnis und fzg_mdn_normalisierung_ergebnis.
Durchf√ºhrung der Korrelation f√ºr die MDN-Ergebnisse und DM-Fehler
Aggregation der Daten
Zusammenf√ºhrung der Aggregationsergebnisse aller Quelltabellen (Verdichtung)
Speicherung der Verdichtung in den Zieltabellen fzg_verdichtung_ergebnis und fzg_verdichtung_ergebnis_by_ehk_id
[DSJ-QSFzG-0060] FzgVerdichtungPopulation  (LH-QSAV-3520, -4239, -3184, -4366)
Paket: de.tollcollect.zme.qs.fzg.rule	
Der Job FzgVerdichtungPopulation erf√ºllt im Wesentlichen dieselben Aufgaben wie der Job FzgVerdichtung mit dem Unterschied, dass die Verdichtung nicht f√ºr einzelne FzG, sondern gruppiert  nach den FzG-Typen erfolgt (=plattformspezifische Verdichtung).
Die folgende Abbildung zeigt schematisch die Implementierung des Jobs.

Abbildung 31: Job-Ablauf (schematisch): FzGgVerdichtungPopulation
 Trigger: Die FzG-Populations-bezogene Verdichtung wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
Laden der Daten aus den Quelltabellen lds_dm_en_q_fehler, fzg_fsm_normalisierung_ergebnis und fzg_mdn_normalisierung_ergebnis.
Ermittlung des FzG-Typs aus der Seriennummer und Speicherung in Spalte fzg_typ 
Durchf√ºhrung der Korrelation f√ºr die MDN-Ergebnisse und DM-Fehler
Aggregation der Daten
Zusammenf√ºhrung der Aggregationsergebnisse aller Quelltabellen (Verdichtung)
Speicherung der Verdichtung in den Zieltabellen fzg_verdichtung_pop_all_by_zeit, fzg_verdichtung_pop_fzg_typ und fzg_verdichtung_pop_fzg_typ_by_fzg_typ
Allgemeine Hinweise
Die Verdichtung basiert auf dem Timebucket und nicht dem Zeitstempel der Daten. Der Job bekommt vom Bookkeeper nur das Anfangs-Timebucket und berechnet selbst alle 96 Timebuckets f√ºr den ganzen Tag. Die Timebucket-Gr√∂√üe von 15 Minuten ist im Job fest hinterlegt.
[DSJ-QSFzG-0070] FspProcessor
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job FspProcessor dient der Verarbeitung der aus DM in QS-AV eingehenden FSP-Eingangs-Events (Fahrspurdatei-Eingangs-Events). F√ºr jedes FzG soll der Zeitstempel des letzten Empfangs einer FSP-Datei persistiert werden. Der zuvor letzte Zeitstempel wird dabei √ºberschrieben. Die FSP-Datei selbst wird nicht √ºbertragen.

Abbildung 32: Job-Ablauf (schematisch): FspProcessor
 Trigger: Der Job wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
Einlesen der FSP-Eingangs-Events (15min Timebucket) aus lds_dm_en_q_fsp.
Ermittlung des aktuellsten Eintrags pro EHK-Id (FzG) und speichern/√ºberschreiben in  fzg_fsp_eingang. (Es existiert immer nur ein Eintrag pro EHK-Id in der Tabelle fzg_fsp_eingang).
L√∂schen aller Daten des bearbeiteten Timebucket aus lds_dm_en_q_fsp.
[DSJ-QSFzG-0080] FzgBewerter
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job FzgBewerter bewertet die Ergebnisse der t√§glichen FzG-Verdichtung, in der das Auftreten definierter Fehlerbilder (wie z.B. Geringe Batteriespannung) je EHK-Id gez√§hlt wird. Im Rahmen der Bewertung wird anhand der konfigurierten Schwellwerte beurteilt, ob die H√§ufigkeit mit der ein bestimmtes Fehlerbild auftritt, eine Auff√§lligkeit darstellt. 
Das Ergebnis der Bewertung wird pro FzG-Typ verdichtet. Hierbei wird die H√§ufigkeit der Fehlerbilder f√ºr jeden FzG-Typ gez√§hlt. 
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 33: Job-Ablauf (schematisch):  FzgBewerter
 Trigger:  Der FzgBewerter-Job wird 1x am Tag vom Bookkeeper getriggert.
Ablauf
Die Bewertung wird je Fehlerbild folgenderma√üen durchgef√ºhrt:
Laden der Daten der FzG-Verdichtung. Je Fehlerbild werden dabei die aktiven Tage betrachtet, die via Schwellwert konfiguriert sind (fzg.bewertung.<Fehlerbild>.aktiveTage)  
F√ºr jeden aktiven Tag pro Fehlerbild wird gepr√ºft, ob der Fehlerwert >= dem Schwellwert f√ºr die min. Anzahl an fehlerhaften Tagen ist (fzg.bewertung.<Fehlerbild>.fehler). In diesen F√§llen wird der Tag als ‚Äûauff√§lliger Tag‚Äú bewertet. 
Pr√ºfung ob die Anzahl der auff√§lligen aktiven Tage >= dem Schwellwert der min. Anzahl an fehlerhaften Tagen im Zeitraum der aktiven Tage ist (fzg.bewertung.<Fehlerbild>.aktiveTageFehler). Sofern dies gegeben ist, wird das Fehlerbild in der Verdichtung als positiv (TRUE = auff√§llig) gewertet, andernfalls als negativ (FALSE = nicht auff√§llig)
Verdichtung der FzG-Bewertung pro FzG-Typ durch Aufsummierung der Fehlerbilder.
Speicherung der Ergebnisse in folgenden Cassandra-Tabellen:
fzg_bewertung_ergebnis (Tabelle f√ºr die Bewertungsergebnisse pro EHK-Id)
fzg_bewertung_ergebnis_fehlerfrei (Tabelle die je EHK-Id dokumentiert, ob mehr als ein Fehlerbild mit TRUE bewertet wurde)
fzg_bewertung_verdichtung_pop (Verdichtungen pro fzg_typ)
Allgemeine Hinweise
In der Ausgangstabelle der Verdichtung der FzG-Bewertung fzg_bewertung_verdichtung_pop kennzeichnet der Wert "alle" eine Verdichtung √ºber alle FzG-Typen und der Wert "kein" eine Verdichtung √ºber Eintr√§ge ohne FzG-Seriennummer (da sich der FzG-Typ aus dieser ableitet).
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert. Sie setzen sich zusammen aus 19 FzG-Fehlerbildern zu je folgenden 3 Kategorien:
Kategorie aktiveTage: Anzahl der aktiven Tage (aktiver Tag = Eintrag in FzG-Verdichtungstabelle existiert), die pro ehk_id bei der Bewertung ber√ºcksichtigt werden
Kategorie fehler: Min. Anzahl an Fehlern pro aktivem Tag, damit der Tag als fehlerhaft bewertet wird
Kategorie aktiveTageFehler: Min. Anzahl an fehlerhaften Tagen im Zeitraum der aktiven Tage, damit die FzG-Bewertung das Fehlerbild mittels TRUE als ‚Äûauff√§llig‚Äú bewertet. 
Die Schwellwert-Keys die sich daraus ergeben haben folgendes Muster:
fzg.bewertung.<Fehlerbild>.<Kategorie>
Die folgende Tabelle enth√§lt alle Defaultwerte f√ºr die Schwellwerte (Mapping von Fehlerbild auf Kategorie):
Fehlerbild
aktiveTage
fehler
aktiveTageFehler
fallback
1
1
1
dienst_de_aktiv_defekt
1
1
1
sw_version_nicht_aktuell
1
1
1
batt_spannung_gering
5
1
3
dauerplus
1
1
1
keine_tachodaten
5
1
4
keine_gyrodaten
5
1
4
tachodaten_unplausibel
5
1
4
gyrodaten_unplausibel
5
1
4
capa_flash
5
1
4
uw_errorflag
5
1
4
probefahrt_nicht_bestanden
5
1
1
ehk_fehler
5
1
4
gps_fehler
5
1
4
zeitsprung
5
1
4
wtr_sprung
5
1
4
sat_fs
5
1
4
mac_fehler
1
1
1
loop_fehler
1
1
1

Tabelle 43: FzgBewerter - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Ist der aus dem Start-Timebucket abgeleitete Verarbeitungstag ung√ºltig, da die Zeitangabe nicht Mitternacht entspricht, wird eine IllegalArgumentException geworfen und der Job abgebrochen.
Ist einer der drei Schwellwerte je Fehlerbild ung√ºltig, wird das Fehlerbild nicht bewertet (Wert null in Ergebnistabelle) und eine Warnung geloggt (Applikationslog). Folgendes wird als ‚Äûung√ºltig‚Äú angesehen:
Schwellwert ist nicht vorhanden
Schwellwert ist keine Ganzzahl
Schwellwert ist kleiner 1
[DSJ-QSFzG-0090] DeklarationEventsNormierung
Paket: de.tollcollect.zme.qs.fzg.rule
Der Job DeklarationEventsNormierung ist f√ºr die Persistierung und Normierung der via SST 833 von ES √ºbermittelten Deklarations-Events zust√§ndig.
Persistierung
Je nach Art des Deklarations-Events (Attribut: deklarationsart) gelten unterschiedliche Persistierungsregeln:
initial: Werden historisiert abgespeichert
aktiv: Werden historisiert abgespeichert
passiv: D√ºrfen NICHT historisiert abgespeichert werden. Pro FzG und Event-Typ darf es nur einen Wert geben, alte Werte werden von neuen √ºberschrieben.
Hinweis: Die Eingangsdaten in der LDS-Tabelle lds_es_en_q_deklaration_events werden via TTL nach 24 Stunden gel√∂scht.
Normierung: Plausibilit√§tspr√ºfung der passiven Events / Setzen von Regelbits
Passive Deklarations-Events werden auf Plausibilit√§t gepr√ºft. Die Ergebnisse der Pr√ºfung werden persistiert.
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 34: Job-Ablauf (schematisch): DeklarationEventsNormierung
Trigger:  Der DeklarationEventsNormierung-Job wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
Einlesen der Timebuckets gem√§√ü Konfiguration im Job-Schema aus der Eingangstabelle lds_es_en_q_deklaration_events.
Persistierung der Deklarations-Events in folgenden Tabellen:
fzg_deklaration_events_initial: Historisierte Speicherung der Deklarations-Events  mit deklarationsart=initial
fzg_deklaration_events_aktiv: Historisierte Speicherung der Deklarations-Events  mit deklarationsart=aktiv
fzg_deklaration_events_passiv: Speicherung des jeweils neuesten Deklarations-Events mit deklarationsart=passiv je FzG-Seriennummer und Event-Typ
√úberpr√ºfung der Plausibilit√§t der Deklarations-Events:
Die Ergebnisse der Plausibilit√§tspr√ºfung werden mit FzG-Seriennummer und FzG-Typ aus der Tabelle lds_dm_fzg_stammdaten angereichert, sofern vorhanden.
Es werden folgende Regelbits berechnet:
rb_a_p_inkonsistent: Abweichung des deklarierten Werts zum letzten aktiven Event
rb_p_p_inkonsistent: Abweichung des deklarierten Werts zum letzten passiven Event
rb_dekl_p_gewicht_ohne_achs: Passive Gewichtsdeklaration ohne nachfolgende passive Achsdeklaration (in konfigurierbarem Zeitintervall)
Speicherung der Ergebnisse in der Zieltabelle fzg_deklaration_events_interpretation.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
fzg.deklaration_events.interpretation.dekl_
p_gewicht_ohne_achs.max_zeitdifferenz_
sekunden
10000
Maximale Zeitdifferenz in Sekunden zwischen passivem Gewichtsdeklarations-Event und passivem Achsdeklarations-Event, damit das Regelbit rb_dekl_p_gewicht_ohne_achs den Wert FALSE hat. 
Beachte: Obere Grenze nicht eingeschlossen.
Wichtig: Dieser Schwellwert darf nicht gr√∂√üer sein als die Timebucket-Gr√∂√üe der Eingangstabelle lds_es_en_q_ deklaration_events. Andernfalls muss die Bookkeeper-Konfiguration angepasst werden. 

Tabelle 44: DeklarationEventsNormierung ‚Äì Konfiguration in Tabelle tech_rule_schwellwert
Wichtige Hinweise
Der Job wird alle 15 Minuten vom Bookkeeper gestartet. Der Start erfolgt jedoch um 15 Minuten versetzt, weil f√ºr das Regelbit rb_dekl_p_gewicht_ohne_achs in die Zukunft geschaut werden muss. Der Job erh√§lt somit immer ein Timebucket mehr (neuer) als er zu verarbeiten hat. Es gibt  somit ein Sliding-Window. Das neuste Timebucket wird nicht verarbeitet, sondern dient nur zur Pr√ºfung des Regelbits. Das Sliding-Window kann √ºber die Bookkeeper-Konfiguration angepasst werden.

Beispiele 
1. Timebucket-Gr√∂√üe entspricht Job-Aufrufintervall:
Job bekommt Timebuckets T1, T2 und verarbeitet T1     
Job bekommt Timebuckets T2, T3 und verarbeitet T2     
Job bekommt Timebuckets T3, T4 und verarbeitet T3     
usw.
2. Timebucket- Gr√∂√üe ist 15 Minuten, Job-Aufrufintervall ist 1 Stunde: 
Job bekommt Timebuckets T1, T2, T3, T4, T5 und verarbeitet T1, T2, T3, T4
Job bekommt Timebuckets T5, T6, T7, T8, T9 und verarbeitet T5, T6, T7, T8   
Job bekommt Timebuckets T9, T10, T11, T12, T13 und verarbeitet T9, T10, T11, T12
usw.
Fehlerbehandlung
Sollte der Schwellwert fzg.deklaration_events.interpretation.dekl_p_gewicht_ohne_achs.max_zeitdifferenz_sekunden nicht gesetzt worden sein, wird eine Exception (SchwellwertException) geworfen.
[DSJ-QSFzG-0100] MonpFzgEventEmitter
Paket: de.tollcollect.zme.qs.fzg.rule.monp
Der Job MonpFzgEventEmitter erzeugt FzG-spezifische Metrik-Events f√ºr die Monitoring-Plattform (MonP) und reiht diese an die QS-AV-interne globale Versand-Warteschlange ein, bevor die Events asynchron per SST 819 an MonP √ºbertragen werden. 
Einen allgemeinen √úberblick √ºber die Schnittstelle und die beteiligten Programmteile finden Sie im Kapitel 6.1.6 SST 819 QS-AV ü°™ MonP unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Informationen zu den zu √ºbertragenen Events entnehmen Sie bitte der Schnittstellenspezifikation im Referenzdokument [8] [S_819_SST_00] SST 819 SST-Spezifikation QS-AV ‚Äì MonP.
Die folgende Tabelle listet die Input-Tabellen die f√ºr die Event-Erstellung durch den MonpFzgEventEmitter herangezogen werden.
Input-Tabelle
Events
fzg_verdichtung_pop_fzg_typ_by_fzg_typ 
Populationsanzahlen betreffend (z.B.: PopAnzDienstDeAktivDefekt)

Tabelle 45: Input-Tabellen der FzG-spezifischen Metrik-Events
Bei der Einreihung der Events in die Versand-Warteschlange werden folgende Output-Tabellen beschrieben, um den Versand durch den ScheduledMonpSender (siehe auch Kapitel 6.1.6 SST 819 QS-AV ü°™ MonP) zu erm√∂glichen:
Output-Tabelle
Aufgabe
tech_monp_pending_metric_event
Warteschlange zu versendender Events.

Tabelle 46: Output-Tabellen der FzG-spezifischen der Metrik-Events
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 35: Job-Ablauf (schematisch): MonpFzgEventEmitter
Trigger:  Der MonpFzgEventEmitter-Job wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
Einlesen der Eingangsdaten aus den oben genannten Input-Tabellen, zur Ermittlung des Event-Inhalts, der f√ºr das Monitoring verwendet werden soll (siehe Tabelle 45: Input-Tabellen der FzG-spezifischen Metrik-Events).
Generierung der Metrik-Events gem√§√ü Schnittstellenspezifikation. Daf√ºr werden die Daten in ein vorgegebenes JSON-Format eingebettet.
Einreihung der erstellten Events in die globale Versand-Warteschlange (Tabelle tech_monp_pending_metric_event). 
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
erh.monp.plattform_prefix
Plattform
Pr√§fix f√ºr den FzG Plattformnamen, der im Event-Inhalt von einigen MonP-Events verwendet wird. Beispiel: Der Pr√§fix "Plattform" wird mit dem Herstellernamen ‚ÄûBosch1G‚Äú zusammengesetzt zu "Plattform Bosch1G".

Tabelle 47: MonpErhEventEmitter ‚Äì Konfiguration in Tabelle tech_config
F√ºr allgemeine Konfigurationseinstellungen bez√ºglich der MonP-Schnittstelle siehe Kapitel 6.1.6 SST 819 QS-AV ü°™ MonP unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation. 
Wichtige Hinweise
Der Inhalt der Metrik-Events ist abh√§ngig von der Bef√ºllung der Tabelle fzg_hersteller. Die Spalte SENDEN_AN_MONP bestimmt, ob ein FzG-Typ als Gruppierungskriterium und somit Daten f√ºr diesen FzG-Typ in das Event mit aufgenommen werden oder nicht. Die Schreibweise des Herstellers muss in der Spalte HERSTELLER_NAME_MONP definiert werden. Ist diese Spalte nicht gesetzt (null), so wird anstelle dessen eine Zusammensetzung aus HERSTELLER_NAME_ANZEIGE und HERSTELLER_NAME verwendet. Dem ermittelten Herstellernamen wird in jedem Fall das plattform_prefix aus der Konfiguration vorangestellt, um auf die im Event verwendete Schreibweise der Keys zu kommen. Aus "Bosch1G" wird somit "Plattform Bosch1G" (siehe Konfiguration).
[DSJ-QSFzG-0110] CognosFzgMiscUpdater
Paket: de.tollcollect.zme.qs.fzg.rule.cognos
Der Job stellt FzG-bezogene Kennzahlen zur Anzahl der aktiven FzG, dem Erhebungsmodus (zME/dME) und zum Kontrollmodus f√ºr die Cognos Schnittstelle (SST 884) zur Erstellung des FzG-Reports bereit. 
Einen allgemeinen √úberblick √ºber die Schnittstelle und die beteiligten Programmteile finden Sie im Kapitel 6.1.7 SST 884 QS-AV ü°™ Cognos unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 36: Job-Ablauf (schematisch): CognosFzgMiscUpdater
Trigger:  Der CognosFzgMiscUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
Laden und Zusammenf√ºhren (Join) der FSM-Normierungsergebnisse (fzg_fsm_normalisierung_ergebnis) und der FzG-Stammdaten (lds_dm_fzg_stammdaten). Betrachtet werden dabei alle Daten aus dem konfigurierten Betrachtungszeitraum (siehe Konfigurations-Key fzg.cognos.datenumfang_tage_fzg_misc) ausgehend von den FSM-Normierungsergebnissen. 
Aggregation der Daten je Tag und FzG-Typ:
Anzahl aller FzG, die an diesem Tag eine FSM abgesetzt haben
Anzahl aller FzG im zME-Modus
Anzahl aller FzG im Kontrollmodus CCC_AKTIV
L√∂schen aller Daten aus der Zieltabelle tech_cognos_fzg_misc. 
(Hintergrund: Die Daten werden immer f√ºr den gesamten Betrachtungszeitraum neu generiert.)
Speicherung der neu ermittelten Daten in der Tabelle tech_congos_fzg_misc.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
fzg.cognos.datenumfang_tage_fzg_misc
5
Anzahl der Tage (r√ºckwirkend), f√ºr die FzG-Daten f√ºr Cognos bereitgestellt werden sollen; definiert den Betrachtungszeitraum.

Tabelle 48: CognosFzgMiscUpdater ‚Äì Konfiguration in Tabelle tech_config
[DSJ-QSFzG-0120] CognosFzgMapUpdater
 Paket: de.tollcollect.zme.qs.fzg.rule.cognos
Der Job stellt das Mapping zwischen FzG-Typencodes (z.B. 0202) und FzG-Typennamen f√ºr die Cognos Schnittstelle (SST 884) bereit. Das Mapping wird auf Seiten Cognos bei der Reportgenerierung ben√∂tigt, um nicht nur FzG-Typencodes sondern auch die FzG-Typennamen anzeigen zu k√∂nnen.
Einen allgemeinen √úberblick √ºber die Schnittstelle und die beteiligten Programmteile finden Sie im Kapitel 6.1.7 SST 884 QS-AV ü°™ Cognos unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar. 

Abbildung 37: Job-Ablauf (schematisch): CognosFzgMapUpdater
Trigger:  Der CognosFzgMapUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
Laden der FzG-Hersteller aus der Tabelle fzg_hersteller, die an Cognos √ºbertragen werden sollen. Sie sind in der Tabelle mit dem Flag senden_an_cognos versehen. 
L√∂schen aller Daten aus der Zieltabelle tech_cognos_fzg_map. Es handelt sich bei der Tabelle um eine Mappingtabelle, bei der immer nur der aktuellste Stand ben√∂tigt wird.
Erweiterung des Herstellernamens (Attribut hersteller_name_cognos) um das konfigurierte Pr√§fix (siehe Konfigurations-Key fzg.cognos.fzg_plattform_prefix). 
Speicherung des Mappings in der Tabelle tech_congos_fzg_map.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
fzg.cognos.fzg_plattform_prefix
Plattform
Plattform-Pr√§fix f√ºr FzG-Herstellernamen, der bei der Reportgenerierung auf Seiten Cognos verwendet wird.

Tabelle 49: CognosFzgMiscUpdater ‚Äì Konfiguration in Tabelle tech_config
 [DSJ-QSFzG-0130] CognosFzgStatusFehlerUpdater
Paket: de.tollcollect.zme.qs.fzg.rule.cognos
Der Job stellt Kennzahlen zu FzG-bezogenen Fehlerbildern (zME) f√ºr die Cognos Schnittstelle (SST 884) bereit. 
Einen allgemeinen √úberblick √ºber die Schnittstelle und die beteiligten Programmteile finden Sie im Kapitel 6.1.7 SST 884 QS-AV ü°™ Cognos unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
 
Abbildung 38: Job-Ablauf (schematisch): CognosFzgStatusFehlerUpdater
Trigger:  Der CognosFzgStatusFehlerUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
Laden der Kennzahlen zu FzG-bezogenen Fehlerbildern aus der Tabelle fzg_bewertung_verdichtung_pop f√ºr den konfigurierten Betrachtungszeitraum (definiert durch Konfigurations-Key fzg.cognos.datenumfang_tage_fzg_status_fehler).
L√∂schen aller Daten aus der Zieltabelle tech_cognos_fzg_status_fehler. 
(Hintergrund: Die Daten werden immer f√ºr den gesamten Betrachtungszeitraum neu generiert.)
Speicherung der neu ermittelten Daten in der Tabelle tech_congos_fzg_status_fehler.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
fzg.cognos.datenumfang_tage_fzg_status_fehler
5
Anzahl der Tage (r√ºckwirkend), f√ºr die Kennzahlen zu FzG-Fehlerbilder f√ºr Cognos bereitgestellt werden sollen; definiert den Betrachtungszeitraum.

Tabelle 50: CognosFzgStatusFehlerUpdater ‚Äì Konfiguration in Tabelle tech_config
Anonymisierung, Archivierung und L√∂schung 
Die Anonymisierung, Archivierung und L√∂schung von personenbezogenen Daten der Komponente qs-fzg erfolgt via TTLs (Time-To-Live) und durch den Job FzGDeletion. 
Vorgaben und Job-Details sind dem Systeml√∂schkonzept (Referenzdokument [14]) zu entnehmen.

qs-erh: Qualit√§tssicherung der Erhebung
Die Subkomponente qs-erh enth√§lt die Anwendungslogik inklusive der Qualit√§tssicherungsalgorithmen f√ºr die Qualit√§tssicherung der Erhebung. Die Implementierung ist komplett in Scala gehalten und erfolgt innerhalb einzelner Jobs, die durch den Bookkeeper gesteuert werden.
Jobs
In den nachfolgenden Unterkapiteln werden die Jobs f√ºr qs-fzg beschrieben. Der Ablauf der Jobsteuerung kann dem Kapitel 7 Ablaufsicht entnommen werden. 
Datenmodell: In den folgenden Unterkapiteln werden Tabellen und Attribute genannt, die im Datenmodell gelistet sind. Das Datenmodell (Tool-Export) [13] wird zusammen mit den Software-Lieferungen zur Verf√ºgung gestellt.
Die Ablaufsicht aller Jobs inklusive ihrer Namen, Ausf√ºhrungsintervalle und Abh√§ngigkeiten ist im Bookkeeper-Graph dargestellt. Siehe Anhang [BK-Graph].
[DSJ-QSErh-0010] BetriebsdatenRestImporter 
Paket: de.tollcollect.zme.qs.erh.rule.betriebsdaten
Im Zuge des Betriebsdaten-Imports werden die von GDBS bereitgestellten Betriebsdaten-Versionen in die QS-AV Datenbank importiert.  Die Daten werden gem√§√ü [4] [S_814_SST_00] SST 814 SST Spezifikation GDBS ‚Äì QS-AV per REST-Schnittstelle abgerufen. 
Folgende Betriebsdaten werden importiert:
Tarifabschnittliste
Abschnittssequenzen
Erkennungsmodell
Release-Historie
Release-Notes
Baustellen
  
Abbildung 39: Job-Ablauf (schematisch): BetriebsdatenRestImporter
 Trigger: Der Betriebsdaten-REST-Import wird alle 15 Minuten vom Bookkeeper getriggert.
Ablauf
URL f√ºr REST-Service aus der Umgebungsvariable QSAV_GDBS_URL ermitteln.
Folgendes wird f√ºr jeweils alle Instanzen (PRODUKTION, VALIDIERUNG) ausgef√ºhrt:
Release-Historie herunterladen und in Cassandra DB speichern (erh_gdbs_betriebsdatenhistorie)
Fehlende Produkte aus erh_gdbs_betriebsdaten_importer_state ermitteln.
Fehlende Produkte via REST-Service abrufen, Daten auf Cassandra-Tabellen mappen und persistieren in Tabellen: 
erh_gdbs_release_notes
erh_gdbs_mautobjekt
erh_gdbs_mautpunkt
erh_gdbs_abschnittssequenz
erh_gdbs_erkennungsobjekte
Sofern eine BD-Version mit allen zugeh√∂rigen Tarifabschnittslisten erfolgreich importiert wurde: 
MonP-Information-Event in MonP-Warteschlange schreiben (Tabelle tech_monp_pending_information_event).
√Ñnderungshistorie in Tabelle erh_mo_aenderungsstatus schreiben (ge√§nderte Mautobjekte je BD-Version).
Importer-Status in Cassandra DB aktualisieren: erh_gdbs_betriebsdaten_importer_state
Aktualit√§t der Baustellenliste anhand Zeitstempel per REST-Call pr√ºfen und bei Bedarf neue Baustellenliste via REST-Service abrufen und Daten in Cassandra DB persistieren (erh_gdbs_baustelle und erh_gdbs_baumassnahmeart).
Allgemeine Hinweise
F√ºr das Caching der Betriebsdaten in der Applikation zur Performance-Optimierung der Regel-Zugriffe werden die vorhandenen Spark Caching-Mechanismen (LAZY caching) eingesetzt.
Konfiguration 
Die Konfiguration erfolgt mittels Umgebungsvariablen. Weitere Details sind im Referenzdokument [16] [A_QSAV_HBI_00] QS-AV Installationshandbuch beschrieben.
Umgebungsvariable
Beschreibung
QSAV_GDBS_URL
URL zum GDBS-Rest-Service (SST 814 GDBS ‚Äì QS-AV)
z.B. http://tem2r68fd.e2e.ux.tc.corp:8443/rest
QSAV_GDBS_USERNAME
Benutzername f√ºr die Anmeldung am GDBS-Rest-Service
QSAV_GDBS_PASSWORD
Passwort zur Anmeldung am GDBS f√ºr den GDBS-Benutzer (QSAV_GDBS_USERNAME)
QSAV_GDBS_ABRUFER
Feld ‚ÄòAbrufer‚Äô f√ºr den Aufruf des GDBS-Rest-Service 
(SST 814 GDBS ‚Äì QS-AV)
z.B. QSAV
GDBS_IGNORE_CERTIFICATE
GDBS-Rest-Service (TLS, SST 814 GDBS ‚Äì QS-AV):
true: G√ºltigkeit des Zertifikats wird nicht gepr√ºft
false: G√ºltigkeit des Zertifikats wird gepr√ºft 
QSAV_EXTRA_CERTIFICATES
Semikolon-separierte Liste mit (Aussteller-) Zertifikatsdateien, die zur Pr√ºfung des GDBS-Serverzertifikats verwendet werden. Relevant, wenn der Parameter GDBS_IGNORE_CERTIFICATE=false ist.
Format:
‚Äö/apps/secret-volume/cert1.p12;/apps/secret-volume/cert2.p12;<etc.>‚Äô

Tabelle 51: BetriebsdatenRestImporter ‚Äì Konfiguration der Umgebungsvariablen
Weitere Betriebsdaten 
Bei folgenden Tabellen handelt es sich um Tabellen mit Basisdaten, die mit den Initialskripten in die Datenbank eingespielt werden: erh_mo_monitoring_parameter, erh_mo_mautobjektklasse, erh_mo_fahrleistungsklassen, erh_mo_ausnahme, erh_mo_regel
Folgende Tabelle dient dem Fachbereich dazu, sp√§ter manuell zus√§tzliche Mautobjektklassen hinzuzuf√ºgen: erh_mo_mautobjekt_mautobjektklasse
Fehlerbehandlung
Im Fehlerfall werden alle bislang konsistent eingelesenen Daten persistiert und eine entsprechende Fehlermeldung geloggt (Applikationslog). Die betroffene Betriebsdaten-Version gilt als nicht erfolgreich importiert (entsprechender Status in erh_gdbs_betriebsdaten_importer_state) und wird beim n√§chsten Import erneut bearbeitet.
[DSJ-QSErh-0011] GdbsProductsCreator
Paket: de.tollcollect.zme.qs.erh.rule.betriebsdaten
Dieser Job ist f√ºr die Erstellung der via SST 814 von QS-AV an GDBS √ºbermittelten Produktdokumente zust√§ndig (siehe auch Kapitel 6.1.2 SST 814 Grunddaten Bereitstellungs-Server (GDBS) ü°®ü°™  QS-AV).
Nach Erstellung reiht der GdbsProductsCreator die Produkte in eine FIFO-Warteschlange in der Datenbank ein. Der eigentliche Upload nach GDBS erfolgt asynchron durch den Job [DSJ-QSErh-0012] GdbsUploadPump.
In der aktuellen Implementierung werden folgenden Produkte erstellt:
Auff√§lligkeiten (Modellauff√§lligkeiten)
Fahrleistungen (Fahrleistungsklassen der Abschnitte)
Hinweis: Die in der SST-Spezifikation festgelegten L√ºckenschlussvorschl√§ge werden (noch) nicht erstellt.
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 40: Job-Ablauf (schematisch): GdbsProductsCreator
 Trigger: Die Erstellung der ausgehenden GDBS-Produkte wird 1x t√§glich vom Bookkeeper getriggert.
Ablauf
Die Erstellung der Produkte wird innerhalb des Jobs nacheinander ausgef√ºhrt.
Auff√§lligkeiten
Mit Hilfe des Konfigurationswerts erh_gdbs_auffaelligkeiten_document_creation_interval_days und des letzten gespeicherten Produkterstellungszeitpunkts (Tabelle erh_tech_gdbs_product_creation_state) pr√ºfen, ob das Produkt f√ºr die Auff√§lligkeiten in diesem Job-Durchlauf erstellt werden soll. 
Falls nein, die weiteren Schritte im Abschnitt  √ºberspringen. 
Produktdokument f√ºr die Auff√§lligkeiten anhand der Ergebnisse der Modellbewertung (aus Tabelle erh_modellbewertung) f√ºr alle Nutzungstage seit dem letzten Erstellungszeitpunkt generieren.
Das erstellte Produktdokument als ausstehenden Produkt-Upload in die Datenbank-Queue f√ºr die asynchrone Verarbeitung durch den Job GdbsUploadPumpe einreihen (Tabelle erh_tech_gdbs_pending_uploads). 
Hinweise: Das Produktdokument wird dabei mit einer UUID versehen. Der Inhalt des Produktdokuments wird als XML serealisiert und in der Spalte content als BLOB gespeichert.
Den letzten Produkterstellungszeitpunkt in der Datenbank protokollieren (Tabelle erh_tech_gdbs_product_creation_state).
Fahrleistungen
Hinweis: Das Produkt ‚ÄûFahrleistungen‚Äú soll immer am Monatsletzten neu generiert werden.
Anhand des aktuellen Datums pr√ºfen, ob in diesem Job-Durchlauf eine Erstellung durchgef√ºhrt werden soll. 
Falls nein, endet der Job mit diesem Schritt.
Produktdokument f√ºr die Fahrleistungen anhand der Ergebnisse der MOBA-Verdichtung (Tabelle erh_moba_mo_verdichtung_tag_by_tag_mo_mvw) in Verbindung mit den Fahrleistungsklassen (Tabelle erh_mo_fahrleistungsklassen) f√ºr alle Nutzungstage des letzten Monats generieren.
Analog Schritt I.3.
Analog Schritt I.4.
Konfiguration 
Die Konfiguration erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
erh_gdbs_auffaelligkeiten_document_creation_
interval_days
1
Bestimmt, wie oft das Produkt ‚ÄûAuff√§lligkeiten‚Äú erstellt wird. Bei einem Intervall von 1 Tag, sind alle Auff√§lligkeiten eines Tages in dem generierten Produktdokument enthalten.

Tabelle 52: GdbsProductsCreator ‚Äì Konfiguration in Tabelle tech_config
Fehlerbehandlung
Unbekannte Produkte (alle au√üer Auff√§lligkeiten, Fahrleistungen und L√ºckenschlussvorschl√§ge) k√∂nnen nicht in die Warteschlage eingereiht werden. Eine entsprechende Log-Meldung wird ausgegeben.
Produktdokumente, deren XML-Struktur nicht der XSD gem√§√ü SST-Spezifikation 814 entsprechen, werden ebenfalls nicht in die Warteschlange eingereiht. Eine Fehlermeldung wird geloggt und eine IllegalArgumentException geworfen, die zum Job-Abbruch f√ºhrt.
[DSJ-QSErh-0012] GdbsUploadPump
Paket: de.tollcollect.zme.qs.erh.rule.betriebsdaten
Dieser Job ist f√ºr die √úbermittlung der innerhalb QS-AV erstellten Produkte ‚ÄûAuff√§lligkeiten‚Äú, ‚ÄûFahrleistungen‚Äú und ‚ÄûL√ºckenschlussvorschl√§ge‚Äú per Schnittstelle 814 an GDBS zust√§ndig (siehe auch 6.1.2 SST 814 Grunddaten Bereitstellungs-Server (GDBS) ü°®ü°™  QS-AV).
Die Produkte werden asynchron vom Job  erstellt und in eine FIFO-Warteschlange eingereiht. Der Job GdbsUploadPump holt fertig erstellte Produkte aus der Warteschlange und versendet sie  an GDBS. 
Die folgende Abbildung stellt dem Jobablauf schematisch dar. 

Abbildung 41: Job-Ablauf (schematisch): GdbsUploadPump
 Trigger: Der Job GdbsUploadPump wird st√ºndlich vom Bookkeeper getriggert.
Ablauf
Ausstehende Produkt-Uploads aus der Warteschlange abfragen (Tabelle erh_tech_gdbs_pending_uploads). 
Falls keine Uploads ausstehen, endet der Job mit diesem Schritt.
Ausstehende Produkt-Uploads durchf√ºhren, siehe auch Referenzdokument [4] [S_814_SST_00] SST 814 SST Spezifikation GDBS ‚Äì QS-AV.
Hinweise: Fehlgeschlagene Uploads werden in der Warteschlange belassen, ein Retry-Counter wird hochgez√§hlt (num_trials). 
Ist die laut Konfigurationseinstellung erh_gdbs_upload_max_retries maximale Anzahl an Versuchen erreicht, wird der betreffende ausstehende Produkt-Upload aus der Warteschlange entfernt.
Erneute Upload-Versuche nach einem Fehlschlag werden fr√ºhestens im Rahmen des n√§chsten Job-Ausf√ºhrungsintervalls und nach Verstreichen des konfigurierten Retry-Intervalls (erh_gdbs_upload_retry_interval_s) unternommen.
Konfiguration 
Die Konfiguration erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
erh_gdbs_upload_retry_interval_s
3600
Intervall nachdem ein nicht erfolgreicher Upload fr√ºhestens wiederholt werden darf. Der Wert muss immer in Verbindung mit dem Ausf√ºhrungsintervall des GdbsUploadPump betrachtet werden.
ü°™ Retry fr√ºhestens nach letztem Uploadversuch plus Verstreichen von erh_gdbs_upload_retry_interval_s im Rahmen einer regul√§ren Jobausf√ºhrung.
erh_gdbs_upload_max_retries
120
Maximale Anzahl an Versuchen bevor ein Produkt-Upload verworfen wird. 

Tabelle 53: GdbsUploadPump ‚Äì Konfiguration in Tabelle tech_config
Fehlerbehandlung
Unbekannte Produkte (alle au√üer Auff√§lligkeiten, Fahrleistungen und L√ºckenschlussvorschl√§ge) werden nicht √ºbertragen und entsprechende Produkt-Uploads aus der Warteschlagen entfernt. Eine entsprechende Log-Meldung wird ausgegeben.
Technischen √úbertragungsfehler werden je Produkt-Upload gefangen und als Warnung geloggt. 
[DSJ-QSErh-0020] ProcessONQ15Job 
Paket: de.tollcollect.zme.qs.erh.rule.onq
√úberblick ONQ
Die Berechnung der Operativen Nichterkennungsquote (ONQ) und die Verdichtung der Ergebnisse erfolgt in zwei Stufen mit unterschiedlicher Taktung. Die folgende Abbildung soll dies veranschaulichen.

Abbildung 42: √úberblick ONQ
√úber die Schnittstelle 833 empf√§ngt QS-AV die vom Erkenner √ºbermittelten RsE-Kontakte. Sie werden durch den Job [DSJ-QSErh-0100] ProcessRSEIdKorrektur aus der Cassandra-Tabelle lds_es_en_q_rse_kontakt gelesen, ggf. korrigiert und in der Tabelle erh_onq_rse_kontakt_id_korrigiert gespeichert.
Der Bookkeeper startet den Job ProcessONQ15Job alle 15 Minuten. Der Job verarbeitet in jedem Durchgang ein 15 Minuten umfassendes Timebucket an korrigierten RsE-Kontakten. Er bewertet f√ºr jeden RsE-Kontakt anhand der Sollzuordnung (erh_onq_sollzuordnung_mo_tag und erh_onq_sollzuordnung_eo_tag), ob eine Erkennung oder Nicht-Erkennung vorliegt und berechnet im Anschluss die ONQ bezogen auf Maut- und Erkennungsobjekte und speichert diese in der Tabelle erh_onq_15min_2. Als Zwischenergebnis aktualisiert der Job die Tabelle erh_onq_soll_ist_vergleich_hlp, die zur Verdichtung der Quoten durch den Job ProcessONQ24Job ben√∂tigt wird. 
Einmal t√§glich startet der Bookkeeper den Job ProcessOnqSollzuordnungJob. Dieser verarbeitet das 24 Stunden umfassende Timebucket des vorigen Tages an RsE-Kontakten, aktualisiert die Sollzuordnungen zwischen Maut- sowie Erkennungsobjekten und RsE anhand der gesammelten Daten und speichert sie in den Tabellen erh_onq_sollzuordnung_mo_tag und erh_onq_sollzuordnung_eo_tag sowie erh_onq_sollzuordnung_eo_mo_tag (letztere wird f√ºr die Verarbeitung innerhalb des ProcessDSRC15Job ben√∂tigt). 
Einmal t√§glich startet der Bookkeeper den Job ProcessONQ24Job. Dieser verdichtet die Ergebnisse aus dem Soll-Ist-Vergleich des Vortages und speichert sie in den Zieltabellen des Jobs.
Umsetzung
Die Berechnung der ONQ ist in der Klasse ProcessONQ15Job, wie folgt implementiert:

Abbildung 43: Job-Ablauf (schematisch): ProcessONQ15Job
 Trigger: Die Berechnung der ONQ wird alle 15 Minuten vom Bookkeeper getriggert.
Ablauf 
Lesen der RsE-Kontakte (15min Timebucket) aus der Quelltabelle (erh_onq_rse_kontakt_id_korrigiert).
Hinweis: Es werden nur produktive RsE-Kontakte (BD-Instanz = Produktion) ber√ºcksichtigt. Dubletten und gesperrte RsE-Kontakte werden ignoriert. 
Soll-Ist-Bewertung f√ºr alle eingelesenen RsE-Kontakte mit Hilfe der Sollzuordnung (erh_onq_sollzuordnung_mo_tag und erh_onq_sollzuordnung_eo_tag) durchf√ºhren und in der Tabelle erh_onq_soll_ist_vergleich_hlp speichern.
Anhand der Soll-Ist-Bewertung der eingelesenen RsE-Kontakte werden die Gut- und Schlechtf√§lle bei der Erkennung aufsummiert. Per Formel werden daraus die Operativen Nichterkennungsquoten (ONQ) f√ºr das aktuelle Timebucket (15min) berechnet und in der Tabelle erh_onq_15min_2 gespeichert. 
Die Ergebnisse werden dabei gruppiert nach FzG-Typ, LED-Farbe (rot, gr√ºn oder gesamt), Objekt-Typ (MO, EO oder alle) und Stra√üentyp (BAB, BS oder gesamt).
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.onq.led_rot_grund_query
19028,
19478,
2620
Dient zum Ausschluss von RsE-Kontakten. 
Die hier konfigurierten Rotgr√ºnde (Nummern)  weisen auf manuelles Verfahren hin.
erh.onq.led_rot_grund_rule_sperre_query
33420, 
35376
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgr√ºnde (Nummern) indizieren gesperrte FzG.
erh.onq.bd_instanz_produktion
1
RsE-Kontakte mit bd_instanz=1 stammen aus der produktiven BD-Instanz.
erh.onq.konau_nummernkreis_oben
5000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten
4000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben
8000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten
6000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.zeit_onq_sek
3600
Anzahl der Sekunden r√ºckwirkend des RSE-Kontaktzeitpunkts, der f√ºr die Berechnung betrachtet wird.
erh.onq.led_farbe_rot
0
LED-Farbe der RsE-Kontakte mit led_farbe=0 ist rot.
erh.onq.led_farbe_gruen
1
LED-Farbe der RsE-Kontakte mit led_farbe=1 ist gr√ºn.

Tabelle 54: ProcessONQ15  - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException geworfen.

[DSJ-QSErh-0021] ProcessOnqSollzuordnungJob
Paket: de.tollcollect.zme.qs.erh.rule.onq
Der Job ProcessOnqSollzuordnungJob berechnet anhand der RsE-Kontakte des Vortages die Sollzuordnungen zwischen RsE-Id und Mautobjekten bzw. Erkennungsobjekten, die als Grundlage f√ºr die Berechnung der ONQ sowie der DSRC-Quoten dienen.
Der Job ist in der Klasse ProcessOnqSollzuordnungJob, wie folgt implementiert:

Abbildung 44: Job-Ablauf (schematisch): ProcessOnqSollzuordnungJob
 Trigger: Der Job ProcessOnqSollzuordnungJob wird 1x t√§glich vom Bookkeeper getriggert.
Ablauf 
Einlesen der RsE-Kontakte des gestrigen Tages aus der Quelltabelle (erh_onq_rse_kontakt_id_korrigiert).
Hinweis: Es werden nur produktive RsE-Kontakte (BD-Instanz = Produktion) ber√ºcksichtigt. Dubletten und gesperrte RsE-Kontakte werden ignoriert. 
Ermittlung der Sollzuordnungen: Die Sollzuordnungen werden unter Anwendung der Schwellwerte berechnet: Die dabei jeweils am h√§ufigsten auftretende Kombination von MO-ID bzw. EO-ID und RsE-Id wird als Sollzuordnung gesetzt.
Die Sollzuordnungen in den Zieltabellen speichern.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.onq.led_rot_grund_query
19028,
19478,
2620
Dient zum Ausschluss von RsE-Kontakten. 
Die hier konfigurierten Rotgr√ºnde (Nummern) indizieren manuelles Verfahren.
erh.onq.bd_instanz_produktion
1
RsE-Kontakte mit bd_instanz=1 stammen aus der produktiven BD-Instanz.
erh.onq.konau_nummernkreis_oben
5000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten
4000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben
8000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten
6000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.anz_gruen
3
Schwellwert f√ºr die Mindestanzahl an RsE-Kontakten mit led_farbe gruen f√ºr eine RsE-ID.
erh.onq.ant_ausfahrt
20
Schwellwert f√ºr die erlaubte Quote an Ausfahrten f√ºr eine RsE.
erh.onq. anz_rse_gesamt_eo
280
Anzahl der RSE-Kontakte, die mindestens notwendig sind, um eine neue EO-Sollzuordnung zu berechnen.
erh.onq. anz_rse_gesamt_mo
280
Anzahl der RSE-Kontakte, die mindestens notwendig sind, um eine neue MO-Sollzuordnung zu berechnen.
erh.onq. anz_rse_gesamt_eo_mo
280
Anzahl der RSE-Kontakte, die mindestens notwendig sind, um eine neue EO-MO-Sollzuordnung zu berechnen.
erh.onq.led_farbe_gruen
1
LED-Farbe der RsE-Kontakte mit led_farbe=1 ist gr√ºn.
erh.onq.led_rot_grund_rule_sperre_query
33420, 
35376
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgr√ºnde (Nummern) indizieren gesperrte FzG.
erh.onq.erhebungsparameterabfahrt
2
RsE-Kontakte mit erhebungsparameter=2 indizieren Abfahrten.

Tabelle 55: ProcessOnqSollzuordnungJob  - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte  nicht gesetzt worden sein, wird eine SchwellwertException geworfen.


[DSJ-QSErh-0030] ProcessONQ24Job
Paket: de.tollcollect.zme.qs.erh.rule.onq
Der Job ProcessONQ24Job verdichtet die Ergebnisse des Jobs ProcessONQ15Job tageweise. Einen √úberblick √ºber die Zusammenh√§nge der ONQ-Berechnung finden Sie im Kapitel 5.2.5.1.4 [DSJ-QSErh-0020] ProcessONQ15Job  unter √úberblick. 
Der Job ist in der Klasse ProcessONQ24Job, wie folgt implementiert:

Abbildung 45: Job-Ablauf (schematisch): ProcessONQ24Job
 Trigger: Der Job ProcessONQ24Job wird 1x t√§glich vom Bookkeeper getriggert.
Ablauf 
Ergebnisse des Soll-Ist-Vergleichs des gestrigen Tages (erh_onq_soll_ist_vergleich_hlp) einlesen.
Kennzahlen und Quoten f√ºr die letzten 24 Stunden berechnen, verdichten und in den Zieltabellen speichern.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.onq.konau_nummernkreis_oben
5000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben), zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten
4000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten), zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben
8000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben), zur Kennzeichnung der Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten
6000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten), zur Kennzeichnung der Kontrollart KonSL.
erh.onq.led_farbe_rot
0
LED-Farbe der RsE-Kontakte mit led_farbe=0 ist rot.
erh.onq.led_farbe_gruen
1
LED-Farbe der RsE-Kontakte mit led_farbe=1 ist gr√ºn.

Tabelle 56: ProcessONQ24Job  - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException geworfen.

[DSJ-QSErh-0040] ProcessDSRC15Job 
Paket: de.tollcollect.zme.qs.erh.rule.onq
√úberblick DSRC-Quotenberechnung
Die Berechnung der Dedicated Short Range Communication Quote (DSRC-Quote) und die Verdichtung der Ergebnisse erfolgt in zwei Stufen mit unterschiedlicher Taktung. Die folgende Abbildung soll dies veranschaulichen.

Abbildung 46: √úberblick DSRC-Quotenberechnung
√úber die Schnittstelle 833 empf√§ngt QS-AV die vom Erkenner √ºbermittelten Befahrungsparameter und RsE-Kontakte. Die RsE-Kontakte werden durch den Job [DSJ-QSErh-0100] ProcessRSEIdKorrektur aus der Cassandra-Tabelle lds_es_en_q_rse_kontakt gelesen, ggf. korrigiert und in der Tabelle erh_onq_rse_kontakt_id_korrigiert gespeichert.
Der Bookkeeper startet den Job ProcessDSRC15Job alle 15 Minuten. Der Job verarbeitet in jedem Durchgang ein 15 Minuten umfassendes Timebucket an Befahrungsparametern. Er bewertet f√ºr jeden Befahrungsparameter anhand der Sollzuordnung (erh_onq_sollzuordnung_mo_tag und erh_onq_sollzuordnung_eo_mo_tag), ob ein RsE-Kontakt erwartet wird und gleicht diesen Erwartungswert mit den RsE-Kontakten ab (erh_onq_rse_kontakt_id_korrigiert). Aus dem Ergebnis kann im Anschluss die DSRC-Quote berechnet werden. Dabei werden RsE-Kontakte aus der konfigurierten Blacklist (erh_onq_dsrc_blacklist_rse) ignoriert. Als Zwischenergebnis aktualisiert der Job die Tabelle erh_onq_dsrc_soll_ist_vergleich_2, die zur t√§glichen Verdichtung der Quoten durch den Job ProcessDSRC24Job ben√∂tigt wird. 
Einmal t√§glich um 0 Uhr startet der Bookkeeper den Job ProcessDSRC24Job. Dieser verarbeitet die 24 Stunden umfassenden Timebuckets des vorigen Tages aus der Tabelle erh_onq_dsrc_soll_ist_vergleich_2 und verdichtet die DSRC-Quoten des vergangenen Tages st√ºndlich nach RsE-Id, nach EHK-Id. Auch hier werden RsE-Kontakte aus der konfigurierten Blacklist (erh_onq_dsrc_blacklist_rse) ignoriert.

Abbildung 47: Job-Ablauf (schematisch): ProcessDSRC15Job
 Trigger: Die DSRC-Quotenberechnung wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
Laden der Befahrungsparameter (in drei 5min Timebuckets) aus lds_es_en_q_befahrungsparameter_eo.
Laden der aktuellen MO-Sollzuordnung (erh_onq_sollzuordnung_mo_tag) und EO/MO-RsE-Sollzuordnung (erh_onq_sollzuordnung_eo_mo_tag), sofern vorhanden, als Referenz zu den Befahrungsparametern.
F√ºr jeden Befahrungsparameter wird √ºberpr√ºft, ob f√ºr das darin gespeicherte letzte Mautobjekt (MO) bzw. Erkennungsobjekt (EO) eine Sollzuordnung in den Tabellen erh_onq_sollzuordnung_(eo_)mo_tag vorliegt. Dabei gilt die Einschr√§nkung, dass nur Befahrungsparameter betrachtet werden, deren Befahrungszeit nicht l√§nger, als die mittels des Schwellwerts erh.onq.schwellwert_erhebung_tage_zurueck konfigurierte Zeitspanne zur√ºck liegt.  F√ºr die weitere Quotenberechnung und Verdichtung werden nur Befahrungsparameter aus dem konfigurierten Befahrungszeitraum verwendet, f√ºr die eine Sollzuordnung ermittelt werden konnte.
Laden der RsE-Kontakte aus erh_onq_rse_kontakt_id_korrigiert.
Die in Schritt 3 indirekt ermittelten Befahrungsparameter-RsE-Soll-Zuordnungen werden mit den tats√§chlich registrierten RsE-Kontakten (erh_onq_rse_kontakt_id_korrigiert) abgeglichen. Bei √úbereinstimmung wird ein Gutfall gez√§hlt, ansonsten wird ein Schlechtfall gez√§hlt. Die Ergebnisse werden in der Tabelle erh_onq_dsrc_soll_ist_vergleich_2 gespeichert.
Die Daten der Tabelle erh_onq_dsrc_soll_ist_vergleich_2 der letzten Viertelstunde werden mit folgenden Gruppierungskriterien verdichtet: Anzahl der Erhebungen per RsE-Id, Gut- und Schlechtf√§lle. Das Ergebnis der Verdichtung wird in der Tabelle  erh_onq_dsrc_15min gespeichert. Dabei werden RsE-Kontakte aus der konfigurierten Blacklist (erh_onq_dsrc_blacklist_rse) ignoriert.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.onq.led_rot_grund_query
19028,
19478,
2620
Dient zum Ausschluss von RsE-Kontakten. 
Die hier konfigurierten Rotgr√ºnde (Nummern) indizieren manuelles Verfahren.
erh.onq.bd_instanz_produktion
1
RsE-Kontakte mit bd_instanz=1 stammen aus der produktiven BD-Instanz.
erh.onq.konau_nummernkreis_oben
5000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten
4000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben
8000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten
6000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.led_rot_grund_rule_sperre_query
33420, 
35376
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgr√ºnde (Nummern) indizieren gesperrte FzG.
erh.onq.schwellwert_erhebung_tage_zurueck
5
Zeitraum in Tagen, wie lange ein Befahrungsparameter r√ºckwirkend f√ºr die Berechnungen herangezogen werden darf.
erh.onq.rse_suchfenster_bs
3600
Sekunden r√ºckwirkend zum RSE-Kontaktzeitpunkt: Festlegung des Starts des Suchfensters f√ºr die Berechnung der DSRC-Quote (Ende = Kontaktzeitpunkt).

Tabelle 57: ProcessDSRC15Job - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException geworfen.

[DSJ-QSErh-0050] ProcessDSRC24Job
Paket: de.tollcollect.zme.qs.erh.rule.onq
Der Job ProcessDSRC24Job dient der t√§glichen Verdichtung der Ergebnisse des Jobs ProcessDSRC15Job. Einen √úberblick √ºber die DSRC-Quotenberechnung und die Abh√§ngigkeiten der Jobs finden Sie im Kapitel 5.2.5.1.7 [DSJ-QSErh-0040]  unter √úberblick DSRC. 
Der Job ProcessDSRC24Job ist in der Klasse ProcessDSRC24Job, wie folgt implementiert:

Abbildung 48: Job-Ablauf (schematisch): ProcessDSRC24Job
 Trigger: Die Verdichtung wird 1x t√§glich vom Bookkeeper gestartet.
Ablauf
Laden aller viertelst√ºndlichen Ergebnisse des Jobs ProcessDSRC15Job aus erh_onq_dsrc_soll_ist_vergleich_2 f√ºr den vergangenen Tag sowie der Blacklist mit den auszuschlie√üenden RsE-IDs (erh_onq_dsrc_blacklist_rse).
Wichtig: RsE-Kontakte, die in der Blacklist konfiguriert sind, werden f√ºr die folgenden Verdichtungen ausgeschlossen.
Verdichtung der Daten per RsE-ID und je FzG-Typ-Aspekten (alle/prod/pilot), Berechnung der DSRC-Quoten f√ºr den gesamten Tag und Speicherung in der Tabelle erh_onq_dsrc_verdichtung_rse_2. 
Verdichtung der Daten per EHK-ID, Berechnung der DSRC-Quote f√ºr den gesamten Tag und Speicherung in der Tabelle erh_onq_dsrc_verdichtung_fzg_2. Dabei ist der Schwellwert erh.onq.schwellwert_dsrc_quote zu beachten: F√ºr die Verdichtung d√ºrfen nur Daten zu RsEs mit einer DSRC-Quote gr√∂√üer des angegebenen Schwellwertes ber√ºcksichtigt werden, um die FzG-bezogenen Verdichtungsergebnisse nicht zu verf√§lschen.
Verdichtung der Daten per FzG-Typ und Stunde und Speicherung in der Tabelle erh_onq_dsrc_verdichtung_stunde.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.onq.dsrc_quote
98
Mindestwert f√ºr die DSRC-Quote einer RsE, damit  sie f√ºr die FzG-bezogene DSRC-Quoten-Verdichtung ber√ºcksichtigt wird.
Hintergrund: F√ºr die Verdichtung d√ºrfen nur Daten zu RsEs mit einer DSRC-Quote gr√∂√üer des angegebenen Schwellwertes ber√ºcksichtigt werden, um die FzG-bezogenen Verdichtungsergebnisse nicht zu verf√§lschen.
erh.onq.konau_nummernkreis_oben
5000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben), zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten
4000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten), zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben
8000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben), zur Kennzeichnung der Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten
6000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten), zur Kennzeichnung der Kontrollart KonSL.

Tabelle 58: ProcessONQ24  - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException geworfen.

[DSJ-QSErh-0060] MobaEventCreator
Paket: de.tollcollect.zme.qs.erh.rule
Es werden MOBA-Events zur Aufdeckung von Ein-Abschnitts-Erhebungsl√ºcken und doppelt gebuchter Abschnitte (Fehler 1. Art) erzeugt. Die Events der Klasse MobaEventCreator werden durch einen Vergleich erhobener Abschnitte (Befahrungsparameter vom Erkenner) mit der  Abschnittstopologie aus der Abschnittssequenz (aus den Betriebsdaten) aufgedeckt. Befahrungsparameter, wie z.B. Befahrungszeit des Abschnittes und Erhebungsparameter (Einfahrt, Durchfahrt, Ausfahrt) flie√üen dabei mit ein.

Abbildung 49: Job-Ablauf (schematisch): MobaEventCreator
 Trigger: Die Erzeugung der MOBA-Events wird alle 5 Minuten vom Bookkeeper in Abh√§ngigkeit der eingehenden Befahrungsparameter vom ES getriggert.
Ablauf
Befahrungsparameter des aktuellen Timebuckets aus der Quelltabelle (lds_es_en_q_befahrungsparameter_eo) lesen.
Hinweis: Alle Befahrungsparameter mit aktuellemAbschnitt.Befahrungszeit √§lter als dem Betrachtungszeitraum (erh.moba.betrachtungszeitraum_tage) werden ignoriert.
MOBA-Regeln ausf√ºhren und potentielle MOBA-Events generieren:
Regel 120 ‚Äì Ung√ºltiger Streckenabschnitt
Regel 131 ‚Äì Erhebungsparameter Einfahrt an Streckenabschnitt ohne Startm√∂glichkeit
Regel 132 ‚Äì Erhebungsparameter Ausfahrt an Streckenabschnitt ohne Zielm√∂glichkeit
Regel 240 ‚Äì Doppelt gebuchte Abschnitte  
Regel 241 ‚Äì Doppelt gebuchter Abschnitt, ED 
Regel 310 ‚Äì Ein-Abschnitts-L√ºcken mit falscher Aus-/Einfahrtskennung
Regel 313 ‚Äì Ein-Abschnitts-L√ºcke mit doppelt gebuchtem Abschnitt ED  
Regel 318 ‚Äì Ein-Abschnitts-L√ºcke mit doppelt gebuchtem Abschnitt DD 
Regel 323 ‚Äì Sichere Ein-Abschnitts-L√ºcke mit falscher Kennung ED 
Regel 328 ‚Äì Sichere Ein-Abschnitts-L√ºcke mit falscher Kennung DD 
Regel 350 ‚Äì Falsche Aus-Einfahrtskennung, AE
Potentielle MOBA-Events aus Schritt 2) gem√§√ü Ausnahmeliste (erh_mo_ausnahme) filtern.
Finale MOBA-Events mit Daten aus Fzg-Stammdaten (lds_dm_fzg_stammdaten) und Regelliste (erh_mo_regel) anreichern, Ausnahmestatistik aktualisieren.
MOBA-Events in Cassandra DB speichern (erh_moba_event: unter Verwendung des Quell-Timebuckets der Befahrungsparameter), Ausnahmestatistik in Cassandra DB aktualisieren (erh_moba_ausnahme_statistik). 
Hinweis: Die Tabelle erh_moba_ausnahme_statistik_historized ist eine Hilfstabelle zur Aktualisierung der Ausnahmestatistik (Idempotenz).
Allgemeine Hinweise
Nutzungstag und Nutzungsstunde eines MOBA Events werden aus Befahrungszeit des ersten aktuellen Abschnitts berechnet.
Zu G√ºltigkeitsangaben in den Regeln siehe auch Kapitel 10.12.1 G√ºltigkeitsbereiche.
Konfiguration 
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.moba.betrachtungszeitraum_tage
5
Konfiguration des Betrachtungszeitraums. Es d√ºrfen nur Befahrungen in MOBA und in der Verdichtung ber√ºcksichtigt werden, die gemessen an der Befahrungszeit nicht √§lter sind, als die konfigurierbare Anzahl von Tagen (Default-Wert).

Tabelle 59: MobaEventCreator - Konfiguration in Tabelle tech_rule_schwellwert
Die Konfiguration einer Default-Mautobjektklasse erfolgt in der Cassandra-Tabelle tech_config mittels des folgenden Keys:
Key
Default
Beschreibung
default.mautobjektklasse
1
Default-Wert, der als Mautobjektklasse gesetzt wird, wenn f√ºr ein Mautobjekt keine Mautobjektklasse ermittelt werden konnte (in Tabelle erh_mo_mautobjekt_mautobjektklasse).

Tabelle 60: MobaEventCreator - Konfiguration in Tabelle tech_config
Fehlerbehandlung
Folgender Fehler f√ºhrt zum Abbruch der MOBA-Event-Erzeugung f√ºr das aktuelle Timebucket (Exception in Richtung Bookkeeper): 
Formatfehler f√ºr die BD-Versionen in den Befahrungsparametern.
Folgende Inkonsistenz f√ºhrt zur Verwerfung eines Befahrungsparameters f√ºr die MOBA-Event-Erzeugung: 
Betriebsdaten gem√§√ü BD-Version des Befahrungsparameters sind nicht in QS-AV vorhanden.
Sollte einer der Schwellwerte  nicht gesetzt worden sein, wird eine SchwellwertException geworfen.
[DSJ-QSErh-0070] MobaVerdichtungJob
Paket: de.tollcollect.zme.qs.erh.rule
Der Job MobaVerdichtungJob f√ºhrt in unterschiedlichen Granularit√§ten die Verdichtung der MOBA-Ereignisse basierend auf den Befahrungsparameter und den MOBA-Events durch. Die Befahrungsparameter werden dabei jeweils nach Befahrungszeitpunkt der Abschnitte  gefiltert. Ziel der Verdichtung ist es eine Bewertung des Optimierungspotenzials des Erkennungsmodells vornehmen zu k√∂nnen.
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 50: Job-Ablauf (schematisch): MobaVerdichtungJob
 Trigger: Die MOBA-Verdichtung wird st√ºndlich vom Bookkeeper gestartet. 
Ablauf 
Laden der 12 abgeleiteten Timebuckets von lds_es_en_q_befahrungsparameter_eo und erh_moba_event.
Es werden nur Befahrungsparameter weiterverarbeitet, deren Befahrungszeit des aktuellen Abschnitts nicht mehr Tage zur√ºckliegt, als der Schwellwert erh.moba.betrachtungszeitraum_tage vorgibt. Der aktuelle Tag wird vom Input-Timebucket abgeleitet (bereitgestellt durch den Job MobaEventCreator).
Durchf√ºhrung der FzG-bezogenen Verdichtungen mit Befahrungsparametern gefiltert nach BD-Instanz ‚ÄûProduktion‚Äú und Speicherung der Ergebnisse in den entsprechenden Zieltabellen. 
Durchf√ºhrung der Regelverdichtung f√ºr Erkennungsobjekte pro FzG und pro MO und Speicherung der Ergebnisse in den entsprechenden Zieltabellen. 
Durchf√ºhrung der Erkennungsobjektverdichtung pro Stunde und pro Tag und Speicherung der Ergebnisse in den entsprechenden Zieltabellen. 
Hinweise: 
Alle FzG-bezogenen Verdichtungen werden mit einer TTL persistiert (gem√§√ü Konfigurationswert erh.fzg_verdichtung.deletion_interval_sec).
Bei den Tabellen *_hist handelt es sich um Hilfstabellen zur Aktualisierung der Verdichtungen (Idempotenz). Auch die t√§glichen Verdichtungen werden st√ºndlich aktualisiert.
Allgemeine Hinweise
Defaultwerte f√ºr einzelne Mautobjekte werden vom Job MoVerdichtungDefaultValuesSetter zu Beginn eines Tages geschrieben und von diesem √ºberschrieben.
Da die Verdichtungen der MOBA- und der EOBA-Events √§hnliche Abl√§ufe und Strukturen aufweisen, wurde durch die Klasse ModellVerdichter eine gemeinsame Basisklasse implementiert, welche die jeweilige Verdichtung (EOBA/MOBA) steuert.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.moba.betrachtungszeitraum_tage
5
Konfiguration des Betrachtungszeitraums. Es d√ºrfen nur Befahrungen in MOBA und in der Verdichtung ber√ºcksichtigt werden, die gemessen an der Befahrungszeit nicht √§lter sind, als die konfigurierbare Anzahl von Tagen (Default-Wert).

Tabelle 61: MobaVerdichtungJob - Konfiguration in Tabelle tech_rule_schwellwert
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
erh.fzg_verdichtung.deletion_interval_sec
10368000 
(120 Tage) 
Sekunden nach denen Werte in Verdichtungstabellen von EOBA/MOBA und ONQ mit FzG-Bezug via EHK-ID gel√∂scht werden.

Tabelle 62: MobaVerdichtungJob ‚Äì Konfiguration in Tabelle tech_config
Fehlerbehandlung
Existiert einer der Schl√ºssel erh.fzg_verdichtung.deletion_interval_sec oder erh.moba.betrachtungszeitraum_tage nicht in der Konfiguration, sind keine Werte gesetzt oder sind die Werte nicht vom Typ Integer, wird eine Exception in Richtung Bookkeeper geworfen und der Job abgebrochen.
Sollte einer der Schwellwerte  nicht gesetzt worden sein, wird eine SchwellwertException geworfen.
[DSJ-QSErh-0080] MoVerdichtungDefaultValueSetter
Paket: de.tollcollect.zme.qs.erh.rule
Dieser Job hat die Aufgabe zur Vorbereitung der MOBA-Verdichtung in den Tabellen erh_moba_mo_verdichtung_tag und erh_moba_mo_verdichtung_stunde f√ºr den kommenden Tag Defaultwerte f√ºr alle Mautobjekte zu hinterlegen. Der Job MoVerdichtungDefaultValuesSetter muss aus diesem Grund jeden Tag vor dem  MobaVerdichter laufen.
Der Job ist in der Klasse MoVerdichtungDefaultValuesSetter wie folgt implementiert:

Abbildung 51: Job-Ablauf (schematisch): MoVerdichtungDefaultValueSetter
 Trigger: Der MoVerdichtungDefaultValuesSetter wird  1x t√§glich vom Bookkeeper gestartet. 
Ablauf 
Aktuellen Tag aus dem Start-Timebucket des Jobs ermitteln ‚Äì f√ºr diesen Tag sollen die Default-Werte gesetzt werden.
Anhand von erh_gdbs_betriebsdatenhistorie werden alle g√ºltigen Tarifabschnittslisten-Versionen (ta_version) f√ºr den aktuellen Tag ermittelt.
Alle Mautobjekte der ermittelten TAL-Versionen anhand der Tabelle erh_gdbs_mautobjekt ermitteln und mit ihrem √Ñnderungsstatus aus der Tabelle erh_mo_aenderungsstatus anreichern (Defaultwert = unveraendert).
F√ºr alle ermittelten Mautobjekte Default-Eintr√§ge f√ºr den aktuellen Tag in die Tabellen erh_moba_mo_verdichtung_tag und erh_moba_mo_verdichtung_stunde schreiben.
bd_aenderungsstatus: <siehe Schritt 3>
anz_ausfahrten: 0 
anz_einfahrten: 0 
anz_durchfahrten: 0 
anz_erhebungen_lueckenschluss: 0 
anz_events: 0 
anz_events_erhebungsluecken: 0 
moba_quote: 100 
erhebungsquote: 100 
fahrleistung:  0
Allgemeine Hinweise
Der MobaVerdichter (siehe Kapitel ÔÇ∑ Sollte einer der Schwellwerte  nicht gesetzt worden sein, wird eine SchwellwertException geworfen.
[DSJ-QSErh-0070] MobaVerdichtungJob) verdichtet Befahrungsparameter, die bis zu f√ºnf Tage alt sind. Bei Jobstart auf einer leeren Datenbank kann es f√ºr die vier vorangegangenen Tage keine Default-Werte und keinen BD-√Ñnderungsstatus geben, da an diesen Tagen der MoVerdichtungDefaultValuesSetter -Job nicht ausgef√ºhrt w√ºrde. Aus diesem Grund werden beim blanken Deployment automatisch per Skript entsprechende Defaultwerte gesetzt.
[DSJ-QSErh-0090] GenericMetrikVerdichtung
Paket: de.tollcollect.zme.qs.erh.rule.edm
Der Tarifierer (EDM) liefert zyklisch alle 15 Minuten die Anzahl der Tarifierungen metrisch je Betriebsdaten-Release und Betriebsdateninstanz an QS-Erh. Diese Eingangswerte werden auf Verarbeitungstag und -stunde hin durch den Job GenericMetrikVerdichtung verdichtet. 

Abbildung 52: Job-Ablauf (schematisch): GenericMetrikVerdichtung
 Trigger:  Der Job wird vom Bookkeeper mit zwei Verdichtungs-Granularit√§ten gestartet: Die Stundenverdichtung wird st√ºndlich, die Tagesverdichtung 1x t√§glich. 
Ablauf
Einlesen der Timebuckets der Metrik-Events aus lds_edm_en_q_metriken.
Verdichtung nach Stunden respektive Tag pro BD-Release, BD-Instanz und Event-ID.
Hinweis: Bei der Verdichtung werden ausschlie√ülich Datens√§tze mit g√ºltigen BD-Releases und -Instanzen ber√ºcksichtigt. Die von EDM √ºbermittelte Aggregation ‚Äûrestliche‚Äú wird ignoriert.
Speicherung der Verdichtung in erh_edm_metriken_verdichtung_1h bzw. erh_edm_metriken_verdichtung_24h.
Konfiguration
Die Aggregations-Granularit√§t des Jobs GenericMetricVerdichtung wird vollst√§ndig √ºber die Bookkeeper gesteuert (siehe auch Abschnitt Fehlerbehandlung). Eine variable Konfiguration (ohne Deployment) ist nicht vorhanden.
Fehlerbehandlung
Spark-DataFrame bzw. SQL-Fehler f√ºhren zu einem Abbruch des gesamten Jobs. 
Bei der Job-Konfiguration  muss beachtet werden, dass prinzipiell mehrere Timebucket-Schemata als Vorbedingungen definiert werden k√∂nnen, sie sich jedoch immer auch auf genau eine Quell-Tabelle beziehen m√ºssen. Gleichzeitig muss genau ein Timebucket-Schema als Nachbedingung konfiguriert werden, das auch die Ziel-Tabelle f√ºr die konfigurierte Verdichtung festlegt. Der Job scheitert, wenn diese Randbedingungen nicht erf√ºllt werden.
[DSJ-QSErh-0100] ProcessRSEIdKorrektur
Paket: de.tollcollect.zme.qs.erh.rule.onq
Der Job ProcessRSEIdKorrektur dient dazu, die vom Erkennungsservice (ES) gelieferten RsE-Kontakte vorzuselektieren, um sicherzustellen, dass ausschlie√ülich RsE-Kontakte der Kontrollart KonAu (automatische Kontrolle) f√ºr die ONQ/DSRC-Berechnung verwendet werden. 
Die Selektion wird anhand der RsE-ID vorgenommen. Sie wird als Dezimalwert √ºbermittelt und setzt sich zusammen aus der 13Bit Manufacturer-ID und der 27Bit Individual-ID. Die Manufacturer-ID gibt Auskunft √ºber den Hersteller, die Individual-ID bezieht sich auf die konkrete RsE. F√ºr KonAu wird eine Individual-ID im Nummernkreis von 4000 bis 4999 (durch den Fachbereich festgelegt) erwartet. Die aktuell √ºbermittelte RsE-ID ist l√§nger. Sie soll auf Seiten KonAu ge√§ndert werden. Um den Zeitraum der Umstellung zu √ºberbr√ºcken, wurde ein Workaround umgesetzt, der die gelieferten RsE-IDs auf die von QS-AV erwarteten Werte mappt. 
 
Abbildung 53: Job-Ablauf (schematisch): ProcessRSEIdKorrektur
 Trigger:  Der ProcessRSEIdKorrektur-Job wird alle 15 Minuten vom Bookkeeper getriggert.
Ablauf
Pr√ºfe anhand des Schwellwerts erh.onq.schwellwert_use_onq_workaround, ob der Workaround des RsE-ID-Mappings f√ºr die Verarbeitung des/der aktuellen Timebuckets verwendet werden soll.
Falls nein, f√ºhre die Selektierung der  RsE-IDs durch: 
√úberpr√ºfe ob RsE-Kontakte des Herstellers erlaubt sind und weiterverarbeitet werden sollen. Dazu wird gepr√ºft, ob die Manufacturer-ID den Wert ‚Äû10101‚Äú (bin√§r) bzw. ‚Äû21‚Äú (dezimal) aufweist. Sofern die ID nicht √ºbereinstimmt, wird der RsE-Kontakt nicht weiter verarbeitet.
Pr√ºfe, ob die 4-stellige Zahl f√ºr die Kontrollart der Individual-ID innerhalb der konfigurierten Schwellwertgrenzen (erh.onq.nummernkreis_oben und erh.onq.nummernkreis_unten) liegt. Andernfalls wird der RsE-Kontakt nicht weiter verarbeitet.
Falls ja, f√ºhre die Selektierung anhand der mitgelieferten RsE-ID und der Mapping-Tabelle durch: 
√úberpr√ºfe, ob RsE-Kontakte des Herstellers erlaubt sind und weiterverarbeitet werden sollen. Dazu wird gepr√ºft, ob die Manufacturer-ID den Wert ‚Äû10101‚Äú (bin√§r) bzw. ‚Äû21‚Äú (dezimal) aufweist. Sofern die ID nicht √ºbereinstimmt, wird der RsE-Kontakt nicht weiter verarbeitet.
Ermittle die korrekte RsE-ID anhand der Mapping-Tabelle erh_onq_rse_mapping_table und pr√ºfe, ob die 4-stellige Zahl f√ºr die Kontrollart innerhalb der konfigurierten Schwellwertgrenzen (erh.onq.nummernkreis_oben und erh.onq.nummernkreis_unten) liegt. Andernfalls wird der RsE-Kontakt nicht weiter verarbeitet.
Persistiere zutreffende RsE-Kontakte aus 1.a bzw. 1.b in der Tabelle erh_onq_rse_kontakt_id_korrigiert.
Allgemeine Hinweise
Die Tabelle erh_onq_rse_kontakt_id_korrigiert ist analog zur Tabelle lds_es_en_q_rse_kontakte aufgebaut und wird auf Grund des Workarounds verwendet, um die RsE-Kontakte zu speichern. Bei RsE-Kontakten, bei denen ein Mapping der RsE-ID erforderlich war, wird die aus dem Mapping ermittelte korrigierte RsE-ID gespeichert.  
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.onq.konau_nummernkreis_oben
5000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben), zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten
4000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten), zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben
8000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben), zur Kennzeichnung der Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten
6000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten), zur Kennzeichnung der Kontrollart KonSL.
erh.onq.use_onq_workaround
true
Gibt an ob der Workaround des RsE-ID-Mappings f√ºr die Verarbeitung verwendet werden soll. 
true= Workaround soll verwendet werden, false= Workaround soll nicht verwendet werden.

Tabelle 63: ProcessRSEIdKorrektur - Konfiguration in Tabelle tech_rule_schwellwert 
[DSJ-QSErh-0110] EobaEventCreator
Paket: de.tollcollect.zme.qs.erh.rule
Der EobaEventCreator erzeugt EOBA-Events auf Basis des Erkennungsmodells und der Befahrungsparameter. 
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 54: Job-Ablauf (schematisch): EobaEventCreator
 Trigger:  Der EobaEventCreator-Job wird alle 5 Minuten vom Bookkeeper getriggert.
Ablauf
Laden der Befahrungsparameter f√ºr das aktuelle Timebucket aus lds_es_en_q_befahrungsparameter_eo.
Befahrungsparameter und Betriebsdaten (Erkennungsmodell) werden mit Hilfe der Betriebsdatenhistorie (erh_gdbs_betriebsdatenhistorie) √ºber eine √ºbereinstimmende Version und ID miteinander verkn√ºpft. Hierbei werden Datens√§tze verworfen, deren Ausfahrtszeitpunkt mehr als erh_eoba_betrachtungszeitraum_tage (siehe Konfiguration) zur√ºckliegt.
Ausf√ºhrung aller statischen EOBA-Regeln basierend auf den gefilterten Eingangsdaten:
Regel 510: L√ºcke bei Teilbefahrung
Regel 511: L√ºcke mit √ºberlanger Distanz
Regel 512: L√ºcke mit Transexzentrizit√§t
Regel 522: Vergeb√ºhrung mit knapper EO-Befahrung
Regel 530: Winkel der Befahrung nicht gem√§√ü Erkennungsmodell 
Regel 540: EO-Doppelbefahrung
Regel 571: L√ºcke bei fehlendem EO mit Vergeb√ºhrungskonsequenz
Regel 582: Sichere Ein-Abschnitts-L√ºcke (L√ºckenschlussvorschlag)
Als Ergebnis bleiben alle Befahrungsparameter-Erkennungsobjekt-Kombinationen √ºbrig, bei denen mindestens eine der Regeln  gegriffen hat = potentielle EOBA-Events. 
Hinweis: Wenn diese Menge leer ist, sind die nachfolgendenSchritte obsolet.
Filterung der potentiellen EOBA-Events mit Hilfe der Ausnahmeliste (erh_eo_ausnahme). 
Das Ergebnis spaltet die potenziellen EOBA-Events in folgende Mengen:  
Als solche zu speichernde EOBA-Events (keine Ausnahme greift)
Ausgenommene EOBA-Events (mindestens eine Ausnahme greift)
Anreicherung der zu speichernden EOBA-Events (4.a) mit Daten aus den FzG-Stammdaten und der Regelliste, Persistierung in erh_eoba_event.
Aggregation (Z√§hlung) der ausgenommenen EOBA-Events (4.b) und Verwendung der Events zum Update der Ausnahmestatistik in der Tabelle erh_eoba_ausnahme_statistik.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.eoba.betrachtungszeitraum_tage
5
Gibt die Anzahl der Tage an, die die Befahrungszeit des aktuellen Mautobjekts des Befahrungsparameter-Datensatzes in der Vergangenheit liegen darf, um noch im Betrachtungszeitraum zu liegen.

Tabelle 64: EobaEventCreator - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException geworfen.
[DSJ-QSErh-0120] EobaModellBewerter
Paket: de.tollcollect.zme.qs.erh.rule.eoba
In der EOBA-Modellbewertung werden Anomalien in der Erhebung zu einzelnen Erkennungsobjekten mit Auswirkung auf die zu erreichende Erhebungsqualit√§t identifiziert. Dabei werden die Ergebnisse aus der vorangegangenen EOBA-Verdichtung anhand eines Regelwerkes automatisch bewertet und Modellbewertungsergebnisse und Modellauff√§lligkeiten gespeichert. Hinweis: Die √úbermittlung der Auff√§lligkeiten an GDBS erfolgt durch die Jobs [DSJ-QSErh-0011] GdbsProductsCreator und [DSJ-QSErh-0012] GdbsUploadPump.

Abbildung 55: Job-Ablauf (schematisch): EobaModellBewerter
 Trigger:  Der EobaModellBewerter-Job wird 1x m Tag vom Bookkeeper getriggert. 
Ablauf
Laden der Daten aus den Eingangstabellen erh_eoba_eo_verdichtung_tag, erh_eoba_regel_verdichtung_eo_tag und erh_eoba_event.
Filtern der Tabellen nach Betrachtungstag (Timebucket) und BD-Instanz ‚ÄûProduktion‚Äú.
Anwenden der statischen EOBA-Modellbewertungsregeln:
Regel 2000: Bewertung Modellauff√§lligkeit Teilbefahrung EO
Regel 2001: Bewertung Modellauff√§lligkeit Erhebungsquote EO
Regel 2002: Bewertung Modellauff√§lligkeit Fehlvergeb√ºhrung EO
Regel 2003: Bewertung Modellauff√§lligkeit Befahrungs√§nderung EO
Hinweis: Die Regeln 2000 und 2001 ben√∂tigen zur Einsch√§tzung der Priorit√§t einer Auff√§lligkeit die Fahrleistungsklasse. Diese wird anhand der mo_id_eo aus dem erh_eoba_event durch Bestimmung der Fahrleistung und Fahrleistungsklasse mit Hilfe der Tabellen erh_moba_mo_verdichtung_tag (Fahrleistung f√ºr den aktuellen Tag) und erh_mo_fahrleistungsklassen (Zuordnung der Fahrleistung zu einer Fahrleistungsklasse) ermittelt.
Zusammenf√ºhren der Modellbewertungsergebnisse und Modellauff√§lligkeiten der einzelnen Regeln.
Persistieren der Modellbewertungsergebnisse in  erh_eoba_modellbewertungs_ergebnisse und der Auff√§lligkeiten  in erh_eoba_modellbewertungs_incidents.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.modellbewertung.grenzwert_anzahl_
teilbefahrung
20
Grenzwert f√ºr die Mindestanzahl an Events pro EO, die als Teilbefahrungs-Event klassifiziert wurden, damit eine Auff√§lligkeit f√ºr das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
erhebungsquote
20
Grenzwert f√ºr die Mindestanzahl an Events pro EO, die als Erhebungsquoten-Event klassifiziert wurden, damit eine Auff√§lligkeit f√ºr das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
fehlvergebuehrung
20
Grenzwert f√ºr die Mindestanzahl an Events pro EO, die als Fehlvergeb√ºhrungs-Event klassifiziert wurden, damit eine Auff√§lligkeit f√ºr das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
befahrungsaenderung
20
Grenzwert f√ºr die Mindestanzahl an Events pro EO, die als Befahrungs√§nderungs-Event klassifiziert wurden, damit eine Auff√§lligkeit f√ºr das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_stunden
5
Grenzwert f√ºr die Mindestanzahl unterschiedlicher Nutzungsstunden aller zu einer EO-ID zugeh√∂rigen EOBA-Events an diesem Tag, damit eine Auff√§lligkeit generiert wird.
erh.modellbewertung.grenzwert_anzahl_benutzer
5
Grenzwert f√ºr die Mindestanzahl unterschiedlicher Benutzer aller zu einer EO-ID zugeh√∂rigen EOBA-Events an diesem Tag, damit eine Auff√§lligkeit generiert wird.
erh.modellbewertung.grenzwert_anzahl_events
30
Grenzwert f√ºr die Mindestanzahl an Events gegen das Erkennungsobjekt in der t√§glichen Erkennungsobjektverdichtung, damit eine Auff√§lligkeit f√ºr das EO generiert wird.
erh.modellbewertung.grenzwert_eoba_quote
98
Schwellwert f√ºr die EOBA-Quote, unterhalb derer die Verdichtungsergebnisse √ºberhaupt weiter untersucht werden.

Tabelle 65: EobaModellBewerter - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException geworfen.
Bei anderen auftretenden Fehlern wird kein Ergebnis produziert.

[DSJ-QSErh-0130] EoVerdichtungDefaultValuesSetter
Paket: de.tollcollect.zme.qs.erh.rule
Der Job EoVerdichtungDefaultValueSetter hat die Aufgabe, in der Tabelle erh_eoba_eo_verdichtung_tag f√ºr den aktuellen Tag Defaultwerte f√ºr alle Erkennungsobjekte zu setzen. Er wird daher zu Beginn jedes Tages (0 Uhr) aufgerufen. Die Defaultwerte werden vom Job EobaVerdichter √ºberschrieben, sofern die Erkennungsobjekte verarbeitet werden. 
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 56: Job-Ablauf (schematisch): EoVerdichtungDefaultValuesSetter
 Trigger:  Der EoVerdichtungDefaultValuesSetter-Job wird 1x t√§glich vom Bookkeeper getriggert.
Ablauf
Aktuellen Tag aus dem Start-Timebucket des Jobs ermitteln. F√ºr diesen werden die Defaultwerte gesetzt.
Anhand der Betriebsdatenhistorie aus der Tabelle erh_gdbs_betriebsdatenhistorie alle f√ºr den aktuellen Tag g√ºltigen Erkennungsmodell-Versionen (em_version) ermitteln.
Laden aller Erkennungsobjekte aus erh_gdbs_erkennungsobjekte, die mit den ermittelten aktuell g√ºltigen EM-Versionen √ºbereinstimmen.
F√ºr alle ermittelten Erkennungsobjekte die Defaultwerte f√ºr den aktuellen Tag in die Tabelle erh_eoba_eo_verdichtung_tag schreiben.
anz_befahrungen: 0 
anz_events: 0 
anz_events_erhebungsluecken: 0
anz_eoba_quote: 0 
[DSJ-QSErh-0140] EobaVerdichtungJob
Paket: de.tollcollect.zme.qs.erh.rule
Der Job EobaVerdichtungJob f√ºhrt in unterschiedlichen Granularit√§ten die Verdichtung der EOBA-Ereignisse basierend auf den Befahrungsparametern und den EOBA-Events durch. Die Befahrungsparameter werden dabei jeweils nach Ausfahrtszeitpunkt (EO) gefiltert. Ziel der Verdichtung ist es, eine Bewertung des Optimierungspotenzials des Erkennungsmodells vornehmen zu k√∂nnen.
Die FzG-bezogenen EOBA-Verdichtungen werden ausschlie√ülich f√ºr Befahrungsparameter der BD-Instanz ‚ÄûProduktion‚Äú angefertigt.
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 57: Job-Ablauf (schematisch): EobaVerdichtungJob
 Trigger:  Der EobaVerdichtungJob wird 1x pro Stunde vom Bookkeeper getriggert.
Ablauf
Laden der Daten der abgeleiteten 12 Timebuckets von lds_es_en_q_befahrungsparameter_eo (Befahrungsparameter) und erh_eoba_event (EOBA-Events erstellt durch EobaEventCreator). 
Filterung der Befahrungsparameter nach Ausfahrzeitpunkt. Der Ausfahrzeitpunkt (EO) darf nicht mehr Tage, als der Schwellwert erh.eoba.betrachtungszeitraum_tage vorgibt, zur√ºck liegen.
Durchf√ºhrung der FzG-bezogenen Verdichtungen mit Befahrungsparametern gefiltert nach BD-Instanz ‚ÄûProduktion‚Äú und Speicherung der Ergebnisse in den entsprechenden Zieltabellen. Nach folgenden Merkmalen wird gruppiert:
FzG und Tag
Fzg, Tag und Regel
FzG und EO
FzG, EO und Regel
Durchf√ºhrung der Regelverdichtung f√ºr Erkennungsobjekte pro Stunde und pro Tag mit den gefilterten Befahrungsparametern und den EOBA-Events und Speicherung der Ergebnisse in den entsprechenden Zieltabellen. 
Durchf√ºhrung der Erkennungsobjektverdichtung pro Stunde und pro Tag mit den gefilterten Befahrungsparametern und den EOBA-Events und Speicherung der Ergebnisse in den entsprechenden Zieltabellen. 
Hinweis: Bei den Tabellen *_hist handelt es sich um Hilfstabellen zur Aktualisierung der Verdichtungen (Idempotenz). Auch die t√§glichen Verdichtungen werden st√ºndlich aktualisiert.
Allgemeine Hinweise
Defaultwerte je Erkennungsobjekt werden vom Job EoVerdichtungDefaultValuesSetter zu Beginn eines Tages geschrieben und vom EobaVerdichtungJob √ºberschrieben. Der EobaVerdichtungJob  verdichtet Befahrungsparameter die bis zu x Tage alt sind. Bei Jobstart auf einer leeren Datenbank kann es f√ºr die vier vorangegangenen Tage keine Defaultwerte und keinen BD-√Ñnderungsstatus geben, da an diesen Tagen der EoVerdichtungDefaultValuesSetter -Job nicht ausgef√ºhrt wurde.
Da die Verdichtungen der MOBA- und der EOBA-Events √§hnliche Abl√§ufe und Strukturen aufweisen, wurde durch die Klasse ModellVerdichter eine gemeinsame Basisklasse implementiert, welche die jeweilige Verdichtung (EOBA/MOBA) steuert.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.eoba.betrachtungszeitraum_tage
5
Gibt die Anzahl der Tage an, die die Befahrungszeit des aktuellen Erkennungsobjekts des Befahrungsparameter-Datensatzes in der Vergangenheit liegen darf, um noch im Betrachtungszeitraum zu liegen (es gilt der Ausfahrtszeitpunkt vom EO).

Tabelle 66: EobaVerdichtungJob - Konfiguration in Tabelle tech_rule_schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte nicht gesetzt worden sein, wird eine SchwellwertException geworfen.
[DSJ-QSErh-0150] ModellVerdichterWeekly
Paket: de.tollcollect.zme.qs.erh.rule
Der Job ModellVerdichterWeekly f√ºhrt auf Basis der st√ºndlichen Modell-Verdichtung (Ergebnisse der Jobs EobaVerdichtungJob und MobaVerdichtungJob) eine w√∂chentliche Modellverdichtung durch.
Die in den MOBA- und EOBA-Verdichtungen integrierten FzG-Verdichtungen, werden ausschlie√ülich f√ºr BD-Instanz ‚ÄûProduktion‚Äú angewendet.

Abbildung 58: Job-Ablauf (schematisch): ModellVerdichterWeekly
 Trigger:  Der ModellVerdichterWeekly-Job wird 1x pro Woche vom Bookkeeper getriggert. 
Ablauf
F√ºr alle vier Verdichtungen (MO, MO/Regel, EO und EO/Regel) ist der Ablauf jeweils folgenderma√üen:
Laden der Daten aus der Input-Tabelle f√ºr die Vorwoche. F√ºr MOBA-Verdichtungen zus√§tzliche Filterung nach BD-Instanz ‚ÄûProduktion‚Äú. Die EOBA-Verdichtungen liegen bereits nur f√ºr die produktive BD-Instanz vor.
Verdichtung der geladenen Daten und Speicherung in der entsprechenden w√∂chentlichen Verdichtungstabelle (Output-Tabelle). 
[DSJ-QSErh-0160] BetriebsdatenValidierung
Paket: de.tollcollect.zme.qs.erh.rule.bdv
Der Job vergleicht die Betriebsdaten-Instanzen ‚ÄûProduktion‚Äú und ‚ÄûValidierung‚Äú anhand der MOBA- und EOBA-Verdichtungen sowie der EDM-Metrik-Events. 
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 59: Job-Ablauf (schematisch): BetriebsdatenValidierung
 Trigger:  Der BetriebsdatenValidierung-Job wird st√ºndlich vom Bookkeeper getriggert.
Ablauf
Ermittlung des Referenzzeitpunkts anhand des aktuellen Timebuckets (Timebucket plus 1h), sowie der zu diesem Referenzzeitpunkt g√ºltigen Betriebsdaten-Version f√ºr die Betriebsdaten-Instanzen ‚ÄûValidierung‚Äú und ‚ÄûProduktion‚Äú aus der Tabelle erh_gdbs_betriebsdatenhistorie. 
Regelbasierte Abgleich der BD-Instanzen hinsichtlich der EOBA- sowie MOBA-Verdichtungen unter Verwendung der konfigurierten Schwellwerte (siehe Konfiguration):
Selektion der Verdichtungen je EO (erh_eoba_eo_verdichtung_stunde) bzw. MO (erh_moba_mo_verdichtung_stunde) innerhalb des Referenzzeitraums, bestimmt durch erh.bdval.betrachtungszeitraum_min relativ zum Referenzzeitpunkt f√ºr ‚ÄûProduktion‚Äú und ‚ÄûValidierung‚Äú.
EOs bzw. MOs mit Hilfe der Release-Notes (aus der Tabelle erh_gdbs_release_notes) unterteilen nach "neu", "ge√§ndert" und "nicht ge√§ndert".
Z√§hlen der auff√§lligen EOs und MOs aus den Verdichtungstabellen. Regelbasierter Abgleich der Anzahlen mit Hilfe der konfigurierten Schwellwerte. Bei Schwellwert√ºberschreitung, werden entsprechende Auff√§lligkeiten gez√§hlt und vermerkt.
Speicherung der Auff√§lligkeiten und der aufsummierten Anzahlen, f√ºr ‚Äûneu‚Äú, ‚Äûge√§ndert‚Äú und ‚Äûnicht ge√§ndert‚Äú in der Tabelle erh_bdv_bewertungen.
Regelbasierte Abgleich der BD-Instanzen hinsichtlich der EDM-Metrik-Events unter Verwendung der konfigurierten Schwellwerte (siehe Konfiguration):
Ermittlung der EDM-Metrik-Events f√ºr die Betriebsdaten-Instanzen ‚ÄûProduktion‚Äú und ‚ÄûValidierung‚Äú im Referenzzeitraum (definiert durch Schwellwert erh.bdval.edm_betrachtungszeitraum_min)
Summierung der Events und Bewertung der summierten Events anhand der Schwellwerte. Wenn sich die summierten Events (jeder Event-Typ enth√§lt eine Summe) um den Schwellwert in Prozent unterscheiden, werden diese als ‚Äûauff√§llig‚Äú eingestuft.
Speicherung der aufsummierten Ergebnisse f√ºr die BD-Instanzen ‚ÄûProduktion‚Äú und ‚ÄûValidierung‚Äú, sowie ggf. der Auff√§lligkeiten in der Tabelle erh_bdv_bewertungen.
Allgemeine Hinweise
Um die aktuell g√ºltige Produktions- und Validierungsversion f√ºr den Referenzzeitpunkt zu bestimmen, m√ºssen f√ºr die BD-Instanzen ‚ÄûProduktion‚Äú und ‚ÄûValidierung‚Äú mindestens die Release-Historie und die Release-Notes der Betriebsdaten vorliegen.
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.bdval.ng_befahrungen_grenzwert
10000
Grenzwert Anzahl auff√§lliger, nicht ge√§nderter Abschnitte in Bezug auf Befahrungen 
erh.bdval.ng_mobaquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, nicht ge√§nderter Abschnitte in Bezug auf MOBA-Quote
erh.bdval.ng_erhquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, nicht ge√§nderter Abschnitte in Bezug auf Erhebungsquote
erh.bdval.ng_mobaquote_toleranz
5 
Toleranz Abweichung MOBA-Quote
erh.bdval.ng_erhquote_toleranz
5
Toleranz Abweichung EOBA-Quote
erh.bdval.ng_befahrungen_toleranz
5
Toleranz Abweichung Befahrungen
erh.bdval.g_befahrungen_grenzwert
10000
Grenzwert Anzahl auff√§lliger, ge√§nderter Abschnitte in Bezug auf Befahrungen 
erh.bdval.g_mobaquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, ge√§nderter Abschnitte in Bezug auf MOBA-Quote
erh.bdval.g_erhquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, ge√§nderter Abschnitte in Bezug auf Erhebungsquote
erh.bdval.g_mobaquote_toleranz
5
Toleranz Abweichung MOBA-Quote ver√§nderter MO
erh.bdval.g_erhquote_toleranz
5
Toleranz Abweichung Erhebungsquote ver√§nderter MO
erh.bdval.g_befahrungen_toleranz
5
Toleranz Abweichung Befahrungen ver√§nderter MO
erh.bdval.n_mobaquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, ge√§nderter Abschnitte in Bezug auf MOBA-Quote
erh.bdval.n_erhquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, neuer Abschnitte in Bezug auf Erhebungsquote
erh.bdval.n_erhquote_toleranz
95
Toleranz Abweichung Erhebungsquote neue MO
erh.bdval.n_mobaquote_toleranz
95
Toleranz Abweichung MOBA-Quote neue MO
erh.bdval.ng_eobaquote_toleranz
5
Toleranz Abweichung EOBA-Quote 
erh.bdval.ng_eobaquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger Erkennungsobjekte in Bezug auf EOBA-Quote
erh.bdval.g_eobaquote_toleranz
5
Toleranz Abweichung EOBA-Quote
erh.bdval.g_eobaquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger Erkennungsobjekte in Bezug auf EOBA-Quote 
erh.bdval.n_eobaquote_toleranz
95
Toleranz Abweichung EOBA-Quote
erh.bdval.n_eobaquote_grenzwert
10000


erh.bdval.tq_sum_maut_ok_toleranz
1
Schwellwerte  f√ºr Tarfierungsqualit√§t
erh.bdval.tq_sum_ext_kosten_toleranz
1
Schwellwerte  f√ºr Tarfierungsqualit√§t
erh.bdval.tq_anz_tarif_ok_toleranz
1
Schwellwerte  f√ºr Tarfierungsqualit√§t
erh.bdval.tq_anz_tarif_nok_toleranz
1
Schwellwerte  f√ºr Tarfierungsqualit√§t

Tabelle 67: BetriebsdatenValidierung - Konfiguration in Tabelle tech_rule_schwellwert
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
erh.bdval.betrachtungszeitraum_min
120
Betrachtungszeitraum f√ºr Stundenverdichtung in Minuten
erh.bdval.edm_betrachtungszeitraum_min
120
Betrachtungszeitraum f√ºr EDM-Events in Minuten

Tabelle 68: BetriebsdatenValidierung ‚Äì Konfiguration in Tabelle tech_config
Fehlerbehandlung
Sind die Betriebsdaten inkonsistent (z.B. liegt zum aktuellen Zeitpunkt keine g√ºltige Produktions-/Validierungsversion vor), wird eine Exception (InconsistentBetriebsdatenException) geworfen und der Job abgebrochen.
Fehlende oder fehlerhafte Angaben (z.B. negative Zeiten) von Konfigurations- oder Schwellwerten f√ºhren zu Exceptions (ConfigurationException bzw. SchwellwertException) und zum Abbruch des Jobs.
[DSJ-QSErh-0170] EobaKreisClusterBuilder
Paket: de.tollcollect.zme.qs.erh.rule.eoba
Der Job EobaKreisClusterBuilder erzeugt mittels dichtebasiertem Clusterverfahren (OPTICS-Algorithmus) Cluster vom Typ ‚ÄûKreis‚Äú, die zur Analyse von Befahrungen kreisf√∂rmiger EOs eingesetzt werden Das dichtebasierte Clusterverfahren bildet Cluster als H√§ufung von Datenpunkten im d-dimensionalem Raum ab.
Hintergrund des Clustering: F√ºr jedes Erkennungsobjekt k√∂nnen mehrere m√∂gliche Befahrungen existieren. Mittels statistischer Auswertung ist es m√∂glich, die Befahrungen eines EOs- oder MOs anhand der Befahrungsparameter (z. B. Distanzen, Richtungs√§nderungen) in Erwartungen aus Befahrungsparametercluster zu gruppieren. Diese bilden f√ºr den L√ºckenschluss und die Auswertung der Befahrungen von EOs zul√§ssige Befahrungen ab. Zudem ist es m√∂glich neue oder ver√§nderte Befahrungsm√∂glichkeiten zu registrieren. 
F√ºr die Kreis-Clusterbildung werden zu jedem vorhandenen EO-ID-Paar, bestehend aus ‚Äûaktueller EO-ID‚Äú und ‚Äûletzter EO-ID‚Äú, die Befahrungsparameter aus dem Befahrungsparameter-Archiv (kurz BfpA)  gesammelt, gez√§hlt und dem Clustering unterzogen. Dabei werden folgende Informationen (Dimensionen) betrachtet:
Einfahrtswinkel
Ausfahrtswinkel
Distanz im EO
Distanz zum letzten EO
Richtungs√§nderung im EO
Richtungs√§nderung zum letzten EO


Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 60: Job-Ablauf (schematisch): EobaKreisClusterBuilder
 
 Trigger:  Der EobaKreisClusterBuilder-Job wird 1x t√§glich vom Bookkeeper getriggert.
Ablauf
Einlesen der aktuellsten Cluster-Konfiguration f√ºr den Cluster-Typ ‚ÄûKreis‚Äú aus der Tabelle erh_eoba_clusterconfig, die g√ºltig ist.
Laden der BfpA f√ºr die entsprechenden Timebuckets aus der Tabelle erh_befahrungsparameter_archiv_eo, gruppiert nach EO-ID-Paaren, BD-Instanz und BD-Version. Je  Gruppierung wird die Anzahl der BfpA gez√§hlt und gepr√ºft, ob die Anzahl innerhalb der Grenzwerte anzahl_befahrungen_mo_min und anzahl_befahrungen_mo_max der Cluster-Konfiguration liegt. 
Sofern die Grenzwerte nicht √ºber- bzw. unterschritten wurden, werden die Daten in der Tabelle erh_eoba_anzahl_eo_bef persistiert. Ist die Anzahl kleiner wird an dieser Stelle abgebrochen. Ist die Anzahl gr√∂√üer, werden zuf√§llige BfpA entsprechend dem Max-Grenzwert ermittelt (ohne Doppelungen) und ebenfalls persistiert.
Anwendung des generischen OPTICS-Algorithmus (gem√§√ü der Cluster-Konfiguration, z.B. zeitintervall, min_pts) auf die 6-dimensionalen Daten ‚ÄûAus-/Einfahrtswinkel‚Äú, ‚ÄûDistanz im/letztes EO‚Äú, ‚ÄûRichtungs√§nderung im/letztes EO‚Äú je Gruppierung. Die Anzahl der zu generierenden Cluster ist √ºber anzahl_cluster_max konfigurierbar.
Speicherung der Min- und Maxwerte f√ºr jede der 6 Dimensionen je gefundenem Cluster in der Tabelle erh_eoba_kreiscluster. 
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle erh_eoba_clusterconfig konfiguriert:
Key
Default
Beschreibung
config_id
2
Eindeutige ID der Konfiguration (Ganzzahl)
cluster_typ
2
Clustertyp: 0 = L√ºckenschluss, 1 = Tor, 2 = Kreis
anzahl_befahrungen_mo_min
50
Minimale Anzahl an Befahrungsparameter aus dem Archiv je EO-ID, die f√ºr die Durchf√ºhrung des Clustering erforderlich ist.
anzahl_befahrungen_mo_max
1000
Maximale Anzahl an Befahrungsparameter aus dem Archiv je EO-ID. Existieren mehr BfpA als durch diesen Key konfiguriert, werden genauso viele BfpA zuf√§llig ausgew√§hlt.
zeitintervall
1
Anzahl an vergangenen Tagen, f√ºr die die Befahrungsparamater aus dem Befahrungsparameter-Archiv in die Analyse einbezogen werden sollen.
min_pts
5
OPTICS-spezifischer Parameter: Mindestanzahl von Punkten in einer Nachbarschaft vom Radius Epsilon, ab der eine Gruppe von Punkten als Cluster betrachtet werden kann.
epsilon
1.0
OPTICS-spezifischer Parameter: Radius in dem nach Punkten gesucht wird, damit diese als Nachbarn gelten.
epsilon_cl
1.0
OPTICS-spezifischer Parameter: Erforderlicher Mindestabstand zwischen den Zentren von Clustern, damit diese noch als eigenst√§ndige Cluster betrachtet werden.
anzahl_cluster_max
1
Maximal erlaubte Anzahl von Clustern. Werden mehr Cluster erzeugt, werden die Cluster nicht weiter verarbeitet.
gueltig_von
2006-12-31T23:59:59Z
Startzeitpunkt des G√ºltigkeitszeitraums (Timestamp) der Konfiguration.
gueltig_bis
2006-12-31T23:59:59Z
Endzeitpunkt des G√ºltigkeitszeitraums (Timestamp) der Konfiguration.

Tabelle 69: EobaKreisClusterBuilder ‚Äì Konfiguration in Tabelle erh_eoba_clustering
Fehlerbehandlung
Sollte keine Konfiguration f√ºr den Cluster-Typ ‚ÄûKreis‚Äú vorliegen, die entsprechend des G√ºltigkeitszeitraums verwendet werden kann, wird der Job abgebrochen und eine Exception (ConfigurationException) geworfen.
[DSJ-QSErh-0180] EobaTorClusterBuilder
Paket: de.tollcollect.zme.qs.erh.rule.eoba
Der Job EobaTorClusterBuilder erzeugt mittels dichtebasiertem Clusterverfahren (OPTICS-Algorithmus) Cluster vom Typ ‚ÄûTor‚Äú, die zur Analyse von Befahrungen torf√∂rmiger EOs eingesetzt werden Das dichtebasierte Clusterverfahren bildet Cluster als H√§ufung von Datenpunkten im d-dimensionalem Raum ab.
Hintergrund des Clustering: Siehe Kapitel 5.2.5.1.20 [DSJ-QSErh-0170] EobaKreisClusterBuilder.
F√ºr die Tor-Clusterbildung werden zu jedem vorhandenen EO-ID-Paar, bestehend aus ‚Äûaktueller EO-ID‚Äú und ‚Äûletzter EO-ID‚Äú, die Befahrungsparameter aus dem Befahrungsparameter-Archiv (kurz BfpA)  gesammelt, gez√§hlt und dem Clustering unterzogen. Dabei werden folgende Informationen (Dimensionen) betrachtet:
Ausfahrtswinkel
Exzentrizit√§t
ID letztes EO
Distanz zum letzten EO
Richtungs√§nderung zum letzten EO


Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 61: Job-Ablauf (schematisch): EobaTorClusterBuilder

 Trigger:  Der EobaTorClusterBuilder-Job wird 1x t√§glich vom Bookkeeper getriggert.
Ablauf
Einlesen der aktuellsten Cluster-Konfiguration f√ºr den Cluster-Typ ‚ÄûTor‚Äú aus der Tabelle erh_eoba_clusterconfig, die g√ºltig ist. 
Laden der BfpA f√ºr die entsprechenden Timebuckets aus der Tabelle erh_befahrungsparameter_archiv_eo, gruppiert nach EO-ID-Paaren, BD-Instanz und BD-Version. Je  Gruppierung wird die Anzahl der BfpA gez√§hlt und gepr√ºft, ob die Anzahl innerhalb der Grenzwerte anzahl_befahrungen_mo_min und anzahl_befahrungen_mo_max der Cluster-Konfiguration liegt. 
Sofern die Grenzwerte nicht √ºber- bzw. unterschritten wurden, werden die Daten in der Tabelle erh_eoba_anzahl_eo_bef persistiert. Ist die Anzahl kleiner wird an dieser Stelle abgebrochen. Ist die Anzahl gr√∂√üer, werden zuf√§llige BfpA entsprechend dem Max-Grenzwert ermittelt (ohne Doppelungen) und ebenfalls persistiert.
Anwendung des generischen OPTICS-Algorithmus (gem√§√ü der Cluster-Konfiguration, z.B. zeitintervall, min_pts) auf die 4 Datendimensionen ‚ÄûAusfahrtswinkel‚Äú, ‚ÄûExzentrizit√§t‚Äú ‚ÄûDistanz letztes EO‚Äú, ‚ÄûRichtungs√§nderung letztes EO‚Äú je Gruppierung. Die Anzahl der zu generierenden Cluster ist √ºber anzahl_cluster_max konfigurierbar.
Speicherung der Min- und Maxwerte f√ºr jede der 4 Dimensionen je gefundenem Cluster in der Tabelle erh_eoba_torcluster. 
Konfiguration
Um der Anforderung der Versionierung und Nachvollziehbarkeit der Konfigurationen nachkommen zu k√∂nnen, wurde f√ºr das EOBA Clustering die separate Konfigurations-Tabelle erh_eoba_clusterconfig erstellt. Die Inhalte sind exemplarisch im Unterkapitel Konfiguration des Jobs [DSJ-QSErh-0170] EobaKreisClusterBuilder (Kapitel 5.2.5.1.20) beschrieben.
Fehlerbehandlung
Sollte keine Konfiguration f√ºr den Cluster-Typ ‚ÄûTor‚Äú vorliegen, die entsprechend des G√ºltigkeitszeitraums verwendet werden kann, wird der Job abgebrochen und eine Exception (ConfigurationException) geworfen.

[DSJ-QSErh-0190] EobaLueckenschlussClusterBuilder
Paket: de.tollcollect.zme.qs.erh.rule.eoba
Der Job EobaLueckenschlussClusterBuilder erzeugt mittels dichtebasiertem Clusterverfahren (OPTICS-Algorithmus) Cluster vom Typ ‚ÄûL√ºckenschluss‚Äú, die zur Analyse von Befahrungen von EOs mit L√ºckenschlussvorschl√§gen eingesetzt werden Das dichtebasierte Clusterverfahren bildet Cluster als H√§ufung von Datenpunkten im d-dimensionalem Raum ab.
Hintergrund des Clustering: Siehe Kapitel 5.2.5.1.20 [DSJ-QSErh-0170] EobaKreisClusterBuilder
F√ºr die L√ºckenschluss-Clusterbildung werden zu jedem vorhandenen MO-ID-Tripel die Befahrungsparameter aus dem Befahrungsparameter-Archiv (kurz BfpA)  gesammelt, gez√§hlt und dem Clustering unterzogen. Das MO-ID-Tripel besteht aus:
Aktueller MO-ID (MO-ID des aktuellen Abschnitts einer 3-Abschnitts-Kombination), 
Letzter MO-ID (MO-ID des letzten Abschnitts (L√ºcke) einer 3-Abschnitts-Kombination),
Vorletzter MO-ID (MO-ID des vorletzten Abschnitts einer 3-Abschnitts-Kombination)
Beim Clustering werden folgende Informationen (Dimensionen) betrachtet:
Distanz zum vorletzten Mautobjekt (Summe aus ‚ÄûLetztes Mautobjekt‚Äú und ‚ÄûAktuelles Mautobjekt‚Äú)
Richtungs√§nderung zum vorletzten Mautobjekt (Summe aus Letztes Mautobjekt und Aktuelles Mautobjekt)
Befahrungszeit Differenz vorletztes MO (zwischen aktuellem Mautobjekt und vorletztem Mautobjekt)
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 62: Job-Ablauf (schematisch): EobaLueckenschlussClusterBuilder
 
 Trigger:  Der EobaLueckenschlussClusterBuilder-Job wird 1x t√§glich vom Bookkeeper getriggert.
Ablauf
Einlesen der aktuellsten Cluster-Konfiguration f√ºr den Cluster-Typ ‚ÄûL√ºckenschluss‚Äú aus der Tabelle erh_eoba_clusterconfig, die gemessen am G√ºltigkeitszeitraum g√ºltig ist. 
Laden der BfpA f√ºr die entsprechenden Timebuckets aus der Tabelle erh_befahrungsparameter_archiv_mo, gruppiert nach MO-ID-Tripeln, BD-Instanz und BD-Version. Je  Gruppierung wird die Anzahl der BfpA gez√§hlt und gepr√ºft, ob die Anzahl innerhalb der Grenzwerte anzahl_befahrungen_mo_min und anzahl_befahrungen_mo_max der Cluster-Konfiguration liegt. 
Sofern die Grenzwerte nicht √ºber- bzw. unterschritten wurden, werden die Daten in der Tabelle erh_eoba_anzahl_eo_bef persistiert. Ist die Anzahl kleiner wird an dieser Stelle abgebrochen. Ist die Anzahl gr√∂√üer, werden zuf√§llige BfpA entsprechend dem Max-Grenzwert ermittelt (ohne Doppelungen) und ebenfalls persistiert.
Anwendung des generischen OPTICS-Algorithmus (gem√§√ü der Cluster-Konfiguration, z.B. zeitintervall, min_pts) auf die 3-dimensionalen Daten ‚ÄûDistanz vorletztes MO‚Äú, ‚ÄûRichtungs√§nderung vorletztes MO‚Äú ‚ÄûDistanz letztes EO‚Äú, ‚ÄûBefahrungszeit Differenz vorletztes MO‚Äú je Gruppierung. Die Anzahl der zu generierenden Cluster ist √ºber anzahl_cluster_max konfigurierbar.
Speicherung der Min- und Maxwerte f√ºr jede der 3 Dimensionen je gefundenem Cluster in der Tabelle erh_eoba_lueckenschlusscluster. 
Konfiguration
Um der Anforderung der Versionierung und Nachvollziehbarkeit der Konfigurationen nachkommen zu k√∂nnen, wurde f√ºr das EOBA Clustering die separate Konfigurations-Tabelle erh_eoba_clusterconfig erstellt. Die Inhalte sind exemplarisch im Unterkapitel Konfiguration des Jobs [DSJ-QSErh-0170] EobaKreisClusterBuilder (Kapitel 5.2.5.1.20) beschrieben.
Fehlerbehandlung
Sollte keine Konfiguration f√ºr den Cluster-Typ ‚ÄûL√ºckenschluss‚Äú vorliegen, die entsprechend des G√ºltigkeitszeitraums verwendet werden kann, wird der Job abgebrochen und eine Exception (ConfigurationException) geworfen.
[DSJ-QSErh-0200] MonpErhEventEmitter
Paket: de.tollcollect.zme.qs.erh.rule.monp
Der Job MonpErhEventEmitter erzeugt erhebungsspezifische Metrik-Events f√ºr die Monitoring-Plattform (MonP) und reiht diese an die QS-AV-interne globale Versand-Warteschlange ein. 
Einen allgemeinen √úberblick √ºber die Schnittstelle und die beteiligten Programmteile finden Sie im Kapitel 6.1.6  SST 819 QS-AV ü°™ MonP unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Informationen zu den zu √ºbertragenen Events entnehmen Sie bitte der Schnittstellenspezifikation im Referenzdokument [8] [S_819_SST_00] SST 819 SST-Spezifikation QS-AV ‚Äì MonP.
Die folgende Tabelle listet die Input-Tabellen die f√ºr die Eventerstellung durch den MonpErhEventEmitter herangezogen werden. 
Input-Tabelle
Events
erh_onq_15min_2
ONQ betreffend (z.B.: OnqGruenGesamt, OnqGruenBab)
erh_onq_dsrc_15min
DSRC betreffend (z.B.: DsrcQuoteBS, DsrcQuoteBab)

Tabelle 70: Input-Tabellen der erhebungsspezifischen Metrik-Events
Bei der Einreihung der Events in die Versand-Warteschlange werden folgende Output-Tabellen beschrieben, um den Versand durch den ScheduledMonpSender (siehe auch Kapitel 6.1.6 SST 819 QS-AV ü°™ MonP) zu erm√∂glichen:
Output-Tabelle
Aufgabe
tech_monp_pending_metric_event
Warteschlange zu versendender Events.

Tabelle 71: Output-Tabellen der der erhebungsspezifischen Metrik-Events
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 63: Job-Ablauf (schematisch): MonpErhEventEmitter
Trigger:  Der MonpErhEventEmitter-Job wird alle 15 Minuten vom Bookkeeper gestartet.
Ablauf
Einlesen der Eingangsdaten aus den oben genannten Input-Tabellen, zur Ermittlung des Event-Inhalts, der f√ºr das Monitoring verwendet werden soll (siehe Tabelle 70: Input-Tabellen der erhebungsspezifischen Metrik-Events).
Generierung der Metrik-Events gem√§√ü Schnittstellenspezifikation. Daf√ºr werden die Daten in ein vorgegebenes JSON-Format eingebettet.
Einreihung der erstellten Events in die globale Versand-Warteschlange (Tabelle tech_monp_pending_metric_event). 
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
erh.monp.plattform_prefix
Plattform
Pr√§fix f√ºr den FzG Plattformnamen, der im Event-Inhalt von einigen MonP-Events verwendet wird. Beispiel: Der Pr√§fix "Plattform" wird mit dem Herstellernamen ‚ÄûBosch1G‚Äú zusammengesetzt zu "Plattform Bosch1G".

Tabelle 72: MonpErhEventEmitter ‚Äì Konfiguration in Tabelle tech_config
F√ºr allgemeine Konfigurationseinstellungen bez√ºglich der MonP-Schnittstelle siehe Kapitel 6.1.6 SST 819 QS-AV ü°™ MonP unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation. 
Wichtige Hinweise
Der Inhalt der Metrik-Events ist abh√§ngig von der Bef√ºllung der Tabelle fzg_hersteller. Die Spalte SENDEN_AN_MONP bestimmt, ob ein FzG-Typ als Gruppierungskriterium und somit Daten f√ºr diesen FzG-Typ in das Event mit aufgenommen werden oder nicht. Die Schreibweise des Herstellers muss in der Spalte HERSTELLER_NAME_MONP definiert werden. Ist diese Spalte nicht gesetzt (null), so wird anstelle dessen eine Zusammensetzung aus HERSTELLER_NAME_ANZEIGE und HERSTELLER_NAME verwendet. Dem ermittelten Herstellernamen wird in jedem Fall das plattform_prefix aus der Konfiguration vorangestellt, um auf die im Event verwendete Schreibweise der Keys zu kommen. Aus "Bosch1G" wird somit "Plattform Bosch1G" (siehe Konfiguration).
[DSJ-QSErh-0210] MonpNeueSollzuordnungenRse
Paket: de.tollcollect.zme.qs.erh.rule.monp
Der Job MonpNeueSollzuordnungenRse dient der Erstellung der beiden MonP-Information-Events NeueZuordRseMoBAB und NeueZuordRseEoBS. Die Events werden erzeugt, sobald sich die Sollzuordnungen von RsE-EO (BS) bzw. RsE-MO (BAB) ge√§ndert haben (siehe dazu auch Job [DSJ-QSErh-0021] ProcessOnqSollzuordnungJob in Kapitel 5.2.5.1.5). 
Einen allgemeinen √úberblick √ºber die Schnittstelle und die beteiligten Programmteile finden Sie im Kapitel 6.1.6  SST 819 QS-AV ü°™ MonP unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.

Abbildung 64: Job-Ablauf (schematisch): MonpNeueSollzuordnungenRse
 Trigger:  Der MonpNeueSollzuordnungenRse-Job wird 1x t√§glich vom Bookkeeper getriggert.
Ablauf
Generierung der Info-Events f√ºr MO/BAB:
Einlesen der RsE-MO-Sollzuordnungen (erh_onq_sollzuordnung_mo_tag) und der zugeh√∂rigen RsE-Kontakte (erh_onq_rse_kontakt_id_korrigiert)
Erzeugen eines Info-Events f√ºr jede neue Sollzuordnung des aktuellen Tages und Einreihung des Events in die Warteschlange (tech_monp_pending_information_event). 
Generierung der Info-Events f√ºr EO/BS:
Einlesen der RsE-EO-Sollzuordnungen (erh_onq_sollzuordnung_eo_tag) und der zugeh√∂rigen RsE-Kontakte (erh_onq_rse_kontakt_id_korrigiert)
Erzeugen eines Info-Events f√ºr jede neue Sollzuordnung des aktuellen Tages und Einreihung des Events in die Warteschlange (tech_monp_pending_information_event).
[DSJ-QSErh-0220] CognosErhebungsquotenUpdater
Paket: de.tollcollect.zme.qs.erh.rule.cognos
Der Job stellt die Kennzahlen zu Mautobjekten f√ºr die Cognos Schnittstelle (SST 884) zur sp√§teren Erstellung des Erhebungsquotenreports bereit. 
Einen allgemeinen √úberblick √ºber die Schnittstelle und die beteiligten Programmteile finden Sie im Kapitel 6.1.7 SST 884 QS-AV ü°™ Cognos unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
 
Abbildung 65: Job-Ablauf (schematisch): CognosErebungsquotenUpdater
Trigger:  Der CognosErhebungsquotenUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
L√∂schen aller Daten aus der Zieltabelle tech_cognos_erhebungsquoten
(Hintergrund: Die Daten werden immer f√ºr den gesamten Betrachtungszeitraum neu generiert.)
Zusammenf√ºhrung der Daten der Input-Tabellen (Join). Betrachtet werden dabei alle Daten, die dem konfigurierten Betrachtungszeitraum entsprechen (definiert durch Konfigurations-Key erh.cognos.datenumfang_tage_erh). Die Daten werden √ºbernommen und ggf. zusammengesetzt oder berechnet. Beispiele sind: 
zeitscheibe_start/ende ü°™ Nutzungstag als UNIX-Timestamp in Sekunden
streckenname ü°™ Streckenname aus den GDBS-Tabellen f√ºr das Mautobjekt
fahrleistungsklasse ü°™ Errechnet aus dem Nutzungstag und den Fahrleistungsklassen der Tabelle erh_mo_fahrleistungsklassen
bewertungsergebnis ü°™ √úbernahme des Feldes beschreibung aus der Tabelle erh_modellbewertungen f√ºr das erste passende Nicht-Null-Element, aufsteigend nach regel_id und regel_sub_id
Speicherung der ermittelten Daten in der Tabelle tech_cognos_erhebungsquoten.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
erh.cognos.datenumfang_tage_erh
5
Anzahl der Tage (r√ºckwirkend), f√ºr die die Erhebungsquoten f√ºr Cognos bereitgestellt werden sollen; definiert den Betrachtungszeitraum.

Tabelle 73: CognosErhebungsquotenUpdater ‚Äì Konfiguration in Tabelle tech_config
Fehlerbehandlung
Fehler w√§hrend des Joins, die aufgrund von null-Werten der Attribute auftreten werden abgefangen, sofern es sich bei dem Attribute nicht um einen Prim√§rschl√ºssel handelt. Die Zieltabelle ist so gestaltet, dass s√§mtliche Attribute, ausgenommen Prim√§rschl√ºssel, null sein k√∂nnen. 

[DSJ-QSErh-0230] CognosOnqUpdater
Paket: de.tollcollect.zme.qs.erh.rule.cognos
Der Job stellt ONQ-spezifische Kennzahlen f√ºr die Cognos Schnittstelle (SST 884) bereit. 
Einen allgemeinen √úberblick √ºber die Schnittstelle und die beteiligten Programmteile finden Sie im Kapitel 6.1.7 SST 884 QS-AV ü°™ Cognos unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
 
Abbildung 66: Job-Ablauf (schematisch): CognosOnqUpdater
Trigger:  Der CognosOnqUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
Zusammenf√ºhrung der Daten der Input-Tabellen (Join). Betrachtet werden dabei alle Daten, die dem konfigurierten Betrachtungszeitraum (definiert durch Konfigurations-Key erh.cognos.datenumfang_tage_onq) entsprechen. 
L√∂schen aller Daten aus der Zieltabelle tech_cognos_onq.
(Hintergrund: Die Daten werden immer f√ºr den gesamten Betrachtungszeitraum neu generiert.) 
Speicherung der neu ermittelten Daten in der Tabelle tech_congos_onq.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
erh.cognos.datenumfang_tage_onq
5
Anzahl der Tage (r√ºckwirkend), f√ºr die die ONQ-Kennzahlen f√ºr Cognos bereitgestellt werden sollen; definiert den Betrachtungszeitraum.

Tabelle 74: CognosOnqUpdater ‚Äì Konfiguration in Tabelle tech_config
Fehlerbehandlung
M√∂gliche Fehlerquelle: In sehr seltenen F√§llen, kann es zu null-Werten in der Output-Tabelle kommen, sollten die FzG-Typen der beiden Input-Tabellen sich unterscheiden. Bei einem Join kann somit keine Zuordnung vorgenommen werden, was dazu f√ºhrt, dass null-Werte f√ºr die betroffenen Attribute der Output-Tabelle gesetzt werden.
[DSJ-QSErh-0240] CognosDsrcUpdater
Paket: de.tollcollect.zme.qs.erh.rule.cognos
Der Job stellt DSRC-spezifische Kennzahlen, welche die Basis f√ºr die DSRC-Fehlerquotenberechnung in QS-AV bilden, f√ºr die Cognos Schnittstelle (SST 884) bereit. 
Einen allgemeinen √úberblick √ºber die Schnittstelle und die beteiligten Programmteile finden Sie im Kapitel 6.1.7 SST 884 QS-AV ü°™ Cognos unter Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation.
Die folgende Abbildung stellt den Jobablauf schematisch dar.
 
Abbildung 67: Job-Ablauf (schematisch): CognosDsrcUpdater
Trigger:  Der CognosDsrcUpdater-Job wird 1x am Tag vom Bookkeeper gestartet.
Ablauf
Laden der DSRC-Verdichtungswerte f√ºr den konfigurierten Zeitraum aus der Tabelle erh_onq_dsrc_verdichtung_stunde gefiltert nach Stra√üentypen BS und BAB, um sicherzustellen, dass keine Daten doppelt gez√§hlt werden. 
Bilden der Anzahlen von Gut- und Schlechtf√§llen, gruppiert nach Tag und FzG-Typ f√ºr den konfigurierten Betrachtungszeitraum (siehe Konfigurations-Key erh.cognos.datenumfang_tage_dsrc).
Leeren der Zieltabelle tech_cognos_dsrc. 
(Hintergrund: Die Daten werden immer f√ºr den gesamten Betrachtungszeitraum neu generiert.)
Speicherung der neu ermittelten Daten in der Tabelle tech_cognos_dsrc.
Konfiguration
Die Konfiguration des Jobs erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys:
Key
Default
Beschreibung
erh.cognos.datenumfang_tage_dsrc
5
Anzahl der Tage (r√ºckwirkend), f√ºr die DSRC-Daten f√ºr Cognos bereitgestellt werden sollen; definiert den Betrachtungszeitraum.

Tabelle 75: CognosDsrcUpdater ‚Äì Konfiguration in Tabelle tech_config
[DSJ-QSErh-0250] ModellBewertungenRuleJobExecutor
Paket: de.tollcollect.zme.qs.erh.rule.modellbewertung
Dieser Job bewertet die Qualit√§t von Maut- und Erkennungsobjekten der produktiven Betriebsdaten. Dazu werden die Ergebnisse der MOBA-Verdichtung regelbasiert ausgewertet und als Modellbewertungsergebnisse und Modellauff√§lligkeiten gespeichert. Der Versand an den Grunddatenprozess ist nicht Teil dieses Jobs, sondern erfolgt mittels der Jobs [DSJ-QSErh-0011] GdbsProductsCreator und [DSJ-QSErh-0012] GdbsUploadPump. 

Abbildung 68: Job-Ablauf (schematisch): ModellBewertungenRuleJobExecutor
Trigger: Der ModellBewertungenRuleJobExecutor-Job wird 1x t√§glich vom Bookkeeper gestartet.
Ablauf
Pr√ºfung anhand der Tabelle tech_kalender, ob f√ºr den vom Bookkeeper √ºbergebenen Bewertungstag eine Modellbewertung vorgenommen werden soll. Soll keine Bewertung erfolgen, wird der Job beendet.
Einlesen folgender Daten f√ºr den Bewertungstag:
MOBA-Events (erh_moba_event_auswahl_fuer_bewertung)
MOBA-Verdichtungsergebnisse je Tag bzgl. MOBA- und Erhebungsquoten (erh_moba_mo_verdichtung_tag_by_tag_mobaquote_mvw und erh_moba_mo_verdichtung_tag_by_tag_erhquote_mvw)
Fahrleistungsklassen (erh_mo_fahrleistungsklassen)
Schwellwertbasierte Ausf√ºhrung der Bewertungsregeln.
Persistierung der Bewertungsergebnisse in der Tabelle erh_modellbewertungen. 
Konfiguration
Die ben√∂tigten Schwellwerte werden in der Cassandra-Tabelle tech_rule_schwellwert konfiguriert:
Key
Default
Beschreibung
erh.modellbewertung.max_erhebungsquote
99
Maximaler Schwellwert unterhalb (<=) dessen weitere Bedingungen pro Mautobjekt gepr√ºft werden.
erh.modellbewertung.min_anzahl_erhebungen_
auffaelliges_mautobjekt
500
Minimaler Schwellwert ab dem (>=) weitere Bedingungen pro Mautobjekt gepr√ºft werden.
erh.modellbewertung.min_anzahl_
erhebungsluecken_auffaelliges_mautobjekt
50
Minimaler Schwellwert ab dem (>=) weitere Bedingungen pro Mautobjekt gepr√ºft werden. Achtung: bei Regel 1001 begrenzt dieser Schwellwert die minimale Anzahl von Doppelerhebungen und nicht von Erhebungsl√ºcken.
erh.modellbewertung.max_anzahl_benutzer_
indikator_baustellenverkehr
10
Maximaler Schwellwert Anzahl verschiedener Nutzer unterhalb (<=) dessen eine Erhebungsanomalie pro Mautobjekt als Baustellenverkehr kategorisiert wird.
erh.modellbewertung.max_anzahl_stunden_fuer_
temporaere_erhebungsanomalie
5
Maximales Zeit-Intervall in Stunden eines Bewertungstages innerhalb (<=) dessen das Auftreten einer Erhebungsanomalie pro Mautobjekt als tempor√§r kategorisiert wird.
erh.modellbewertung.id_string_prod_bd_instanz
PROD
String-Konstante zur Identifizierung der Produktions-Instanz.
erh.modellbewertung.max_mobaquote
98
Maximaler Schwellwert unterhalb (<=) dessen weitere Bedingungen pro Mautobjekt gepr√ºft werden.
erh.modellbewertung.min_anzahl_events_
auffaelliges_mautobjekt
50
Minimale Anzahl an Events pro Mautobjekt, ab der (>=) weitere Bedingungen gepr√ºft werden.

Tabelle 76: ModellBewertungenRuleJobExecutor ‚Äì Konfiguration in Tabelle tech_rule_Schwellwert
Fehlerbehandlung
Sollte einer der Schwellwerte  nicht gesetzt worden sein, wird eine SchwellwertException geworfen.
Anonymisierung, Archivierung und L√∂schung 
Die Anonymisierung, Archivierung und L√∂schung von personenbezogenen Daten der Komponente qs-erh erfolgen in der Regel via TTLs (Time-To-Live: Standardl√∂schintervalle, die bei Anlage der Datenbanktabellen definiert werden). Vorgaben und Job-Details sind dem Systeml√∂schkonzept [14] zu entnehmen.
Folgende Jobs werden dort zus√§tzlich beschrieben:
BefahrungsparameterArchivCreation

qs-common-test: Testspezifische Basiskomponenten und Utilities
Dieses Paket enth√§lt testspezifische Utilities und Tools. Es wird ausschlie√ülich im Test-Scope als Br√ºcke zur Ausf√ºhrung der Integrations- und Systemtests eingesetzt.
Timebucket-Switcher Applikation
qs-timebuchetswitcher
Der Timebucket-Switcher ist eine propriet√§re Applikation, welche die Timebuckets f√ºr die Cassandra-Tabellen aktualisiert und somit die Datenverarbeitung durch den Bookkeeper taktet. Jede Aktualisierung eines Timebuckets wird im Zustands-Log protokolliert. Anhand dieser Information kann der Bookkeeper entscheiden, welche Jobs gestartet werden k√∂nnen. 
Die Timebucket-Switcher Applikation ist single-threaded und switcht je Durchlaufzyklus alle konfigurierten Timebuckets gem√§√ü ihres festgelegten Intervalls. Nach jedem Zyklus wird 2 Sekunden gewartet bevor ein neuer Zyklus gestartet wird.
Ablauf
 
Abbildung 69: Timebucket-Switcher ‚Äì Klassen und Objekte (Ablauf)
Die Klasse LongRunner wird bei Programmstart instanziiert und gestartet. Sie kontrolliert die Abarbeitung der Timebucket-Switcher-Aufgaben √ºber die Klasse Timebucket-Switcher, die analog zur Klasse JobExecutor des Bookkeepers das Trait LongRunnable erweitert und durch den LongRunner zyklisch ausgef√ºhrt wird. 
Der Timebucket-Switcher liest TimebucketSchemata von Timer- und Tabellen-Timebuckets aus der Konfiguration (RuleConfigTimebuckets und aktualisiert die Timebucket-Zeitstempel dementsprechend in der Tabelle en_q_timebuckets (Timebucket-Katalog) = ‚Äûswitch‚Äú. Die Ergebnisse der Verarbeitung werden im Zustands-Log (bkpr_eventlog) durch neue Protokolleintr√§ge vom Typ 1  (LDSTimebucketSwitched) mit dem Zustand ‚Äûcreated‚Äú festgehalten (siehe dazu auch Kapitel 5.1.6.4 Datenbanktabellen). 
Allgemeine Hinweise
Der Timebucket-Switcher erzeugt auch f√ºr Zeitabschnitte der Eingangsdatenstr√∂me, die z.B. durch eine Downtime des Systems, beim Switch √ºbersprungen wurden, Protokolleintr√§ge im Zustands-Log (bkpr_eventlog) an, um die Job-Verarbeitung im Bookkeeper nicht negativ zu beeinflussen.
Konfiguration
Die Konfiguration des Timebucket-Switchers erfolgt √ºber die Klasse RuleConfigTimebuckets, die TimebucketSchemas definiert. Details zu der Konfiguration sind dem Kapitel 5.1.5 Konfiguration im Schwerpunktkapitel: Bookkeeper (qs-common) zu entnehmen.

Abbildung 70: Timebucket-Switcher ‚Äì Klassen und Objekte (Konfiguration)
DSE-Ring (Datenhaltung)
In diesem Abschnitt werden die Soll- und Schwellwerte, die innerhalb der Persistenzschicht gehalten werden, dokumentiert.
Informationen zum Aufbau und der Technologiebasis des DSE-Rings entnehmen Sie bitte dem gleichnamigen Architekturkapitel 4.2.2 DSE-Ring (Datenhaltung).
Das Datenmodell (siehe Referenzdokument [13]) wird zusammen mit den Software-Lieferungen zur Verf√ºgung gestellt.
Soll- und Schwellwerte
Alle Soll- und Schwellwerte der QS-AV Applikation sowie der Timebucket-Switcher Applikation, mit Ausnahme des EOBA Clustering (siehe Kapitel 5.4.1.9 EOBA Clustering), werden in der Cassandra-Tabelle tech_rule_schwellwert verwaltet. Das EOBA Clustering wurde in die separate Konfigurations-Tabelle erh_eoba_clusterconfig ausgelagert, um den Anforderungen der Versionierung und Nachvollziehbarkeit der Konfiguration nachkommen zu k√∂nnen.
Die Tabellen sind im laufenden Betrieb konfigurierbar und k√∂nnen somit das Verhalten der Applikationssteuerung und insbesondere der Qualit√§tssicherungsalgorithmen steuern. 
Hinweis: Die Soll- und Schwellwerte sind durch den Fachbereich vorgegeben bzw. werden durch den Fachbereich auch im Betrieb verwaltet und werden keiner inhaltlichen Pr√ºfung unterzogen.
Konfigurationshinweis: Double-Werte m√ºssen mit einem Punkt getrennt werden. Beispiel: 0.75
FzG (MDN-Normierung)
Key in Cassandra-Tabelle
Default
Beschreibung
fzg.mdn.1_batt
100
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 1 (Grundig).
fzg.mdn.2_batt
101
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 2 (Siemens).
fzg.mdn.3_batt
102
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_batt
103
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_batt
104
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_batt
105
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_batt
106
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_batt
107
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_batt
108
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_batt
110
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_batt
111
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_batt
112
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_batt
113
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_batt
114
Minimalwert der Batteriespannung f√ºr Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_capa_flash
200
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 1 (Grundig).
fzg.mdn.2_capa_flash
201
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 2 (Siemens).
fzg.mdn.3_capa_flash
202
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_capa_flash
203
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_capa_flash
204
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_capa_flash
205
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_capa_flash
206
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_capa_flash
207
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_capa_flash
208
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_capa_flash
210
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_capa_flash
211
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_capa_flash
212
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_capa_flash
213
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_capa_flash
214
Minimalwert der freien Speicherkapazit√§t (Flash) f√ºr Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 1 (Grundig).
fzg.mdn.2_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 2 (Siemens).
fzg.mdn.3_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_sat_fs
40
Minimalwert der Satellitenfeldst√§rke f√ºr Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 1 (Grundig).
fzg.mdn.2_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 2 (Siemens).
fzg.mdn.3_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_sat_fs_sprung
10
Maximalwert der Satellitenfeldst√§rkenspr√ºnge f√ºr Hersteller mit ID 14 (Bosch 2G Pilot).
fzg.mdn.1_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 1 (Grundig).
fzg.mdn.2_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 2 (Siemens).
fzg.mdn.3_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 3 (Grundig Pilot).
fzg.mdn.4_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 4 (Siemens Pilot).
fzg.mdn.5_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 5 (Grundig Pilot 16MB).
fzg.mdn.6_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 6 (Grundig 16MB).
fzg.mdn.7_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 7 (Siemens DIN-Schacht Pilot).
fzg.mdn.8_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 8 (Siemens DIN-Schacht).
fzg.mdn.9_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 9 (Siemens 1373++ Pilot).
fzg.mdn.10_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 10 (Siemens 1373++).
fzg.mdn.11_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 11 (Bosch DIN-Schacht).
fzg.mdn.12_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 12 (Bosch DIN-Schacht Pilot).
fzg.mdn.13_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 13 (Bosch 2G).
fzg.mdn.14_wtr_sprung
10
Maximalwert der Wheel-Tick-Sprungrate f√ºr Hersteller mit ID 14 (Bosch 2G Pilot).

Tabelle 77: Soll- und Schwellwerte f√ºr FzG (MDN-Normierung) 
FzG-Bewertung
Die Schwellwerte setzen sich zusammen aus 19 FzG-Fehlerbildern zu je folgenden 3 Kategorien:
aktiveTage: Anzahl der aktiven Tage (aktiver Tag = Eintrag in FzG-Verdichtungstabelle existiert), die pro ehk_id bei der Bewertung ber√ºcksichtigt werden
fehler: Min. Anzahl an Fehlern pro aktivem Tag, damit der Tag als fehlerhaft bewertet wird
aktiveTageFehler: Min. Anzahl an fehlerhaften Tagen im Zeitraum der aktiven Tage, damit die FzG-Bewertung das Fehlerbild mittels TRUE als ‚Äûauff√§llig‚Äú bewertet. 
Die Schwellwert-Keys die sich daraus ergeben haben folgendes Muster:
fzg.bewertung.<Fehlerbild>.<Kategorie>
Die folgende Tabelle enth√§lt alle Defaultwerte f√ºr die Schwellwerte unter Ber√ºcksichtigungen des Mapping von Fehlerbild auf Kategorie:
Fehlerbild
aktiveTage
fehler
aktiveTageFehler
fallback
1
1
1
dienst_de_aktiv_defekt
1
1
1
sw_version_nicht_aktuell
1
1
1
batt_spannung_gering
5
1
3
dauerplus
1
1
1
keine_tachodaten
5
1
4
keine_gyrodaten
5
1
4
tachodaten_unplausibel
5
1
4
gyrodaten_unplausibel
5
1
4
capa_flash
5
1
4
uw_errorflag
5
1
4
probefahrt_nicht_bestanden
5
1
1
ehk_fehler
5
1
4
gps_fehler
5
1
4
zeitsprung
5
1
4
wtr_sprung
5
1
4
sat_fs
5
1
4
mac_fehler
1
1
1
loop_fehler
1
1
1

Tabelle 78: Soll- und Schwellwerte f√ºr FzG-Bewertung
Eventnormierung
Key in Cassandra-Tabelle
Default
Beschreibung
fzg.deklaration_events.interpretation.dekl_
p_gewicht_ohne_achs.max_zeitdifferenz_
sekunden
10000
Maximale Zeitdifferenz in Sekunden zwischen passivem Gewichtsdeklarations-Event und passivem Achsdeklarations-Event, damit das Regelbit rb_dekl_p_gewicht_ohne_achs den Wert FALSE hat. 
Beachte: Obere Grenze nicht eingeschlossen.
Wichtig: Dieser Schwellwert darf nicht gr√∂√üer sein als die Timebucket-Gr√∂√üe der Eingangstabelle lds_es_en_q_ deklaration_events. Andernfalls muss die Bookkeeper-Konfiguration angepasst werden. 

Tabelle 79: Soll- und Schwellwerte f√ºr Eventnormierung
ONQ/DSRC
Key in Cassandra-Tabelle
Default
Beschreibung
erh.onq.led_rot_grund_query
19028,
19478,
2620
Dient zum Ausschluss von RsE-Kontakten. 
Die hier konfigurierten Rotgr√ºnde (Nummern)  indizieren manuelles Verfahren.
erh.onq.led_rot_grund_rule_sperre_query
33420, 
35376
Dient zum Ausschluss von RsE-Kontakten.
Die hier konfigurierten Rotgr√ºnde (Nummern) indizieren gesperrte FzG.
erh.onq.bd_instanz_produktion
1
RsE-Kontakte mit bd_instanz=1 stammen aus der produktiven BD-Instanz.
erh.onq.konau_nummernkreis_oben
5000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konau_nummernkreis_unten
4000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten) , zur Kennzeichnung der Kontrollart KonAu.
erh.onq.konsl_nummernkreis_oben
8000
Schwellwert f√ºr den Nummernkreis der RsE-ID (oben) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.konsl_nummernkreis_unten
6000
Schwellwert f√ºr den Nummernkreis der RsE-ID (unten) , zur Kennzeichnung der Kontrollart KonSL.
erh.onq.zeit_onq_sek
3600
Anzahl der Sekunden r√ºckwirkend des RSE-Kontaktzeitpunkts, der f√ºr die Berechnung betrachtet wird.
erh.onq.led_farbe_rot
0
LED-Farbe der RsE-Kontakte mit led_farbe=0 ist rot.
erh.onq.led_farbe_gruen
1
LED-Farbe der RsE-Kontakte mit led_farbe=1 ist gr√ºn.
erh.onq.anz_gruen
3
Schwellwert f√ºr die Mindestanzahl an RsE-Kontakten mit led_farbe gruen f√ºr eine RsE-ID.
erh.onq.ant_ausfahrt
20
Schwellwert f√ºr die erlaubte Quote an Ausfahrten f√ºr eine RsE.
erh.onq. anz_rse_gesamt_eo
280
Anzahl der RSE-Kontakte, die mindestens notwendig sind, um eine neue EO-Sollzuordnung zu berechnen.
erh.onq. anz_rse_gesamt_mo
280
Anzahl der RSE-Kontakte, die mindestens notwendig sind, um eine neue MO-Sollzuordnung zu berechnen.
erh.onq. anz_rse_gesamt_eo_mo
280
Anzahl der RSE-Kontakte, die mindestens notwendig sind, um eine neue EO-MO-Sollzuordnung zu berechnen.
erh.onq.erhebungsparameterabfahrt
2
RsE-Kontakte mit erhebungsparameter=2 indizieren Abfahrten.
erh.onq.schwellwert_erhebung_tage_zurueck
5
Zeitraum in Tagen, wie lange ein Befahrungsparameter r√ºckwirkend f√ºr die Berechnungen herangezogen werden darf.
erh.onq.rse_suchfenster_bs
3600
Sekunden r√ºckwirkend zum RSE-Kontaktzeitpunkt: Festlegung des Starts des Suchfensters f√ºr die Berechnung der DSRC-Quote (Ende = Kontaktzeitpunkt).
erh.onq.dsrc_quote
98
Mindestwert f√ºr die DSRC-Quote einer RsE, damit  sie f√ºr die FzG-bezogene DSRC-Quoten-Verdichtung ber√ºcksichtigt wird.
Hintergrund: F√ºr die Verdichtung d√ºrfen nur Daten zu RsEs mit einer DSRC-Quote gr√∂√üer des angegebenen Schwellwertes ber√ºcksichtigt werden, um die FzG-bezogenen Verdichtungsergebnisse nicht zu verf√§lschen.
erh.onq.use_onq_workaround
true
Gibt an ob der Workaround des RsE-ID-Mappings f√ºr die Verarbeitung verwendet werden soll. 
true= Workaround soll verwendet werden, false= Workaround soll nicht verwendet werden.

Tabelle 80: Soll- und Schwellwerte f√ºr ONQ/DSRC
MOBA
Key in Cassandra-Tabelle
Default
Beschreibung
erh.moba.betrachtungszeitraum_tage
5
Konfiguration des Betrachtungszeitraums. Es d√ºrfen nur Befahrungen in MOBA und in der Verdichtung ber√ºcksichtigt werden, die gemessen an der Befahrungszeit nicht √§lter sind, als die konfigurierbare Anzahl von Tagen (Default-Wert).

Tabelle 81: Soll- und Schwellwerte f√ºr MOBA
EOBA
Key in Cassandra-Tabelle
Default
Beschreibung
erh.eoba.betrachtungszeitraum_tage
5
Gibt die Anzahl der Tage an, die die Befahrungszeit des aktuellen Erkennungsobjekts des Befahrungsparameter-Datensatzes in der Vergangenheit liegen darf, um noch im Betrachtungszeitraum zu liegen (es gilt der Ausfahrtszeitpunkt vom EO).

Tabelle 82: Soll- und Schwellwerte f√ºr EOBA
Modellbewertung
Key in Cassandra-Tabelle
Default
Beschreibung
erh.modellbewertung.grenzwert_anzahl_
teilbefahrung
20
Grenzwert f√ºr die Mindestanzahl an Events pro EO, die als Teilbefahrungs-Event klassifiziert wurden, damit eine Auff√§lligkeit f√ºr das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
erhebungsquote
20
Grenzwert f√ºr die Mindestanzahl an Events pro EO, die als Erhebungsquoten-Event klassifiziert wurden, damit eine Auff√§lligkeit f√ºr das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
fehlvergebuehrung
20
Grenzwert f√ºr die Mindestanzahl an Events pro EO, die als Fehlvergeb√ºhrungs-Event klassifiziert wurden, damit eine Auff√§lligkeit f√ºr das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
befahrungsaenderung
20
Grenzwert f√ºr die Mindestanzahl an Events pro EO, die als Befahrungs√§nderungs-Event klassifiziert wurden, damit eine Auff√§lligkeit f√ºr das EO generiert wird.
erh.modellbewertung.grenzwert_anzahl_
stunden
5
Grenzwert f√ºr die Mindestanzahl unterschiedlicher Nutzungsstunden aller zu einer EO-ID zugeh√∂rigen EOBA-Events an diesem Tag, damit eine Auff√§lligkeit generiert wird.
erh.modellbewertung.grenzwert_anzahl_
benutzer
5
Grenzwert f√ºr die Mindestanzahl unterschiedlicher Benutzer aller zu einer EO-ID zugeh√∂rigen EOBA-Events an diesem Tag, damit eine Auff√§lligkeit generiert wird.
erh.modellbewertung.grenzwert_anzahl_events
30
Grenzwert f√ºr die Mindestanzahl an Events gegen das Erkennungsobjekt in der t√§glichen Erkennungsobjektverdichtung, damit eine Auff√§lligkeit f√ºr das EO generiert wird.
erh.modellbewertung.grenzwert_eoba_quote
98
Schwellwert f√ºr die EOBA-Quote, unterhalb derer die Verdichtungsergebnisse √ºberhaupt weiter untersucht werden.
erh.modellbewertung.max_erhebungsquote
99
Maximaler Schwellwert unterhalb (<=) dessen weitere Bedingungen pro Mautobjekt gepr√ºft werden.
erh.modellbewertung.min_anzahl_erhebungen_auffaelliges_mautobjekt
500
Minimaler Schwellwert ab dem (>=) weitere Bedingungen pro Mautobjekt gepr√ºft werden.
erh.modellbewertung.min_anzahl_
erhebungsluecken_auffaelliges_mautobjekt
50
Minimaler Schwellwert ab dem (>=) weitere Bedingungen pro Mautobjekt gepr√ºft werden. Achtung: bei Regel 1001 begrenzt dieser Schwellwert die minimale Anzahl von Doppelerhebungen und nicht von Erhebungsl√ºcken.
erh.modellbewertung.max_anzahl_benutzer_
indikator_baustellenverkehr
10
Maximaler Schwellwert Anzahl verschiedener Nutzer unterhalb (<=) dessen eine Erhebungsanomalie pro Mautobjekt als Baustellenverkehr kategorisiert wird.
erh.modellbewertung.max_anzahl_stunden_fuer_temporaere_erhebungsanomalie
5
Maximales Zeit-Intervall in Stunden eines Bewertungstages innerhalb (<=) dessen das Auftreten einer Erhebungsanomalie pro Mautobjekt als tempor√§r kategorisiert wird.
erh.modellbewertung.id_string_prod_bd_instanz
PROD
String-Konstante zur Identifizierung der Produktions-Instanz.
erh.modellbewertung.max_mobaquote
98
Maximaler Schwellwert unterhalb (<=) dessen weitere Bedingungen pro Mautobjekt gepr√ºft werden.
erh.modellbewertung.min_anzahl_events_
auffaelliges_mautobjekt
50
Minimale Anzahl an Events pro Mautobjekt, ab der (>=) weitere Bedingungen gepr√ºft werden.

Tabelle 83: Soll- und Schwellwerte f√ºr Modellbewertung
Betriebsdatenvalidierung
 Key in Cassandra-Tabelle
Default
Beschreibung
erh.bdval.ng_befahrungen_grenzwert
10000
Grenzwert Anzahl auff√§lliger, nicht ge√§nderter Abschnitte in Bezug auf Befahrungen 
erh.bdval.ng_mobaquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, nicht ge√§nderter Abschnitte in Bezug auf MOBA-Quote
erh.bdval.ng_erhquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, nicht ge√§nderter Abschnitte in Bezug auf Erhebungsquote
erh.bdval.ng_mobaquote_toleranz
5 
Toleranz Abweichung MOBA-Quote
erh.bdval.ng_erhquote_toleranz
5
Toleranz Abweichung EOBA-Quote
erh.bdval.ng_befahrungen_toleranz
5
Toleranz Abweichung Befahrungen
erh.bdval.g_befahrungen_grenzwert
10000
Grenzwert Anzahl auff√§lliger, ge√§nderter Abschnitte in Bezug auf Befahrungen 
erh.bdval.g_mobaquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, ge√§nderter Abschnitte in Bezug auf MOBA-Quote
erh.bdval.g_erhquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, ge√§nderter Abschnitte in Bezug auf Erhebungsquote
erh.bdval.g_mobaquote_toleranz
5
Toleranz Abweichung MOBA-Quote ver√§nderter MO
erh.bdval.g_erhquote_toleranz
5
Toleranz Abweichung Erhebungsquote ver√§nderter MO
erh.bdval.g_befahrungen_toleranz
5
Toleranz Abweichung Befahrungen ver√§nderter MO
erh.bdval.n_mobaquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, ge√§nderter Abschnitte in Bezug auf MOBA-Quote
erh.bdval.n_erhquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger, neuer Abschnitte in Bezug auf Erhebungsquote
erh.bdval.n_erhquote_toleranz
95
Toleranz Abweichung Erhebungsquote neue MO
erh.bdval.n_mobaquote_toleranz
95
Toleranz Abweichung MOBA-Quote neue MO
erh.bdval.ng_eobaquote_toleranz
5
Toleranz Abweichung EOBA-Quote 
erh.bdval.ng_eobaquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger Erkennungsobjekte in Bezug auf EOBA-Quote
erh.bdval.g_eobaquote_toleranz
5
Toleranz Abweichung EOBA-Quote
erh.bdval.g_eobaquote_grenzwert
10000
Grenzwert Anzahl auff√§lliger Erkennungsobjekte in Bezug auf EOBA-Quote 
erh.bdval.n_eobaquote_toleranz
95
Toleranz Abweichung EOBA-Quote
erh.bdval.n_eobaquote_grenzwert
10000


erh.bdval.tq_sum_maut_ok_toleranz
1
Schwellwerte  f√ºr Tarfierungsqualit√§t
erh.bdval.tq_sum_ext_kosten_toleranz
1
Schwellwerte  f√ºr Tarfierungsqualit√§t
erh.bdval.tq_anz_tarif_ok_toleranz
1
Schwellwerte  f√ºr Tarfierungsqualit√§t
erh.bdval.tq_anz_tarif_nok_toleranz
1
Schwellwerte  f√ºr Tarfierungsqualit√§t

Tabelle 84: Soll- und Schwellwerte f√ºr Betriebsdatenvalidierung
EOBA Clustering
Hinweis: Die Konfiguration erfolgt in der gesonderten Tabelle erh_eoba_clusterconfig.
Key in Cassandra-Tabelle
Default
Beschreibung
config_id
2
Eindeutige ID der Konfiguration (Ganzzahl)
cluster_typ
2
Clustertyp: 0 = L√ºckenschluss, 1 = Tor, 2 = Kreis
anzahl_befahrungen_mo_min
50
Minimale Anzahl an Befahrungsparameter aus dem Archiv je EO-ID, die f√ºr die Durchf√ºhrung des Clustering erforderlich ist.
anzahl_befahrungen_mo_max
1000
Maximale Anzahl an Befahrungsparameter aus dem Archiv je EO-ID. Existieren mehr BfpA als durch diesen Key konfiguriert, werden genauso viele BfpA zuf√§llig ausgew√§hlt.
zeitintervall
1
Anzahl an vergangenen Tagen, f√ºr die die Befahrungsparamater aus dem Befahrungsparameter-Archiv in die Analyse einbezogen werden sollen.
min_pts
5
OPTICS-spezifischer Parameter: Mindestanzahl von Punkten in einer Nachbarschaft vom Radius Epsilon, ab der eine Gruppe von Punkten als Cluster betrachtet werden kann.
epsilon
1.0
OPTICS-spezifischer Parameter: Radius in dem nach Punkten gesucht wird, damit diese als Nachbarn gelten.
epsilon_cl
1.0
OPTICS-spezifischer Parameter: Erforderlicher Mindestabstand zwischen den Zentren von Clustern, damit diese noch als eigenst√§ndige Cluster betrachtet werden.
anzahl_cluster_max
1
Maximal erlaubte Anzahl von Clustern. Werden mehr Cluster erzeugt, werden die Cluster nicht weiter verarbeitet.
gueltig_von
2006-12-31T23:59:59Z
Startzeitpunkt des G√ºltigkeitszeitraums (Timestamp) der Konfiguration.
gueltig_bis
2006-12-31T23:59:59Z
Endzeitpunkt des G√ºltigkeitszeitraums (Timestamp) der Konfiguration.

Tabelle 85: Soll- und Schwellwerte f√ºr EOBAClustering


Fahrzeuggesundheitsakte
Die Fahrzeuggesundheitsakte (kurz: Gesundheitsakte) stellt eine GUI zur Verf√ºgung, die dem Anwender auf einfachem und schnellen Weg alle Informationen zu einem Fahrzeugger√§t (FzG) anzeigt. 
Dabei soll neben dem g√ºltigen Rollen-Rechte-Konzept und einer hohen Verf√ºgbarkeit auch eine maximale Skalierbarkeit m√∂glich sein.
Dieses Kapitel dient der Dokumentation und Anleitung zur Bedienung und Wartung der Fahrzeuggesundheitsakte. Speziell wird auf die einzelnen Funktionalit√§ten, M√∂glichkeiten und Grenzen eingegangen.
Aufbau und Funktionsweise
Bei der Fahrzeuggesundheitsakte handelt es sich um eine browserbasierte Server-Client Applikation. Der Client sendet einen Post-Request an den Server, dieser antwortet als Response in Form von HTML-Code. Der Server kann sowohl √ºber http als auch √ºber https angesprochen werden. Der Zugriff auf die Gesundheitsakte erfolgt √ºber g√§ngige Browser mit einem benutzerspezifischen Login, das √ºber das Toll-Collect Berechtigungsmanagement beantragt werden muss. 
Die Fahrzeuggesundheitsakte verf√ºgt √ºber eine statische Seitenstruktur (Hauptmen√º und weitere statische Elemente) sowie √ºber dynamische Inhalte (berechtigungsabh√§ngige Men√ºpunkte und Unterseiten).
Die Applikation gliedert sich in folgende Bereiche: 
FzG-Suche
√úPA
Schwellwerte
Admin
Das Berechtigungs- und Rollenkonzept f√ºr die Fahrzeuggesundheitsakte ist im Referenzdokument [12] [A_QSAV_BRK_00] QS-AV (QS-FzG/QS-Erh) Benutzer- Rollenkonzept beschrieben.
Der technische Aufbau ist im Architektur-Kapitel 4.2.5 Fahrzeuggesundheitsakte (Pr√§sentation) beschrieben.
Aufbau der Seitenstruktur 
Die Main.cgi erh√§lt nach erfolgreicher Authentifizierung die Anfragen(Requests) des Benutzers (Client) und leitet diese entsprechend an die Unterseiten weiter. Die Main.cgi baut die Men√ºleiste und alle statischen Elemente selbstst√§ndig zusammen. Der Inhalt der Unterseiten wird anhand der ermittelten Berechtigungsinformationen aus entsprechenden Dateien f√ºr die Unterseiten geliefert und in die statische Struktur anhand von Platzhaltern integriert. 

Abbildung 71: Aufbau des Hauptmen√ºs und der statischen Struktur (hervorgehoben)
Berechtigungskonfiguration
Die Zuordnung von Benutzergruppen (Active Directory-Gruppen) zur Berechtigung auf die Applikationsbereiche FzG-Suche, √úPA, Schwellwerte und Admin k√∂nnen √ºber eine entsprechende Rechtedatei (<basepath>/var/www/merkur/log/rights) konfiguriert werden. Dabei ist immer das aktuelle Berechtigungs- und Rollenkonzept (Referenzdokument [12]) zu beachten.
Bereich: FzG-Suche
Nach dem Einloggen gelangt der Benutzer direkt auf die FzG-Suche. FzG k√∂nnen per EHK-Id gesucht werden.

Abbildung 72: Einstieg FzG-Suche
F√ºr den Men√ºpunkt ‚ÄûFzG-Suche‚Äú existieren spezielle Maskenkonfigurationsdateien, um die berechtigungsabh√§ngigen Abfragen/Darstellungen/Ausgaben zu spezifizieren, ohne dabei den Quellcode anfassen zu m√ºssen. Sie befinden sich unter <basepath>/var/www/merkur/mask. Im Bereich ‚ÄûAdmin‚Äú ü°™ ‚ÄûEinstellungen‚Äú k√∂nnen berechtigte Benutzer diese Konfiguration auch per GUI anpassen.
Nach einer erfolgreichen Suche eines FzG werden die Statusinformationen des FzG in Abh√§ngigkeit der Maskenkonfigurationen angezeigt:

Abbildung 73: FzG-Suche - FzG Statusanzeige
Status√ºberblick
Im oberen Bereich werden FzG-Informationen angezeigt, untergliedert in folgende Tabellen:
Basisdaten
Letzte Meldungen
Dienstzustand
Sie dienen dem schnellen √úberblick √ºber die wichtigsten Statusinformationen des FzG.
Popup-Links (Buttons)
Darunter wird eine Button-Gruppe angezeigt, die Links zu Detailinformationen zum Zustandswechsel (FzG-Statusmeldung Historie) und zu Monitoringnachrichten des FzG zur Verf√ºgung stellt.
Weitere Informationen
Darunter folgen diverse Tabellen zu speziellen Themengebiete, die ebenso wie die zuvor beschriebenen Darstellungen √ºber die Maskenkonfiguration angepasst werden k√∂nnen.
Bereich: √úPA
Der Bereich ‚Äû√úPA‚Äú ist f√ºr zuk√ºnftige Releases vorgesehen und ist aktuell noch nicht mit Funktionen hinterlegt.
Bereich: Schwellwerte
Der Bereich ‚ÄûSchwellwerte‚Äú ist f√ºr zuk√ºnftige Releases vorgesehen und ist aktuell noch nicht mit Funktionen hinterlegt.
Bereich: Admin
Der Bereich ‚ÄûAdmin‚Äú widmet sich der administrativen Verwaltung der Gesundheitsakte. Die Verwaltung gliedert sich in folgende Unterbereiche:
Debugmeldungen:
Sofern im Unterbereich ‚ÄûKonfiguration‚Äú (s.u.) die Option ‚ÄûDebug‚Äú ausgew√§hlt wurde, k√∂nnen hier die Applikationslogs ab dem Level ‚Äûdebug‚Äú eingesehen werden.
Konfiguration:
In diesem Unterbereich k√∂nnen technische Konfigurationen vorgenommen werden:
Debug (BOOLEAN): true = Die Applikationslogs sollen im (=ab) Level ‚Äûdebug‚Äú geschrieben werden. Default = false. 
ldap_ou_dc: Verbindungsparameter zum LDAP-Server
ldap_server: LDAP-Server Adresse
qsav_keyspace: QS-AV Keyspace in der Cassandra-DB
qsav_port: Port zur Adressierung der Cassandra DB
qsav_server: IP zur Adressierung der Cassandra DB
qsav_user_r: Name des technischen Users, der nur lesen darf
qsav_user_r_pwd: Passwort des technischen Users, der nur lesen darf
qsav_user_rw: Name des technischen Users, der lesen und schreiben darf
qsav_user_rw_pwd: Passwort des technischen Users, der lesen und schreiben darf
Schwellwerttabellen: 
In diesem Unterbereich werden alle Tabellen, die im Bereich ‚ÄûSchwellwerte‚Äú editierbar sein sollen,  konfiguriert.
Hinweis: IE Kompatibilit√§tsmodus
Bei Nutzung des Internet Explorers muss die Kompatibilit√§tsansicht f√ºr Intranetseiten deaktiviert werden.

Abbildung 74: IE - Einstellungen der Kompatibilit√§tsansicht


Abbildung 75: IE - Kompatibilit√§tsansicht f√ºr Intranetseiten deaktivieren
Schnittstellensicht
Systemschnittstellen
In diesem Kapitel werden die Systemschnittstellen aufgef√ºhrt und kurz beschrieben. Die detaillierten Schnittstellenspezifikationen finden Sie jeweils in den referenzierten Dokumenten.
√úberblick: Schnittstellen 

Abbildung 76: L√∂sungsskizze Detailansicht QS-AV(Datenflussrichtung)
Nr.
Schnittstelle
Protokoll
Bemerkungen
814
GDBS ü°®ü°™ QS-AV
TCP/IP
Webservice-Schnittstelle
819
QS-AV ü°™ MonP
TCP/IP
Webservice-Schnittstelle (REST)
830
DM ü°™ QS-AV
TCP/IP
DB-Schnittstelle: CQL Inserts via Cassandra-Treiber
830 (II)
DM ü°™ QS-AV
TCP/IP
Umgekehrte Zugriffsrichtung f√ºr FzG-Full-Importe: Von DM bereitgestellte Oracle View via JDBC-Treiber.
833
ES ü°™ QS-AV
TCP/IP
DB-Schnittstelle: CQL Inserts via Cassandra-Treiber
836
EDM ü°™ QS-AV
TCP/IP
DB-Schnittstelle: CQL Inserts via Cassandra-Treiber
838
QS-AV ü°™ LogArchiv Server
TCP/IP
Technische Schnittstelle (via Splunk)
884
QS-AV ü°™ Cognos
TCP/IP
Webservice-Schnittstelle (REST)

Tabelle 86: √úberblick: Schnittstellen in MSD26

Rahmenbedingung bei der Realisierung aller Schnittstellen: Die QS-AV-spezifischen √úberwachungsprozesse sind nicht Bestandteil des Leistungsdatenstroms. Schnittstellen aus dem √úberwachungsprozess zur√ºck in den Leistungsdatenstrom sind nicht vorgesehen. Es muss sichergestellt werden, dass die Leistungsdatenverarbeitung unabh√§ngig und r√ºckkopplungsfrei von den QS-AV-√úberwachungsprozessen erfolgt. 
SST 814 Grunddaten Bereitstellungs-Server (GDBS) ü°®ü°™  QS-AV
Beschreibung
Der Bereitstellungsserver Grunddaten liefert wichtige Modellinformationen f√ºr EOBA und MOBA:
Abschnittssequenzen
Tarifabschnittsliste
Erkennungsmodell
Informationen √ºber aktuelle Baustellen
Release Notes (zusammen mit jeder neuen BD-Version)
Release Historie
QS-AV liefert die Ergebnisse der Modellbewertung an GDBS:
Auff√§lligkeiten
Fahrleistungsklassen
[L√ºckenschlussvorschl√§ge] ‚Äì dieses Produdukt wurde zwar in der SST vorgesehen, wird von QS-AV jedoch nicht geliefert.
Schnittstellenspezifikation
(Referenzdokument)
[4] [S_814_SST_00] SST 814 SST Spezifikation GDBS ‚Äì QS-AV

Tabelle 87: SST 814 GDBS ü°™ QS-AV

Abbildung 77: √úberblick SST 814 GDBS ü°™ QS-AV

Abbildung 78: √úberblick SST 814 QS-AV ü°™  GDBS (L√ºckenschlussvorschl√§ge nicht realisiert)

SST 830 Device Management (DM) - QS-AV
Beschreibung
Folgende f√ºr die FzG-Fehlerdetektion erforderlichen Informationen werden aus dem Device Management an die Qualit√§tssicherung AV √ºbermittelt:
FzG Statusmeldungen (FSM, zeitnah zur Entstehung)
FzG Stammdaten 
FULL (t√§glich)
FzG Einbau-Events (nach Registrierung)
Monitoring-Nachrichten
Fehler Events
MAC-Fehler
Loop-Fehler
Schnittstellenspezifikation
(Referenzdokument)
[5] [S_830_SST_00] SST 830 Schnittstellenspezifikation Device Management ‚Äì QS AV

Tabelle 88: SST 830 DM - QS-AV

Abbildung 79: √úberblick SST 830 DM ü°™ QS-AV
SST 833 Erkennungs-Service (ES) - QS-AV
Beschreibung
Folgende f√ºr die Erhebungsfehlerdetektion erforderlichen Informationen werden aus dem Erkennungsservice an QS-AV √ºbermittelt:
Befahrungsparameter
Events zu RsE-Kontakten
FSP-Bewertungs-Events
Rangier-Events
Events bei R√ºcksetzen des Erkennungszustands
Events bei √Ñnderung der Aufzeichnung
Events bei Rotschaltung aus technischem Grund
Deklarations-Events
Events bei Gr√ºnschaltung
Schnittstellenspezifikation
(Referenzdokument)
[6] [S_833_SST_00] SST 833 Schnittstellenspezifikation Erkennungs-Service (ES) ‚Äì Qualit√§tssicherung Erhebung (QS-Erh)

Tabelle 89: SST 833 ES - QS-AV

Abbildung 80: √úberblick SST 833 ES ü°™ QS-AV
SST 836 Erhebungs-Daten-Management (EDM) - QS-AV
Beschreibung
√úber die Schnittstelle EDM-QS-AV werden Events √ºbertragen, die zur Qualit√§tssicherung und Pr√ºfung der bestehenden Modelle ausgewertet werden. Die Events von EDM k√∂nnen nach Fehler-Events und Metrik-Events kategorisiert werden. Fehler-Events enthalten Informationen zu Verarbeitungsfehlern innerhalb von EDM, die bei der Tarifierung und Fahrtenbildung auftreten k√∂nnen. Metrik-Events enthalten eine oder mehrere Kennzahlen, die Aussagen √ºber die Verarbeitungsmengen und Produkte von EDM in einem bestimmten Zeitraum erm√∂glichen.
EDM-Fehlerevents
EDM-Metrikevents


Schnittstellenspezifikation
(Referenzdokument)
[7] [S_836_SST_00] SST 836 Schnittstellenspezifikation EDM ‚Äì

Tabelle 90: SST 836 EDM - QS-AV

Abbildung 81: √úberblick SST 836 ES ü°™ QS-AV
SST 819 QS-AV ü°™ MonP
Beschreibung
Die Schnittstelle dient dazu, im QS-AV anfallende Informationen (=Events) einheitlich an die Monitoring Plattform zu √ºbermitteln. Dabei wird die regelm√§√üige/aggregierte √úbermittlung von Events (Metrik-Events, z.B. ‚ÄûNichterkennungsquote mit gr√ºner LED Gesamtnetz‚Äú) sowie die ereignisgetriebene √úbermittlung von Events (z.B. Information-Events wie ‚ÄûImport neuer Betriebsdaten in QS-Erh ist fehlgeschlagen‚Äú) unterschieden. Ein periodisch √ºbermittelter Healthcheck (spezielles Information-Events) gibt Auskunft √ºber den Funktionsstatus der QS-AV Applikation.
Event-Daten werden separat in jeder Lastinstanz des Systems QS-AV generiert, ggf. gesammelt und √ºber die Schnittstelle an MonP √ºbertragen.
Die zu √ºbertragenden Events sind in der SST-Spezifikation 819 aufgelistet und beschrieben.
Schnittstellenspezifikation
(Referenzdokument)
[8] [S_819_SST_00] SST 819 SST-Spezifikation QS-AV ‚Äì MonP

Tabelle 91: SST 819 QS-AV ‚Äì MonP
Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation
 
Abbildung 82: SST 819 - Technische Realisierung √úberblick (Information- und Metrik-Events)
Aus der QS-AV Applikation heraus werden Information- und Metrik-Events an MonP √ºbermittelt. 
Information-Events, mit Ausnahme des Healthchecks, werden ereignisbasiert durch Bookkeeper-gesteuerte Jobs erzeugt und in die Warteschlange zum Versand eingereiht (DB-Tabelle tech_monp_pending_information_event): 
[DSJ-QSErh-0010] BetriebsdatenRestImporter (siehe Kapitel 5.2.5.1.1): 
Neues BD-Release wurde erfolgreich von GDBS heruntergeladen 
[DSJ-QSErh-0210] MonpNeueSollzuordnungenRse (siehe Kapitel 5.2.5.1.24): 
Neue RsE-EO/MO-Sollzuordnungen wurden erstellt 
Der Healthcheck ist ein spezielles Information-Event. F√ºr den Versand dieser speziellen Information-Events wird unabh√§ngig vom Bookkeeper beim Start der QS-AV Applikation der HealthCheck in einem eigenen Thread als PeriodicTask gestartet. Der Periodic Task greift auf das Event-Log des Bookkeepers zu, um den Applikationsstatus zu ermitteln.
Metrik-Events werden periodisch durch Bookkeeper-gesteuerte Jobs erzeugt und in die Warteschlange zum Versand eingereiht (DB-Tabelle tech_monp_pending_metric_event):
[DSJ-QSFzG-0100] MonpFzgEventEmitter (siehe Kapitel 5.2.4.1.10): 
Alle FzG-spezifischen Metrik-Events
[DSJ-QSErh-0200] MonpErhEventEmitter (siehe Kapitel 5.2.5.1.23): 
Alle erhebungsspezifischen Metrik-Events
Information- und Metrik-Events (Healthcheck ausgenommen) werden durch den ScheduledMonpSender an MonP versendet. Der  ScheduledMonpSender wird unabh√§ngig vom Bookkeeper beim Start der QS-AV Applikation in einem eigenen Thread als PeriodicTask gestartet. Mit Hilfe der Tabelle tech_monp_instanzbezeichner_counts wird je Instanz und Event eine eindeutige Event-Id gef√ºhrt und beim Versand √ºbergeben.
Konfiguration
Die Konfiguration der Verbindungseinstellungen erfolgt mittels Umgebungsvariablen. 
Umgebungsvariable
Beschreibung
MONP_SERVICE_URL
Muss gesetzt werden (kein Default). In der Testumgebung z.B. "https://monp.dmz.e2e.ux.tc.corp:443/v1", in der Produktion "https://monp.dmz.prod.ux.tc.corp:443/v1". Die Implementierung h√§ngt automatisch "/events" an.
MONP_IGNORE_CERTIFICATE
Default: "false". Falls zu Testzwecken Zertifikate ignoriert werden sollen, kann dies auf  "true" gesetzt werden.

Tabelle 92: SST 819 - Verbindungseinstellungen (Umgebungsvariablen)
Die allgemeine Konfiguration erfolgt in der Cassandra-Tabelle tech_config mittels der folgenden Keys.
Key
Default
Beschreibung
erh.monp.information.teilsystemkuerzel
QSAV
Kurzbezeichnung des Teilsystems, in dem das MonP-Event generiert und verschickt wird. 
erh.monp.information.konfigurationskuerzel
PROD
Kurzbezeichnung der Betriebsdateninstanz.
erh.monp.http_connection_timeout
5
Dauer in Sekunden, wie lange beim Verbinden zu MonP gewartet werden soll bevor ein Timeout erfolgt. 
erh.monp.http_request_timeout
5
Dauer in Sekunden, wie lange beim Versenden von Events auf eine Antwort von MonP gewartet werden soll, bevor ein Timeout erfolgt. 
erh.monp.healthcheck_period
300
Intervall in Sekunden, in dem der MonP HealthCheck Information-Event versandt wird.
erh.monp.event_sender_period
300
Intervall in Sekunden, in dem anstehende Information- und Metrik-Events an MonP versendet werden.

Tabelle 93: SST 819 ‚Äì Allgemeine Konfiguration in Tabelle tech_config
SST 884 QS-AV ü°™ Cognos
Beschreibung
Die Schnittstelle wird von QS-AV als REST-Webservice zur Verf√ºgung gestellt. Der Service bietet Cognos Dienste f√ºr den lesenden Zugriff auf verdichtete und bewertete Daten der QS-AV-Applikation gem√§√ü der Schnittstellen-Spezifikation 884. Cognos ben√∂tigt diese Daten, um automatisiert und nach internen Regeln Berichte und Reports zu generieren.
Schnittstellenspezifikation
(Referenzdokument)
[9] [S_884_SST_00] SST 884 SST-Spezifikation QS-AV ‚Äì Cognos

Tabelle 94: SST 884 QS-AV ‚Äì Cognos
Technische Realisierung der Schnittstelle innerhalb der QS-AV Applikation
F√ºr die Schnittstelle 884 werden durch QS-AV Daten bereitgestellt, die mittels REST-Webservice von Cognos zur Erstellung von Berichten und Reports abgerufen werden k√∂nnen. Der Webservice bietet mehrere Endpunkte (Services). F√ºr jeden Endpunkt existiert eine eigene Quelltabelle (in der Abbildung blau eingef√§rbt), aus der die angefragten Daten gelesen werden. Diese Quelltabelle wird jeweils durch einen durch den Bookkeeper gesteuerten Job (in der Abbildung gr√ºn dargestellt) bef√ºllt, der die relevanten Daten aus seinen Input-Tabellen des QS-AV Datenbestands ermittelt.

Abbildung 83: SST 884 - Technische Realisierung √úberblick Datenbereitstellung f√ºr Cognos
Die einzelnen Jobs sind in der Komponentensicht je nach Zugeh√∂rigkeit in den Kapiteln 5.2.4 qs-fzg: Qualit√§tssicherung der Fahrzeugger√§te oder 5.2.5 qs-erh: Qualit√§tssicherung der Erhebung beschrieben.
SST 838 QS-AV ü°™ LogArchiv Server
Bei der Schnittstelle 838 handelt es sich um eine rein technische Schnittstelle zur automatischen √úbertragung der Applikationslog-Dateien an den zentralen LogArchiv Server (Splunk) mittels Forwarder. 
Die globalen Vorgaben zum Logging m√ºssen eingehalten werden. Diese sind im Anhang [10] White Paper ‚Äì AppAgile Logging-Format spezifiziert.
Ablaufsicht
Der Programmablauf wird bestimmt durch die beiden Komponenten Bookkeeper (QS-AV Applikation) und Timebucket-Switcher, die je als Applikationen innerhalb  eigenst√§ndiger Pods laufen. Die Datenbereitstellung und Taktgebung des Timebucket-Switchers ist die Voraussetzung f√ºr die Durchf√ºhrung und Konzertierung der Qualit√§tssicherungsalgorithmen auf Seiten des Bookkeepers. Detaillierte Beschreibungen der beiden Applikationen k√∂nnen Sie den Kapiteln 5.1 Schwerpunktkapitel: Bookkeeper (qs-common) und 5.3 Timebucket-Switcher Applikation entnehmen.
Timebucket-Switcher-Ablauf

Abbildung 84: Programmablauf auf Seiten des Timebucket-Switchers (vereinfacht)
Hinweis: In diesem Kapitel werden Grundkenntnisse des Timebucket-Switchers vorausgesetzt. Weitere Informationen dazu finden Sie im Kapitel 5.3 Timebucket-Switcher Applikation.
Die Timebucket-Switcher Applikation wird lokal auf dem Timbucket-Switcher-Pod gestartet.
Die Klasse Main startet √ºber den LongRunner den TimebucketSwitcher, der zyklisch die Timebuckets in der Tabelle en_q_timebuckets aktualisiert (‚Äûswitcht‚Äú) und die Verarbeitung in der Tabelle bkpr_eventlog protokolliert. 
Der Timebucket-Switcher wird solange ausgef√ºhrt, bis die Applikation beendet wird. 
Bookkeeper-Ablauf
Die folgende Grafik stellt vereinfacht den Programmablauf dar.

Abbildung 85: Programmablauf auf Seiten des Bookkeepers (vereinfacht)
Hinweis: In diesem Kapitel werden Grundkenntnisse des Bookkeepers vorausgesetzt. Weitere Informationen dazu finden Sie im Kapitel 5.1 Schwerpunktkapitel: Bookkeeper (qs-common).
Die QS-AV Applikation wird durch spark-submit des qs-boot.jar an den Spark-Kontext √ºbermittelt und somit zur Ausf√ºhrung im Cluster gestartet.
Die Klasse Start startet √ºber den LongRunner den JobExecutor, der zyklisch anhand des Zustands-Logs (bkpr_eventlog) mittels PreconditionChecker gem√§√ü der Konfiguration (RuleConfig) RuleJobs ausf√ºhrt, die √ºber das CassandraRuleJobRepository verwaltet werden.  Im Zustands-Log protokolliert der Timebucket-Switcher die Aktualisierung der Timebuckets.
Der Bookkeeper l√§uft solange keine Abbruchbedingung von au√üen gesetzt wird. 
Die Ablaufsicht aller Jobs inklusive ihrer Namen, Ausf√ºhrungsintervalle und Abh√§ngigkeiten wird formatbedingt als digitaler Anhang zur Verf√ºgung gestellt. Siehe Anhang [BK-Graph].

Abbildung 86: Miniaturansicht Bookkeeper Ablaufsicht (Grafik im Anhang [BK-Graph] )
Beispiel f√ºr das Lesen der Grafik: Der MdnParser bezieht seine Daten aus der Tabelle lds_dm_en_q_mdn (15 Min Partitionen), zerlegt den Base64-kodierten String mit den Monitoring-Daten (Payload) und schreibt die Ergebnisse in der Zieltabelle dm_mdn. Die Tabelle dm_mdn fungiert wiederum als Quelltabelle f√ºr die MDN-Normalisierung (MdnNormalizer). Die Ergebnisse der Normalisierung werden viertelst√ºndlich in die Zieltabelle lds_dm_data_primary_mdn_ergebnis geschrieben. F√ºr die 24-st√ºndliche FzG-Verdichtung (FzGVerdichtung) werden Daten aus den Quelltabellen fzg_mdn_normalisierung_ergebnis, lds_dm_em_q_fehler und fzg_fsm_normalisierung_ergebnis  gelesen (jeweils 96 Partitionen √† 15 Minuten), korreliert und verdichtet.
Datenmodell
Das Datenmodell (siehe Referenzdokument [13]) wird zusammen mit den Software-Lieferungen zur Verf√ºgung gestellt.
Verteilungssicht
Aufbau und Verteilung der Systemkomponenten sind im Kapitel 4.2 Technischer Kontext dargestellt und beschrieben. 
Weitere Deployment-spezifische Details werden im Betriebshandbuch (siehe Referenzdokument [17] [A_QSAV_HBB_00] QS-AV Betriebshandbuch) dokumentiert.
Konzepte
Parallelisierung und Threading
Die Ausf√ºhrung der QS-AV Applikation erfolgt single-threaded.
Logging
Das Logging erfolgt gem√§√ü [10] White Paper ‚Äì AppAgile Logging-Format.
Fehlercode- und Fehlerbehandlung
Fehlermeldungen und Fehlerbehandlung werden im Betriebshandbuch dokumentiert (Referenzdokument [17]).
Ablaufsteuerung
Siehe Kapitel 7 Ablaufsicht.
Transaktionsbehandlung
Basierend auf der eingesetzten Technologie wird die Transaktionsbehandlung nicht explizit durch Frameworks oder Produkte abgedeckt. Bei der Implementierung muss darauf geachtet werden, Operationen idempotent zu halten. Durch die Wiederaufsatzfunktion des Bookkeepers werden abgebrochene Jobs erneut durchgef√ºhrt bzw. Jobketten fortgesetzt.
Sessionbehandlung
Das Thema Sessionbehandlung ist im Kontext QS-AV nicht relevant.
Clustering & Replikation
F√ºr Clustering und Replikation werden die Mechanismen der Cassandra DB bzw. DataStax Enterprise genutzt. 
 Weitere Informationen und Konfigurationshinweise finden Sie im Installationshandbuch und im Betriebshandbuch (Referenzdokumente [16] und [17]).
Benutzeroberfl√§chen
Die QS-AV Applikation an sich verf√ºgt √ºber keine Benutzeroberfl√§che. Die Benutzeroberfl√§chen von Zeppelin und dem DSE OpsCenter sind auf den jeweiligen Produktseiten dokumentiert (siehe Kapitel 12 Verweise). Die Benutzeroberfl√§che der Gesundheitsakte ist im Kapitel 5.5 Fahrzeuggesundheitsakte beschrieben.
Benutzer- und Rollenkonzept
Das Benutzer- und Rollenkonzept ist in einem gesonderten Dokument beschrieben, siehe Referenzdokument [12] [A_QSAV_BRK_00] QS-AV (QS-FzG/QS-Erh) Benutzer- Rollenkonzept.
 Datenschutz/Datensicherheit
Die Betrachtung hinsichtlich der Datenaufbewahrungsfristen finden Sie im Systeml√∂schkonzept (Referenzdokument [14]). 
Die Aspekte und Konfiguration der Transportverschl√ºsselung werden in Installations- und Betriebshandbuch behandelt (Referenzdokumente [16] und [17]).
Das Berechtigungs- und Rollenkonzept ist im Referenzdokument [12] [A_QSAV_BRK_00] QS-AV (QS-FzG/QS-Erh) Benutzer- Rollenkonzept beschrieben.
 Hochverf√ºgbarkeit und Skalierbarkeit
F√ºr Hochverf√ºgbarkeit und Skalierbarkeit werden die Konzepte der Cassandra DB bzw. DataStax Enterprise, sowie der AppAgile-Umgebung genutzt. 
 Weitere Informationen dazu finden Sie im Installationshandbuch und im Betriebshandbuch (Referenzdokumente [16] und [17]).
 Allgemeine Festlegungen
G√ºltigkeitsbereiche
G√ºltigkeitsbereiche sollen immer, wie in den folgenden Abschnitten beschrieben, interpretiert werden. Im Zweifel muss, insbesondere hinsichtlich der Verwendung in Regeln, eine Einzelfallkl√§rung durchgef√ºhrt werden.
Zeitr√§ume
Von <Startdatum> - Bis <Enddatum>
Zeitraum beginnend mit dem Startdatum (von = inklusiv) bis einschlie√ülich des Tages vor dem Enddatum. Das Enddatum ist vom Zeitraum ausgeschlossen (bis = exklusiv).
Beispiel:
Startdatum = 17.11.2016
Enddatum = 21.11.2016
ü°™ G√ºltigkeitsbereich = {17.11.2016, 18.11.2016, 19.11.2016, 20.11.2016}
Versionen (z.B. BD-Versionen, Produktversionen):
Von <Erste Version> - Bis <Letzte Version>
Versionsg√ºltigkeit beginnend mit der ersten Version (von = inklusiv) bis einschlie√ülich der letzten Version (bis = inklusiv).
Beispiel:
Erste Version: 1.1
Letzte Version: 1.4
G√ºltigkeitsbereich = {1.1, 1.2, 1.2.1, 1.3, 1.4}
Format der Betriebsdatenversion
Die BD-Version wird von Grunddaten als String geliefert. F√ºr Versionsvergleiche (Version von ‚Äì bis) muss dieser String zerlegt werden.
Die folgende Formatbeschreibung  soll dies erm√∂glichen: 
([a-zA-Z0-9][.])?([0-9])[.]([0-9])([.]([0-9]))? = Die Version eines Betriebsdatenrelease oder Produkts, bestehend aus einem optionalen Umgebungsbezeichner, Major-, Minor- und optionaler Hotfix-Nummer, jeweils getrennt durch Punkte
Beispiele f√ºr korrekte BD Versionen
VALI.16.12.003
HierStehtIrgendetwas.999999.0000015001.999999
8.7.0
Referenzdokumente
Mautsystem Deutschland: Eckdatenkatalog ETC Gesamtarchitektur
Toll Collect GmbH Glossar, Erl√§uterung von Begriffen und Abk√ºrzungen zum Mautsystem Deutschland
Lastenheft QS-FzG und Lastenheft QS-Erhebung
[S_814_SST_00] SST 814 SST Spezifikation GDBS ‚Äì QS-AV
[S_830_SST_00] SST 830 Schnittstellenspezifikation Device Management ‚Äì QS AV Core
[S_833_SST_00] SST 833 Schnittstellenspezifikation Erkennungs-Service (ES) ‚Äì Qualit√§tssicherung Erhebung (QS-Erh)
[S_836_SST_00] SST 836 Schnittstellenspezifikation EDM ‚Äì QSAV
[S_819_SST_00] SST 819 SST-Spezifikation QS-AV ‚Äì MonP
[S_884_SST_00] SST 884 SST-Spezifikation QS-AV ‚Äì Cognos
White Paper ‚Äì AppAgile Logging-Format
Entwicklungsrichtlinien AppAgile ‚Äì Technische Dokumentation
[A_QSAV_BRK_00] QS-AV (QS-FzG/QS-Erh) Benutzer- Rollenkonzept
[A_QSAV_DM_00] QS-AV-Datenmodell
[A_QSAV_SLK_00] Systeml√∂schkonzept
QS-AV ‚Äì Entscheidungsempfehlung zur technischen Architektur (Capgemini)
[A_QSAV_HBI_00] QS-AV Installationshandbuch
[A_QSAV_HBB_00] QS-AV Betriebshandbuch
QS-AV Glossar

Verweise
cloudera.com (Spark Guide) [Online]. - 06. 12 2016. - http://www.cloudera.com/documentation/enterprise/5-6-x/PDF/cloudera-spark.pdf.
DataStax Enterprise - The Database for Cloud Applications [Online]. - 10. 11 2016. - http://www.datastax.com/products/datastax-enterprise.
GitHub - databricks/scala-style-guide: Databricks Scala Coding Style Guide [Online]. - 06. 12 2016. - https://github.com/databricks/scala-style-guide.
OpsCenter: An Architecture Overview [Online]. - 30. 11 2016. - http://www.datastax.com/dev/blog/opscenter-an-architecture-overview.
Spark introduction [Online]. - 22. 11 2016. - https://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/spark/sparkIntro.html.
Submitting Applications - Spark 1.6.1 Documentation [Online]. - 22. 11 2016. - https://spark.apache.org/docs/1.6.1/submitting-applications.html.
Zeppelin [Online]. - 12. 05 2016. - https://zeppelin.apache.org/.


Glossar und Abk√ºrzungsverzeichnis
Das QS-AV projektspezifische Glossar finden Sie im Referenzdokument [18] QS-AV Glossar.
Das allgemeine Glossar zum Mautsystem Deutschland finden Sie im Referenzdokument [2] Toll Collect GmbH Glossar, Erl√§uterung von Begriffen und Abk√ºrzungen zum Mautsystem Deutschland.
Hinweis: Bookkeeper-spezifische technische Fachbegriffe sind im Kapitel 5.1.2 Bookkeeper Glossar erl√§utert.
Anhang 
[BK-Graph] Bookkeeper Ablaufsicht



